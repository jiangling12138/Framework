{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607d07f8-98c5-4910-8ea2-a3c954602773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Running Acrobot-v1 ====\n",
      "Iteration 1, Moving Avg Return (last 3 iters): -103.50, Success Rate: 0.40\n",
      "Failure Pattern: The Acrobot-v1 episode summaries show consistently negative returns, ranging from -72.0 to -136.0, with episode lengths closely matching the absolute return values, indicating that each step incurs a reward of -1 until termination. The most common failure pattern is the agent's inability to solve the task quickly, often requiring 90–137 steps, suggesting inefficient exploration or suboptimal policy decisions. The clustering of returns around -90 to -120 implies the agent frequently fails to swing the lower link high enough within the time limit. While state characteristics are not provided, the action issues likely involve insufficient momentum generation or poor timing in torque application, preventing the agent from achieving the goal state efficiently. Overall, the agent struggles with effective action sequencing, leading to prolonged episodes and consistently negative returns.\n",
      "Iteration 2, Moving Avg Return (last 3 iters): -102.20, Success Rate: 0.50\n",
      "Failure Pattern: Analysis of the Acrobot-v1 episode summaries reveals several key failure patterns. Both policy versions frequently result in negative returns, with most episodes ending between -70 and -140, indicating the agent often fails to swing the tip above the target in the allowed steps. The episode lengths closely match the absolute value of the returns, suggesting the agent typically receives a reward of -1 per timestep until termination, which is standard for Acrobot. There is no evidence of early successes (high positive returns or short episodes), implying the agent struggles with efficient exploration or control. The presence of several episodes with particularly low returns (e.g., -135, -136, -124, -121, -117) suggests the agent sometimes gets stuck in suboptimal behaviors, possibly due to poor state transitions or ineffective action selection, such as failing to build sufficient momentum. The lack of significant improvement from policy version 1 to 2, as seen in similar return distributions, further indicates persistent issues with learning effective state-action mappings. Overall, the main failure patterns are slow or failed task completion, likely due to insufficient exploration, poor momentum-building strategies, and ineffective policy updates.\n",
      "Iteration 3, Moving Avg Return (last 3 iters): -102.73, Success Rate: 0.50\n",
      "Failure Pattern: Analysis of the Acrobot-v1 episode summaries reveals several key failure patterns. The most common returns cluster between -70 and -135, with occasional worse episodes (e.g., -162), indicating frequent failure to reach the goal efficiently. Episode lengths closely track returns, suggesting that failures are typically due to the agent not solving the task within the time limit rather than early termination. Across policy versions, there is no clear trend of improvement, and high-penalty episodes persist, implying inconsistent policy performance. The repeated occurrence of returns near the maximum episode length (e.g., returns of -135, -136, -124, -118, etc.) suggests the agent often gets stuck in suboptimal oscillatory states, likely failing to generate sufficient upward momentum or coordinate joint actions effectively. This points to action selection issues, such as poor timing or magnitude of torque application, and possibly inadequate exploration or exploitation of the state space. Overall, the agent frequently fails to escape local minima, leading to long, unproductive episodes and suboptimal returns.\n",
      "Iteration 4, Moving Avg Return (last 3 iters): -103.33, Success Rate: 0.50\n",
      "Failure Pattern: Across Acrobot-v1 episodes for policy versions 2–4, the most common failure pattern is episodes ending with negative returns clustered between -70 and -140, with frequent returns near -90, -124, and -135, indicating the agent often fails to swing up the pendulum efficiently or maintain the goal state. Episode lengths closely mirror returns, suggesting failures are tied to timeouts or repeated suboptimal actions rather than abrupt catastrophic mistakes. The recurrence of high-magnitude negative returns (e.g., -135, -137) points to the agent getting stuck in unproductive state-action loops, likely near the bottom of the swing, unable to generate enough momentum. There is no evidence of sudden, isolated failures; instead, the agent consistently struggles with state transitions requiring coordinated torque application, reflecting issues with exploration or policy expressiveness in challenging regions of the state space.\n",
      "Iteration 5, Moving Avg Return (last 3 iters): -118.85, Success Rate: 0.47\n",
      "Failure Pattern: The Acrobot-v1 episode summaries reveal several common failure patterns. Most notably, episodes with extremely low returns (e.g., -500) and maximum lengths (500 steps) indicate the agent failed to solve the task within the time limit, likely getting stuck in unproductive states or repeating ineffective actions. Across all policy versions, there is a recurring cluster of episodes with returns between -70 and -130, suggesting the agent often struggles to consistently reach the goal efficiently. The presence of both short and long episodes with poor returns points to instability in policy performance, possibly due to suboptimal action selection or difficulty escaping local minima in the state space. Overall, the agent frequently fails by either taking too long to solve the task or failing to reach the goal at all, reflecting issues with both exploration and exploitation in its learned policy.\n",
      "Iteration 6, Moving Avg Return (last 3 iters): -118.58, Success Rate: 0.47\n",
      "Failure Pattern: Across the Acrobot-v1 episode summaries, the most common failure pattern is episodes ending with moderately negative returns (typically between -70 and -130), indicating the agent often fails to solve the task efficiently but avoids catastrophic failure except for rare outliers (e.g., -500 return). The episode lengths closely mirror the returns, suggesting the agent frequently reaches the maximum step limit or terminates near it, reflecting slow or indecisive action sequences. The state characteristics likely involve the agent struggling to swing the lower link high enough to reach the goal, with repeated oscillations and insufficient momentum transfer. Action issues are implied by the frequent moderate returns: the policy may be stuck in suboptimal cycles, failing to exploit key states for rapid progress. The rare extreme negative return (-500) signals occasional episodes where the agent gets trapped in ineffective action loops, unable to escape poor states. Overall, the agent demonstrates inconsistent control, with most failures stemming from inefficient exploration and lack of decisive action to solve the environment promptly.\n",
      "Iteration 7, Moving Avg Return (last 3 iters): -126.15, Success Rate: 0.40\n",
      "Failure Pattern: Across the Acrobot-v1 episode summaries, the most common failure pattern is the frequent occurrence of moderately negative returns (typically between -70 and -140), indicating that the agent often fails to solve the task efficiently but avoids catastrophic failure in most episodes. However, there are occasional outliers with very poor returns (e.g., -500, -247, -167, -161), suggesting episodes where the agent either gets stuck or fails to swing up the pendulum within the maximum step limit. The episode lengths closely mirror the returns, with longer episodes (close to 500 steps) corresponding to the worst returns, reflecting the environment’s time-out penalty. There is no evidence of consistent improvement across policy versions, as high-variance returns persist even in later versions. The failures likely stem from inconsistent action selection—possibly oscillating or indecisive torques—leading to the agent being unable to build sufficient momentum or stabilize the system. This suggests the policy struggles with reliably transitioning from the initial state to the upright goal, often getting stuck in suboptimal swinging or failing to exploit successful state trajectories.\n",
      "Iteration 8, Moving Avg Return (last 3 iters): -111.00, Success Rate: 0.47\n",
      "Failure Pattern: Across Acrobot-v1 episodes for policy versions 6–8, the most common failure pattern is consistently high negative returns (typically between -74 and -151), indicating the agent often fails to reach the goal quickly. Episode lengths closely mirror returns, suggesting the agent spends many steps without success, likely oscillating or stalling near suboptimal states. There are occasional outliers with very poor returns (e.g., -247), hinting at episodes where the agent is persistently stuck. The lack of improvement in returns across policy versions and repeated moderate failures suggest issues with exploration or suboptimal action selection, possibly failing to exploit states that lead to faster goal achievement. Overall, the agent frequently gets trapped in inefficient state-action loops, leading to prolonged episodes and consistently poor performance.\n",
      "Iteration 9, Moving Avg Return (last 3 iters): -131.05, Success Rate: 0.47\n",
      "Failure Pattern: Across Acrobot-v1 episodes, the most common failure pattern is consistently low returns, with most episodes ending between -70 and -150, and a single severe failure at -500 (maximum episode length, indicating the agent failed to solve the task). Returns and episode lengths are tightly coupled, suggesting the agent often fails to swing the end-effector to the goal in time, resulting in long episodes with poor scores. Policy versions 7–9 show similar issues, with occasional improvements (returns near -74 to -80) but frequent episodes above -100. This indicates the agent struggles with efficient action selection, likely failing to coordinate torque application for upward momentum. The rare short episodes with better returns suggest sporadic success but no consistent mastery. Overall, the agent’s policies lack robustness, with frequent indecisive or suboptimal actions leading to repeated failures to reach the terminal state efficiently.\n",
      "Iteration 10, Moving Avg Return (last 3 iters): -139.12, Success Rate: 0.47\n",
      "Failure Pattern: Across Acrobot-v1 episodes, the most common failure pattern is episodes terminating with the maximum negative return (-500), indicating the agent failed to solve the task within the allowed steps. These failures are sporadic but notable in policy versions 9 and 10, suggesting instability or regression in policy updates. Most other episodes end with returns between -70 and -150, with lengths closely matching the absolute return, reflecting the environment's reward structure (typically -1 per timestep). This implies the agent often fails to swing the end-effector to the target height efficiently, likely due to suboptimal action selection—such as insufficient torque application or poor timing—leading to prolonged episodes and missed goals. The presence of both short and long episodes with varying returns highlights inconsistent policy performance, possibly due to inadequate exploration or overfitting to specific state trajectories. Overall, failure is characterized by inability to reach the goal state, inefficient action sequences, and occasional catastrophic episodes where the agent stalls for the full duration.\n",
      "==== Running CartPole-v1 ====\n",
      "Iteration 1, Moving Avg Return (last 3 iters): 500.00, Success Rate: 1.00\n",
      "Failure Pattern: The provided episode summary for CartPole-v1 shows a return of 500.0 and length 500, indicating a perfect episode where the pole was balanced for the maximum allowed steps. Typically, common failure patterns in CartPole-v1 include the pole falling due to delayed or incorrect actions, especially when the cart or pole velocity becomes high or the pole angle exceeds a critical threshold. Failures often occur when the agent cannot recover from large deviations in pole angle or cart position, frequently due to suboptimal action selection under rapidly changing states. Return patterns in failed episodes usually show early termination with returns well below the maximum, reflecting the agent's inability to maintain stability. In contrast, the provided summary demonstrates no such failures, suggesting effective state management and timely action choices throughout the episode.\n",
      "Iteration 2, Moving Avg Return (last 3 iters): 500.00, Success Rate: 1.00\n",
      "Failure Pattern: Based on the provided episode summaries, both runs in the CartPole-v1 environment achieved the maximum possible return and episode length (500), indicating no failures occurred. Typically, common failure patterns in CartPole-v1 involve the pole falling due to excessive angle deviation, the cart moving out of bounds, or suboptimal action selection leading to instability. These failures are often characterized by abrupt drops in return and shorter episode lengths. However, in your data, the consistent maximum scores suggest stable state management and effective action choices, with no observable failure patterns in these episodes.\n",
      "Iteration 3, Moving Avg Return (last 3 iters): 500.00, Success Rate: 1.00\n",
      "Failure Pattern: In the provided CartPole-v1 episode summaries, each policy version achieves the maximum possible return and episode length (500), indicating no failures occurred during these runs. Typically, common failure patterns in CartPole-v1 involve the pole falling due to excessive angle deviation, the cart moving out of bounds, or suboptimal action selection leading to instability. These failures are often characterized by abrupt drops in return and shorter episode lengths. However, in your data, the consistent maximum scores suggest stable policies with effective state management and action selection, and no observable failure patterns.\n",
      "Converged! Stop training. Moving Avg Return=500.00, Success Rate=1.00\n",
      "==== Running MountainCarContinuous-v0 ====\n",
      "Iteration 1, Moving Avg Return (last 3 iters): 89.33, Success Rate: 0.00\n",
      "Failure Pattern: Across these MountainCarContinuous-v0 episodes, the agent consistently achieves high returns (mostly ~89.4) with episode lengths tightly clustered around 106 steps, indicating stable but suboptimal performance. The slight variation in returns and occasional longer episodes (up to 110 steps) suggest the agent sometimes struggles to efficiently reach the goal, likely due to insufficient momentum buildup or suboptimal action selection near the hill’s peak. The lack of significant return drops or very short episodes implies the agent rarely fails catastrophically, but the repeated near-identical returns and lengths point to a policy that is not fully exploiting the environment’s dynamics—potentially hesitating or oscillating at critical states rather than executing decisive actions to finish faster. Overall, the most common failure pattern is a lack of aggressive, well-timed actions in high-potential states, leading to consistently adequate but not optimal episode outcomes.\n",
      "Iteration 2, Moving Avg Return (last 3 iters): 89.31, Success Rate: 0.00\n",
      "Failure Pattern: Across both policy versions in the MountainCarContinuous-v0 environment, episode returns are tightly clustered around ~89.4, with occasional dips to ~89.0 and ~89.1, and episode lengths mostly at 106 steps, with some outliers at 109–110 steps. This consistency suggests the agent reliably reaches the goal but may not be optimizing for minimal steps or maximum return. The most common failure pattern is a slight inefficiency—episodes with longer lengths and marginally lower returns—likely caused by suboptimal action sequences (e.g., insufficient momentum buildup or delayed acceleration near the hill’s crest). There is no evidence of catastrophic failures (very low returns or excessive episode lengths), indicating the policy is stable but not perfectly tuned for speed or reward maximization.\n",
      "Iteration 3, Moving Avg Return (last 3 iters): 89.34, Success Rate: 0.00\n",
      "Failure Pattern: Across the MountainCarContinuous-v0 episode summaries, the most common failure pattern is the lack of significant variation in returns and episode lengths, with most episodes achieving returns around 89.4 and lengths between 105 and 110 steps, regardless of policy version. This suggests the agent consistently reaches the goal but does not optimize for faster or higher-reward solutions. The state characteristics likely involve the agent oscillating near the minimum energy threshold required to reach the goal, without exploiting momentum for quicker completion. Action issues may include suboptimal throttle control, resulting in repeated near-identical trajectories and missed opportunities for more efficient climbs. Overall, the agent demonstrates reliable but non-optimal performance, plateauing at a mediocre solution rather than exploring strategies for improvement.\n",
      "Iteration 4, Moving Avg Return (last 3 iters): 89.36, Success Rate: 0.00\n",
      "Failure Pattern: Across policy versions 2, 3, and 4 in the MountainCarContinuous-v0 environment, the episodes consistently achieve high returns (mostly between 89.3 and 89.5) and short episode lengths (105–110 steps), indicating that the agent reliably reaches the goal. There is minimal variance in returns and episode lengths, suggesting stable policy performance with no significant regressions or catastrophic failures. The rare slightly lower returns (e.g., 89.0 or 89.1) and marginally longer episodes (up to 110 steps) may indicate occasional suboptimal action choices, likely near the goal state where precise control is required. However, there are no clear signs of persistent failure states (e.g., getting stuck or oscillating) or major action selection issues. Overall, the agent demonstrates robust and consistent behavior, with only minor inefficiencies in a small subset of episodes.\n",
      "Iteration 5, Moving Avg Return (last 3 iters): 89.38, Success Rate: 0.00\n",
      "Failure Pattern: Across policy versions 3, 4, and 5 in the MountainCarContinuous-v0 environment, episode returns are highly consistent, clustering tightly between 89.1 and 89.5, with episode lengths ranging narrowly from 105 to 109 steps. This suggests the agent reliably reaches the goal but does so just above the minimum reward threshold (solving the environment requires a return >90). The most common failure pattern is the inability to achieve higher returns, likely due to suboptimal acceleration timing or insufficient exploitation of momentum, causing the car to reach the goal slowly and with minimal excess reward. There are no catastrophic failures (e.g., very low returns or excessively long episodes), but the agent consistently fails to optimize for faster, more efficient solutions, indicating action selection is adequate but not optimal. State transitions likely show the agent hesitating or oscillating near the goal before completion, reflecting conservative or repetitive action choices rather than decisive, efficient climbs.\n",
      "Iteration 6, Moving Avg Return (last 3 iters): 89.39, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 across policy versions 4–6 show highly consistent returns clustered tightly between 89.1 and 89.5, with episode lengths ranging narrowly from 105 to 109 steps. This suggests the agent reliably reaches the goal but does not achieve significantly higher returns, likely due to suboptimal timing or magnitude of actions near the goal state—possibly overshooting or failing to maximize the reward for early completion. The lack of significant variation in returns and episode lengths indicates that the agent has learned a stable, but not fully optimized, policy. There is no evidence of catastrophic failures (e.g., very low returns or excessively long episodes), but the agent may be exhibiting conservative or repetitive action patterns, such as consistently applying similar force profiles, leading to plateaued performance. The most common failure pattern is thus a lack of further improvement rather than outright failure, likely due to insufficient exploration or fine-tuning in the final approach to the goal state.\n",
      "Iteration 7, Moving Avg Return (last 3 iters): 89.40, Success Rate: 0.00\n",
      "Failure Pattern: Across policy versions 5, 6, and 7 in the MountainCarContinuous-v0 environment, the episodes consistently achieve high returns (mostly between 89.1 and 89.5) and short lengths (105–109 steps), indicating successful completion with minimal variance. There are no clear \"failure\" episodes in this data—returns are tightly clustered near the environment's maximum, and episode lengths are optimal or near-optimal. This suggests that the agent reliably reaches the goal with little deviation. No problematic state characteristics or action issues are evident; the agent avoids common MountainCar failures such as stalling at the hill base or oscillating without progress. The only minor pattern is that slightly lower returns (e.g., 89.1 or 89.2) correspond to marginally longer episodes, likely due to less efficient trajectories, but these are rare and not indicative of significant failure. Overall, the agent demonstrates robust, consistent performance with negligible failure patterns in this dataset.\n",
      "Iteration 8, Moving Avg Return (last 3 iters): 89.36, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 across policy versions 6, 7, and 8 show highly consistent returns clustered tightly between 89.0 and 89.5, with episode lengths ranging from 105 to 110 steps. This suggests the agent reliably reaches the goal but does so with minimal margin, indicating it likely just barely escapes the valley each time. The most common failure pattern is not catastrophic failure but rather a lack of improvement: the agent consistently achieves near-identical returns and episode lengths, implying it has converged to a suboptimal but stable policy. This plateau may be due to conservative action choices that avoid risk but do not optimize for faster or higher-reward trajectories. There is no evidence of severe action missteps or erratic state transitions; instead, the agent’s actions are likely safe but not aggressive enough to reduce episode length or increase returns further.\n",
      "Iteration 9, Moving Avg Return (last 3 iters): 89.34, Success Rate: 0.00\n",
      "Failure Pattern: Across policy versions 7, 8, and 9 in the MountainCarContinuous-v0 environment, episode returns are tightly clustered between 89.0 and 89.5, with episode lengths ranging from 105 to 110 steps. This consistency suggests the agent reliably reaches the goal but does not optimize for faster completion or higher returns. The most common failure pattern is suboptimal efficiency: the agent often requires several extra steps (lengths >105), indicating it may struggle with momentum buildup or precise timing of acceleration near the hilltop. There are no catastrophic failures (very low returns or excessive episode lengths), but the agent's actions likely lack fine-tuned control, leading to repeated near-maximal but not optimal performance. The state characteristics at failure points likely involve insufficient velocity or poor action selection when approaching the goal, causing minor delays rather than major errors.\n",
      "Iteration 10, Moving Avg Return (last 3 iters): 89.28, Success Rate: 0.00\n",
      "Failure Pattern: Across policy versions 8, 9, and 10 in the MountainCarContinuous-v0 environment, episode returns are tightly clustered between 89.0 and 89.5, with episode lengths ranging from 105 to 110 steps. This consistency suggests the agent reliably reaches the goal but does so inefficiently, often taking close to the maximum allowed steps. The most common failure pattern is not catastrophic failure (e.g., not reaching the goal), but rather suboptimal trajectories—likely due to conservative or oscillatory actions that fail to build momentum efficiently up the hill. The lack of high variance in returns or episode lengths indicates the agent is not exploring risky or novel strategies, but is instead stuck in a local optimum where it solves the task slowly and predictably. Key issues likely include insufficient exploitation of the environment’s physics (e.g., not timing accelerations optimally at the bottom of the valley), leading to unnecessarily long episodes and plateaued returns.\n",
      "==== Running MountainCar-v0 ====\n",
      "Iteration 1, Moving Avg Return (last 3 iters): -173.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 show returns of -146.0 and -200.0, with episode lengths of 146 and 200 steps, respectively. The return of -200.0 with maximum episode length indicates the agent failed to reach the goal, likely oscillating near the valley without building enough momentum—common in early-stage policies. The -146.0 return suggests the agent succeeded but was inefficient, taking many steps to solve the task. These patterns point to frequent failures in generating sufficient left-right swings to escape the valley, suboptimal action selection (e.g., not timing accelerations well), and overall slow learning progress, as reflected by consistently negative returns and long episode durations.\n",
      "Iteration 2, Moving Avg Return (last 3 iters): -161.40, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 show that policy_version 1 frequently fails to reach the goal, as indicated by a maximum episode length of 200 and low returns (e.g., -200.0), suggesting the car gets stuck without escaping the valley. With policy_version 2, returns improve and episode lengths decrease, but there are still occasional failures with longer episodes and lower returns (e.g., -182.0, -171.0), indicating inconsistent policy performance. The most common failure pattern is insufficient momentum buildup, likely due to suboptimal action choices (e.g., not alternating left/right actions effectively), causing the car to stall near the bottom of the hill. Overall, failures are characterized by long episode lengths, returns near -200, and repeated inability to reach the goal state.\n",
      "Iteration 3, Moving Avg Return (last 3 iters): -160.47, Success Rate: 0.00\n",
      "Failure Pattern: Across the MountainCar-v0 episode summaries, the most common failure pattern is episodes ending with low returns (often near -200) and maximum lengths (200 steps), indicating the agent failed to reach the goal within the allowed time. This suggests persistent issues with insufficient momentum generation or suboptimal action selection, such as failing to alternate effectively between left and right actions to build speed. Returns improve with higher policy versions, but occasional regressions to -200 or high episode lengths persist, highlighting inconsistent policy learning. Overall, failures are characterized by prolonged episodes, poor reward accumulation, and likely stagnation in low-velocity states near the valley, reflecting inadequate exploration or exploitation strategies.\n",
      "Iteration 4, Moving Avg Return (last 3 iters): -159.33, Success Rate: 0.00\n",
      "Failure Pattern: Across the MountainCar-v0 episode summaries, the most common failure pattern is episodes reaching the maximum length of 200 steps with a return of -200, indicating the agent failed to reach the goal and terminated due to time limits. This suggests frequent issues with insufficient exploration or suboptimal action selection, such as repeatedly choosing ineffective actions that do not generate enough momentum to escape the valley. Returns cluster around -140 to -185 for episodes that end earlier, implying partial progress but inconsistent success. The persistence of high negative returns across policy versions highlights ongoing challenges in learning effective state transitions, particularly in overcoming the car’s initial low-power state and leveraging momentum to reach the goal.\n",
      "Iteration 5, Moving Avg Return (last 3 iters): -176.07, Success Rate: 0.00\n",
      "Failure Pattern: Across the MountainCar-v0 episode summaries, the most common failure pattern is episodes reaching the maximum length of 200 steps with a return of -200, indicating the agent failed to reach the goal and terminated due to time limits. These failures are distributed across all policy versions, suggesting persistent issues with exploration or insufficient acceleration to escape the valley. Returns in other episodes cluster between -130 and -185, showing partial progress but inconsistent success. The agent often gets stuck oscillating near the bottom of the hill, unable to build enough momentum, likely due to suboptimal action selection (e.g., not timing left/right actions effectively). Overall, the key issues are poor exploration, inadequate momentum generation, and frequent time-limit terminations, reflected in the return and episode length patterns.\n",
      "Iteration 6, Moving Avg Return (last 3 iters): -180.30, Success Rate: 0.00\n",
      "Failure Pattern: Across the MountainCar-v0 episode summaries, the most common failure pattern is episodes ending with the minimum return of -200.0 and maximum length of 200 steps, indicating the agent failed to reach the goal within the time limit. These failures occur across multiple policy versions, suggesting persistent issues with exploration or insufficient acceleration. Returns between -134.0 and -185.0 with shorter lengths show occasional success in reaching the goal, but inconsistent performance. The agent likely struggles with building momentum early, possibly due to suboptimal action selection (e.g., not alternating left/right actions to gain speed), resulting in frequent stalls near the starting position or slow progress up the hill. Overall, the main issues are insufficient exploration, poor action sequencing, and repeated time-limit terminations.\n",
      "Iteration 7, Moving Avg Return (last 3 iters): -177.78, Success Rate: 0.00\n",
      "Failure Pattern: Across the MountainCar-v0 episodes, the most common failure pattern is episodes ending with the minimum return of -200, indicating the agent failed to reach the goal within the time limit. These failures are distributed across multiple policy versions (notably versions 3, 4, 5, 6, and 7), suggesting persistent issues with policy improvement. Returns cluster around -140 to -180 for non-terminal failures, reflecting suboptimal but not catastrophic performance. The frequent occurrence of maximum episode lengths (200 steps) in failed runs points to ineffective exploration or insufficient momentum generation, likely due to poor action selection (e.g., not alternating left/right actions to build speed). Overall, the agent struggles with escaping local minima and generating enough force to reach the goal, with policy updates not consistently resolving these issues.\n",
      "Iteration 8, Moving Avg Return (last 3 iters): -164.73, Success Rate: 0.00\n",
      "Failure Pattern: Across the MountainCar-v0 episode summaries, the most common failure pattern is episodes ending with the minimum return of -200, indicating the agent failed to reach the goal within the maximum allowed steps. These failures are distributed across all policy versions, but notably persist even in later versions (5–8), suggesting insufficient policy improvement. Episodes with returns near -200 typically have maximum lengths (200 steps), showing the agent gets stuck in suboptimal states, likely oscillating near the valley without generating enough momentum to escape. Action issues may include repetitive or poorly timed accelerations, failing to exploit the environment’s dynamics. Returns in the -170 to -140 range suggest partial success, where the agent sometimes escapes but lacks consistency. Overall, the key failure pattern is an inability to build momentum early, resulting in repeated timeouts and poor returns.\n",
      "Iteration 9, Moving Avg Return (last 3 iters): -171.45, Success Rate: 0.00\n",
      "Failure Pattern: The most common failure pattern in these MountainCar-v0 episodes is the frequent occurrence of episodes ending with a return of -200 and a length of 200, indicating the agent failed to reach the goal within the maximum allowed steps. This failure is consistent across multiple policy versions (notably 3, 4, 5, 6, 7, 8, and 9), suggesting persistent issues with exploration or insufficient momentum-building strategies. Returns cluster around -170 to -140 in successful (non-failure) episodes, implying the agent sometimes manages to solve the task but often does so inefficiently, likely due to suboptimal action selection (e.g., not timing left/right accelerations well to build momentum). The repeated failures and high episode lengths point to a lack of consistent policy improvement, possibly caused by the agent getting stuck in local minima or failing to generalize across state space, especially in critical regions near the hilltops where precise action sequences are required.\n",
      "Iteration 10, Moving Avg Return (last 3 iters): -173.45, Success Rate: 0.00\n",
      "Failure Pattern: The most common failure pattern in these MountainCar-v0 episodes is the frequent occurrence of the maximum episode length (200 steps) with a corresponding return of -200, indicating the agent often fails to reach the goal within the time limit. This suggests persistent issues with either insufficient exploration or suboptimal action selection, causing the car to get stuck oscillating without building enough momentum to escape the valley. Returns cluster around -170 to -140 in successful episodes, but the repeated -200 returns across all policy versions highlight a recurring inability to solve the environment reliably. The failures are likely due to the agent not learning the critical state-action sequences needed to leverage the car’s momentum, resulting in inefficient or indecisive actions near the state boundaries.\n",
      "==== Running Pendulum-v1 ====\n",
      "Iteration 1, Moving Avg Return (last 3 iters): -867.97, Success Rate: 0.00\n",
      "Failure Pattern: The episode summary for Pendulum-v1 shows a low return (-867.97), indicating poor performance in keeping the pendulum upright and minimizing energy usage. Common failure patterns in such episodes typically include the pendulum spending significant time far from the upright position (large angles from vertical), frequent or excessive torque actions that do not stabilize the pendulum, and oscillatory or erratic control inputs. The consistent episode length (200 steps) suggests the agent survived the full duration but failed to achieve effective control, leading to sustained negative rewards throughout. Key issues are likely suboptimal action selection and inability to recover from unstable states.\n",
      "Iteration 2, Moving Avg Return (last 3 iters): -894.03, Success Rate: 0.00\n",
      "Failure Pattern: Across both Pendulum-v1 episodes, the most common failure patterns are characterized by consistently low returns (around -900), indicating the agent struggles to keep the pendulum upright and near its target angle. The episode lengths are always 200, suggesting the agent survives the full duration but fails to achieve effective control. Likely state issues include frequent large deviations from the upright position and high angular velocities. Action-wise, the agent may be applying insufficient or poorly timed torques, failing to counteract the pendulum’s momentum. Overall, the returns show no improvement between policy versions, highlighting persistent control and stabilization challenges.\n",
      "Iteration 3, Moving Avg Return (last 3 iters): -924.23, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the consistently low returns (ranging from -867.97 to -984.65) indicate persistent failure to keep the pendulum upright and stable. The episode lengths are all at the maximum (200 steps), suggesting the agent survives but fails to achieve effective control. Common failure patterns likely include the pendulum spending significant time far from the upright position (large angles), frequent application of suboptimal or oscillatory torques, and inability to reduce angular velocity. The worsening returns with increasing policy versions suggest either ineffective policy updates or overfitting, leading to degraded performance. Overall, the agent struggles with precise action selection and maintaining favorable state characteristics, resulting in poor cumulative rewards.\n",
      "Iteration 4, Moving Avg Return (last 3 iters): -1138.44, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episode summaries, the most common failure pattern is consistently low returns, ranging from -867.97 to -1510.57, indicating poor control over the pendulum’s upright position. The returns worsen with later policy versions, suggesting either ineffective policy updates or instability in learning. Each episode reaches the maximum length (200 steps), implying the agent fails to solve the task early and likely oscillates or swings erratically rather than stabilizing the pendulum. Typical state characteristics in such failures include the pendulum remaining far from vertical, with high angular velocity and frequent large deviations. Action issues likely involve excessive or poorly timed torque applications, leading to inefficient energy use and inability to counteract gravity effectively. Overall, the agent struggles to learn smooth, corrective actions, resulting in persistently negative returns.\n",
      "Iteration 5, Moving Avg Return (last 3 iters): -1334.87, Success Rate: 0.00\n",
      "Failure Pattern: Across the provided Pendulum-v1 episode summaries, the most common failure pattern is consistently low returns, ranging from approximately -868 to -1511, with all episodes reaching the maximum length of 200 steps—indicating the agent is not solving the task effectively. The returns worsen with later policy versions, suggesting either ineffective policy updates or overfitting. This pattern typically reflects poor control over the pendulum, likely characterized by frequent large deviations from the upright position (state characteristic) and possibly erratic or insufficiently tuned actions (action issue), such as excessive torque or failure to stabilize. Overall, the agent struggles to learn a stable policy, as evidenced by both the persistently negative returns and lack of improvement across versions.\n",
      "Iteration 6, Moving Avg Return (last 3 iters): -1504.85, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episode summaries, the most common failure pattern is consistently poor returns, with values ranging from -867 to -1510, indicating the agent struggles to keep the pendulum upright and near its target position. The returns worsen notably after policy version 3, suggesting either regression in policy quality or instability in training. All episodes have the maximum length (200 steps), implying the agent rarely solves the task early. Typical state failures likely involve the pendulum swinging far from vertical, with high angular velocity and large deviations from the upright position. Action issues may include insufficient or poorly timed torque applications, failing to counteract the pendulum's momentum. Overall, the agent exhibits persistent difficulty in stabilizing the pendulum, reflected in both the negative returns and lack of early termination.\n",
      "Iteration 7, Moving Avg Return (last 3 iters): -1448.94, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episode summaries, the most common failure pattern is consistently low returns, ranging from approximately -867 to -1510, with all episodes reaching the maximum length of 200 steps, indicating persistent difficulty in stabilizing the pendulum. The returns worsen notably after policy version 3, suggesting that later policies struggle more with maintaining upright balance. Typical state characteristics likely involve the pendulum swinging far from vertical, with high angular velocities and frequent large deviations from the target position. Action issues may include insufficient or poorly timed torque applications, leading to ineffective corrections and oscillatory or unstable behavior. Overall, the key failure points are inadequate control strategies resulting in persistent instability and high cumulative penalties.\n",
      "Iteration 8, Moving Avg Return (last 3 iters): -1225.57, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episode summaries, the most common failure pattern is consistently low returns, with all episodes yielding negative rewards and several episodes (policy versions 4–7) showing particularly poor performance (returns below -1300). This suggests the agent frequently fails to keep the pendulum upright and near its target angle, likely spending significant time in high-energy, unstable states. The abrupt drop in performance from policy version 3 to 4 indicates a possible regression in policy learning or instability in training. Action-wise, the agent may be applying excessive or poorly timed torques, leading to oscillations or overcorrections rather than smooth stabilization. The return pattern shows some recovery in later versions (notably version 8), but overall, the agent struggles with consistent control, likely due to insufficient exploration or suboptimal policy updates during training.\n",
      "Iteration 9, Moving Avg Return (last 3 iters): -1232.40, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 reveal consistently poor returns, with most episodes yielding returns between -839 and -1515, far from the optimal value (close to 0). The most common failure pattern is a lack of improvement or even degradation in performance as policy versions increase, particularly from version 4 onward, where returns drop sharply and remain low, indicating the agent struggles to learn effective control. This suggests persistent issues with either exploration or policy stability, possibly leading to erratic or suboptimal actions that fail to keep the pendulum upright. The uniform episode length (200 steps) implies the agent survives the full episode but accumulates high negative rewards, likely due to frequent large-angle deviations and excessive or poorly directed torque. Overall, the agent exhibits difficulty in stabilizing the pendulum, with action choices that do not correct the pendulum’s state effectively, resulting in consistently poor returns.\n",
      "Iteration 10, Moving Avg Return (last 3 iters): -1177.93, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episode summaries, the most common failure pattern is consistently low returns, with several episodes (policy_versions 4–6, 9, and 10) yielding returns below -1300, indicating poor control of the pendulum. These failures likely stem from the agent's inability to maintain the pendulum upright, resulting in frequent large negative rewards. Action issues may include insufficient torque application or erratic actions that fail to stabilize the pendulum, while state characteristics in these episodes likely involve the pendulum swinging far from the upright position with high angular velocities. Notably, some episodes (e.g., policy_versions 1, 2, 3, and 8) show improved returns (closer to -800), suggesting intermittent success in stabilizing the pendulum, but overall, the agent struggles with consistent control, reflected in the return patterns and persistent episode lengths at the maximum of 200 steps.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "changed part:\n",
    "1.Moving average is computed over the last ma_window **iterations** (not episodes). Convergence check only allowed when #iterations ≥ ma_window.\n",
    "2.If any episode gets truncated in an iteration, the next iteration discards the old policy and regenerates a brand new rule-based policy.\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import inspect\n",
    "import math\n",
    "\n",
    "# ----------------------------\n",
    "# LLM HTTP call function\n",
    "# ----------------------------\n",
    "LLM_URL = 'https://api.yesapikey.com/v1/chat/completions'\n",
    "LLM_HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer sk-PaTN85cFufotMmPm97Ae2546B0874aA29b6a86Ae069b7b4b'\n",
    "}\n",
    "\n",
    "def call_llm(prompt, model=\"gpt-4.1-2025-04-14\", temperature=0.2, max_tokens=1024):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(LLM_URL, json=data, headers=LLM_HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                resp_json = response.json()\n",
    "                if 'choices' in resp_json and resp_json['choices']:\n",
    "                    content = resp_json['choices'][0].get('message', {}).get('content')\n",
    "                    return content\n",
    "        except Exception as e:\n",
    "            print(\"LLM call exception:\", e)\n",
    "        time.sleep(2)\n",
    "\n",
    "# ----------------------------\n",
    "# Environment documentation mapping\n",
    "# ----------------------------\n",
    "ENV_DOC_URL = {\n",
    "    \"Acrobot-v1\": \"https://gymnasium.farama.org/environments/classic_control/acrobot/\",\n",
    "    \"CartPole-v1\": \"https://gymnasium.farama.org/environments/classic_control/cart_pole/\",\n",
    "    \"MountainCarContinuous-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\",\n",
    "    \"MountainCar-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car/\",\n",
    "    \"Pendulum-v1\": \"https://gymnasium.farama.org/environments/classic_control/pendulum/\"\n",
    "}\n",
    "\n",
    "def get_env_doc_url(env_id: str) -> str:\n",
    "    return ENV_DOC_URL.get(env_id, \"https://gymnasium.farama.org/\")\n",
    "\n",
    "# ----------------------------\n",
    "# Static Knowledge\n",
    "# ----------------------------\n",
    "STATIC_KNOWLEDGE = {\n",
    "    \"CartPole-v1\": {\n",
    "        \"state_dim\": 4,\n",
    "        \"state_vars\": [\"cart_position\", \"cart_velocity\", \"pole_angle\", \"pole_velocity\"],\n",
    "        \"state_ranges\": [(-4.8, 4.8), (-float(\"inf\"), float(\"inf\")), (-0.418, 0.418), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1],\n",
    "        \"reward_threshold\": 475,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"Acrobot-v1\": {\n",
    "        \"state_dim\": 6,\n",
    "        \"state_vars\": [\"cos_theta1\", \"sin_theta1\", \"cos_theta2\", \"sin_theta2\", \"theta1_dot\", \"theta2_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\")), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -100,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"MountainCar-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -110,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"MountainCarContinuous-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [-1.0, 1.0],\n",
    "        \"reward_threshold\": 90,\n",
    "        \"action_type\": \"continuous\"\n",
    "    },\n",
    "    \"Pendulum-v1\": {\n",
    "        \"state_dim\": 3,\n",
    "        \"state_vars\": [\"cos_theta\", \"sin_theta\", \"theta_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [-2.0, 2.0],\n",
    "        \"reward_threshold\": -200,\n",
    "        \"action_type\": \"continuous\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Knowledge module\n",
    "# ----------------------------\n",
    "class Knowledge:\n",
    "    def __init__(self):\n",
    "        self.static_knowledge = {}\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def load_static_knowledge(self, env_id):\n",
    "        if env_id not in STATIC_KNOWLEDGE:\n",
    "            raise ValueError(\"Unsupported environment\")\n",
    "        self.static_knowledge = STATIC_KNOWLEDGE[env_id]\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def add_dynamic_entry(self, entry):\n",
    "        self.dynamic_knowledge.append(entry)\n",
    "\n",
    "    def get_dynamic_guidance(self, env_id):\n",
    "        prompt = f\"\"\"\n",
    "I am generating a policy in environment {env_id}.\n",
    "Current dynamic knowledge entries: {self.dynamic_knowledge}\n",
    "\n",
    "Focus on environment **principles, physics, and dynamics**, not superficial patterns.\n",
    "Please provide concise heuristic suggestions for policy generation based on this knowledge, such as:\n",
    "- State ranges to prioritize\n",
    "- Common failing action patterns\n",
    "- Recommended threshold adjustments\n",
    "\n",
    "Return a short, structured bullet list (no prose).\n",
    "\"\"\"\n",
    "        guidance = call_llm(prompt)\n",
    "        return guidance\n",
    "\n",
    "# ----------------------------\n",
    "# Memory module\n",
    "# ----------------------------\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.episodes = []\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.episodes.append({\"steps\": [], \"summary\": None})\n",
    "\n",
    "    def add_step(self, s, a, r, done):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"Please call start_episode() before adding steps!\")\n",
    "        self.episodes[-1][\"steps\"].append({\"s\": s, \"a\": a, \"r\": r, \"done\": done})\n",
    "\n",
    "    def add_episode_summary(self, env_id, policy_version):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"No running episode!\")\n",
    "        steps = self.episodes[-1][\"steps\"]\n",
    "        total_reward = sum(step[\"r\"] for step in steps)\n",
    "        length = len(steps)\n",
    "        self.episodes[-1][\"summary\"] = {\n",
    "            \"env_id\": env_id,\n",
    "            \"policy_version\": policy_version,\n",
    "            \"return\": total_reward,\n",
    "            \"length\": length\n",
    "        }\n",
    "\n",
    "    def get_recent_episodes(self, n=5):\n",
    "        summaries = [ep[\"summary\"] for ep in self.episodes if ep[\"summary\"] is not None]\n",
    "        return summaries[-n:]\n",
    "\n",
    "# ----------------------------\n",
    "# Reflection module\n",
    "# ----------------------------\n",
    "class Reflection:\n",
    "    def __init__(self, knowledge: Knowledge):\n",
    "        self.knowledge = knowledge\n",
    "\n",
    "    def metrics(self, recent_episodes):\n",
    "        returns = [ep[\"return\"] for ep in recent_episodes]\n",
    "        lengths = [ep[\"length\"] for ep in recent_episodes]\n",
    "        avg_return = np.mean(returns) if returns else 0\n",
    "        avg_length = np.mean(lengths) if lengths else 0\n",
    "        threshold = self.knowledge.static_knowledge.get(\"reward_threshold\", 0)\n",
    "        success_count = sum(1 for ep in recent_episodes if ep[\"return\"] >= threshold)\n",
    "        success_rate = success_count / len(recent_episodes) if recent_episodes else 0\n",
    "        return {\"avg_return\": avg_return, \"avg_length\": avg_length, \"success_rate\": success_rate}\n",
    "\n",
    "    def failure_pattern(self, recent_episodes, env_id):\n",
    "        prompt = f\"\"\"\n",
    "I have the following {env_id} environment episode summaries: {recent_episodes}\n",
    "Please analyze the most common failure patterns, including state characteristics, action issues, and return patterns. Focus only on key points.\n",
    "Return a concise paragraph.\n",
    "\"\"\"\n",
    "        pattern = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"failure_pattern\": pattern})\n",
    "        return pattern\n",
    "\n",
    "    def edit_suggestion(self, recent_episodes, env_id):\n",
    "        prompt = f\"\"\"\n",
    "Based on recent episode data from environment {env_id}: {recent_episodes}\n",
    "Generate one policy editing suggestion in one of the following formats:\n",
    "- add_rule(condition -> action)\n",
    "- modify_threshold(variable, old_value, new_value)\n",
    "- reprioritize(rule_i over rule_j)\n",
    "\n",
    "Return exactly one line with one edit.\n",
    "\"\"\"\n",
    "        suggestion = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"edit_suggestion\": suggestion})\n",
    "        return suggestion\n",
    "\n",
    "# ----------------------------\n",
    "# Safe policy call\n",
    "# ----------------------------\n",
    "def safe_policy_call(state, policy_fn, sk):\n",
    "    try:\n",
    "        a = policy_fn(state)\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            a = sk[\"action_space\"][0]\n",
    "        else:\n",
    "            a = (sk[\"action_space\"][0] + sk[\"action_space\"][1]) / 2.0\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        a = np.clip(a, sk[\"action_space\"][0], sk[\"action_space\"][1])\n",
    "    return a\n",
    "\n",
    "# ----------------------------\n",
    "# Safe step wrapper\n",
    "# ----------------------------\n",
    "def safe_step(env, action):\n",
    "    sk = STATIC_KNOWLEDGE[env.unwrapped.spec.id]\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        action = np.array([np.clip(action, sk[\"action_space\"][0], sk[\"action_space\"][1])]) if np.isscalar(action) else np.clip(np.array(action), sk[\"action_space\"][0], sk[\"action_space\"][1])\n",
    "    return env.step(action)\n",
    "\n",
    "# ----------------------------\n",
    "# Policy generation helper\n",
    "# ----------------------------\n",
    "def _action_constraints_text(static_knowledge: dict) -> str:\n",
    "    a = static_knowledge[\"action_space\"]\n",
    "    if static_knowledge.get(\"action_type\") == \"discrete\":\n",
    "        return f\"Discrete actions; valid actions are exactly the integers in {a}.\"\n",
    "    else:\n",
    "        lo, hi = a[0], a[1]\n",
    "        return f\"Continuous action; return a single float within [{lo}, {hi}]. Clip if necessary.\"\n",
    "\n",
    "def generate_base_policy(env_id, knowledge: Knowledge):\n",
    "    guidance = knowledge.get_dynamic_guidance(env_id) or \"\"\n",
    "    doc_url = get_env_doc_url(env_id)\n",
    "    sk = knowledge.static_knowledge\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "    state_vars_text = \"\\n\".join([f\"- {name} in range {rng}\" for name, rng in zip(sk[\"state_vars\"], sk[\"state_ranges\"])])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are writing a deterministic, white-box **rule-based policy** for Gymnasium environment \"{env_id}\".\n",
    "Focus on **environment principles, physics, and dynamics**, not superficial patterns.\n",
    "The policy must be based on simple if-else statements or threshold comparisons using state variables.\n",
    "Environment documentation: {doc_url}\n",
    "\n",
    "Observation (state vector):\n",
    "{state_vars_text}\n",
    "\n",
    "Action constraints:\n",
    "- {action_desc}\n",
    "- May import 'math' if needed\n",
    "- Must be deterministic\n",
    "- Do not use loops, functions, or external libraries except math\n",
    "- Example: if state[2] > 0: return 1 else: return 0\n",
    "\n",
    "Dynamic guidance:\n",
    "{guidance}\n",
    "\n",
    "Output requirements:\n",
    "- Only one Python function: def policy(state): ...\n",
    "- No explanations, no markdown, no print\n",
    "- Returned action strictly satisfies constraints\n",
    "\"\"\"\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    policy_code = call_llm(prompt)\n",
    "    try:\n",
    "        exec(policy_code, local_vars)\n",
    "        policy_fn = local_vars.get(\"policy\")\n",
    "        if policy_fn is None:\n",
    "            if sk[\"action_type\"] == \"discrete\":\n",
    "                def policy_fn(state): return sk[\"action_space\"][0]\n",
    "            else:\n",
    "                lo, hi = sk[\"action_space\"]\n",
    "                def policy_fn(state): return (lo + hi)/2.0\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            def policy_fn(state): return sk[\"action_space\"][0]\n",
    "        else:\n",
    "            lo, hi = sk[\"action_space\"]\n",
    "            def policy_fn(state): return (lo + hi)/2.0\n",
    "    return policy_fn\n",
    "\n",
    "def apply_edit(policy_fn, edit_text, knowledge: Knowledge):\n",
    "    sk = knowledge.static_knowledge\n",
    "    try:\n",
    "        existing_src = inspect.getsource(policy_fn)\n",
    "    except Exception:\n",
    "        existing_src = \"def policy(state):\\n    return \" + (str(sk[\"action_space\"][0]) if sk[\"action_type\"]==\"discrete\" else str((sk[\"action_space\"][0]+sk[\"action_space\"][1])/2.0))\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "    doc_url = get_env_doc_url(knowledge.dynamic_knowledge[0].get(\"env_id\", \"\"))\n",
    "    prompt = f\"\"\"\n",
    "Revise deterministic, **rule-based** policy for environment at {doc_url}.\n",
    "Focus on physics, dynamics, and environment principles.\n",
    "Constraints: {action_desc}\n",
    "Current policy:\n",
    "{existing_src}\n",
    "Edit suggestion: {edit_text}\n",
    "You may use 'math' module.\n",
    "- Must remain if-else or threshold based\n",
    "\n",
    "Output only a valid Python function def policy(state): ...\n",
    "\"\"\"\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    policy_code = call_llm(prompt)\n",
    "    try:\n",
    "        exec(policy_code, local_vars)\n",
    "        new_policy_fn = local_vars.get(\"policy\")\n",
    "        return new_policy_fn if new_policy_fn else policy_fn\n",
    "    except Exception:\n",
    "        return policy_fn\n",
    "\n",
    "# ----------------------------\n",
    "# Main closed-loop training\n",
    "# ----------------------------\n",
    "def run_env_loop(env_id, max_iters=10, episodes_per_iter=10, ma_window=3, success_rate_threshold=0.8):\n",
    "    knowledge = Knowledge()\n",
    "    knowledge.load_static_knowledge(env_id)\n",
    "    memory = Memory()\n",
    "    reflection = Reflection(knowledge)\n",
    "    policy_version = 0\n",
    "    first_iter = True\n",
    "    policy_fn = None\n",
    "\n",
    "    iter_returns = []  # store each iteration's avg_return\n",
    "\n",
    "    for iter_idx in range(max_iters):\n",
    "        policy_version += 1\n",
    "        if first_iter:\n",
    "            policy_fn = generate_base_policy(env_id, knowledge)\n",
    "            first_iter = False\n",
    "        else:\n",
    "            # If previous iteration had truncation, generate new policy\n",
    "            recent_episodes = memory.get_recent_episodes()\n",
    "            if any(ep['length']==0 or ep.get(\"truncated\",False) for ep in recent_episodes):\n",
    "                policy_fn = generate_base_policy(env_id, knowledge)\n",
    "            else:\n",
    "                suggestion = reflection.edit_suggestion(recent_episodes, env_id)\n",
    "                policy_fn = apply_edit(policy_fn, suggestion, knowledge)\n",
    "\n",
    "        env = gym.make(env_id)\n",
    "        episode_rewards = []\n",
    "        truncation_occurred = False\n",
    "\n",
    "        for ep in range(episodes_per_iter):\n",
    "            s, _ = env.reset()\n",
    "            done = False\n",
    "            memory.start_episode()\n",
    "            while not done:\n",
    "                a = safe_policy_call(s, policy_fn, knowledge.static_knowledge)\n",
    "                s_next, r, terminated, truncated, info = safe_step(env, a)\n",
    "                if truncated:\n",
    "                    truncation_occurred = True\n",
    "                done = terminated or truncated\n",
    "                memory.add_step(s, a, r, done)\n",
    "                s = s_next\n",
    "            memory.add_episode_summary(env_id, policy_version)\n",
    "            episode_rewards.append(memory.episodes[-1][\"summary\"][\"return\"])\n",
    "            if truncation_occurred:\n",
    "                break\n",
    "\n",
    "        iter_avg_return = np.mean(episode_rewards)\n",
    "        iter_returns.append(iter_avg_return)\n",
    "        moving_avg_return = np.mean(iter_returns[-ma_window:])\n",
    "\n",
    "        recent_episodes_ma = memory.get_recent_episodes(n=episodes_per_iter*ma_window)\n",
    "        metrics = reflection.metrics(recent_episodes_ma)\n",
    "        print(f\"Iteration {iter_idx+1}, Moving Avg Return (last {ma_window} iters): {moving_avg_return:.2f}, Success Rate: {metrics['success_rate']:.2f}\")\n",
    "\n",
    "        pattern = reflection.failure_pattern(recent_episodes_ma, env_id)\n",
    "        print(\"Failure Pattern:\", pattern)\n",
    "\n",
    "        threshold = knowledge.static_knowledge.get(\"reward_threshold\", 0)\n",
    "        # Stop only if we have enough iterations for moving average, and success_rate is high enough\n",
    "        if iter_idx+1 >= ma_window and moving_avg_return >= threshold and metrics[\"success_rate\"] >= success_rate_threshold:\n",
    "            print(f\"Converged! Stop training. Moving Avg Return={moving_avg_return:.2f}, Success Rate={metrics['success_rate']:.2f}\")\n",
    "            break\n",
    "\n",
    "# ----------------------------\n",
    "# Run multiple control tasks\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    env_list = [\n",
    "        \"Acrobot-v1\",\n",
    "        \"CartPole-v1\",\n",
    "        \"MountainCarContinuous-v0\",\n",
    "        \"MountainCar-v0\",\n",
    "        \"Pendulum-v1\"\n",
    "    ]\n",
    "    for env_id in env_list:\n",
    "        print(f\"==== Running {env_id} ====\")\n",
    "        run_env_loop(env_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4685dea-0585-4079-873b-a3026520fd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_rl",
   "language": "python",
   "name": "dl_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
