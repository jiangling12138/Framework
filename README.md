# Framework

## **知识（Knowledge）**

系统在策略生成和优化过程中依赖的所有信息，包含**稳定的事实**和**可适应的经验**。

* **静态知识（Static Knowledge）**

长期稳定、很少变化的信息，用于定义环境、约束条件和策略的表示方式。

* **例子**：状态变量及其取值范围、动作定义、物理/动力学方程、白盒策略的语法和允许的编辑操作。

* **作用**：确保生成的策略**合法**且**可在环境中执行**。

* **动态知识（Dynamic Knowledge）**

在实验和性能反馈中不断更新的短期指导信息。

* **例子**：可能有效的动作模式、常见的失败情况、搜索方向调整、基于经验的性能优化建议。

* **作用**：引导优化过程，让 LLM 聚焦在最有可能提升策略性能的方向。

统一的知识体系可以保证策略生成**始终遵守静态约束**，同时**根据反馈动态改进**。

---

## **记忆模块（Memory，最小版本， 可拓展）**

**目的**：作为一个只追加、不修改的缓冲区，记录实际执行时发生的事情。

* **每一步记录（必需）**：

* `s`（状态）

* `a`（动作）

* `r`（奖励）

* `done`（是否结束，布尔值）

* **每个回合的总结（必需）**：

* `return`（总奖励）

* `length`（步数）

* **最少元数据（必需）**：

* `env_id`（环境标识）

* `policy_version`（策略版本）

---

## **反思模块（Reflection，最小版本， 可拓展）**

**目的**：计算基本指标，并给出一个小而可执行的策略改进建议。

* **输入**：

* 来自 **Memory** 的近期回合数据

* （可选）专家回合数据，用于对比

* **输出**：

* **指标**：`avg_return`（平均总奖励）、`avg_length`（平均步数）、`success_rate`（成功率，如适用）

* **问题定位**：最常见的失败模式（例如“当 θ ∈ \[0.1, 0.2] 时失败”或“执行 LEFT 动作后 20 步内终止”）

* **编辑建议**（文本，且受限于规则）：

* `add_rule(条件 -> 动作)`

* `modify_threshold(变量, 旧值, 新值)`

* `reprioritize(规则_i 优先于 规则_j)`



## 闭环Framework（包含以上三模块）


## 0. 初始化（一次性）

-   **Knowledge（静态+动态）**
    
    -   读取**静态知识**：状态/动作定义、物理约束、白盒策略语法与可编辑操作集合。
        
    -   初始化**动态知识**（可为空）：已有启发式、常见失败模式、搜索方向提示。
        
-   **目标设定**：性能门槛（如 `avg_return ≥ R*` 或成功率 ≥ X%）。
    

----------

## 1. 策略生成（使用 Knowledge）

-   **输入**：任务描述 + **Knowledge**
    
    -   使用静态知识保证**可执行与合规**；
        
    -   使用动态知识给出**搜索方向/启发式**（如优先关注某状态区间或阈值范围）。
        
-   **产出**：**白盒策略**（版本 `v_k`，Python/DSL）。
    

----------

## 2. 执行与记录（写入 Memory）

-   在环境中运行策略 `v_k`，采集若干回合数据。
    
-   **Memory（只追加）记录：**
    
    -   **每步**：`(s, a, r, done)`
        
    -   **每回合**：`return, length`
        
    -   **最少元数据**：`env_id, policy_version=v_k`
        

----------

## 3. 评估与诊断（调用 Reflection）

-   **输入**：从 **Memory** 取出近期回合（可选：专家回合）。
    
-   **Reflection 产出：**
    
    -   **指标**：`avg_return, avg_length, success_rate(可选)`
        
    -   **最常见失败模式**（one finding）
        
    -   **一个受限编辑建议**（one edit），形式限定在：
        
        -   `add_rule(条件 -> 动作)` 或
            
        -   `modify_threshold(变量, 旧值, 新值)` 或
            
        -   `reprioritize(规则_i over 规则_j)`
            

----------

## 4. 动态知识更新（回写 Knowledge）

-   将 **Reflection** 的“失败模式与编辑建议”写入**动态知识**：
    
    -   追加“经验条目”（如：在 `θ∈[0.1,0.2]` 区间易失败；阈值建议从 `0.20→0.15`）。
        
    -   动态知识仅影响**下一轮策略生成的搜索重点**，不改动静态约束。
        

----------

## 5. 策略微调（再次使用 Knowledge）

-   **输入**：当前策略 `v_k` + **Knowledge（含最新动态知识）** + **Reflection 的 edit**。
    
-   **产出**：应用**单一受限编辑**后的新策略 `v_{k+1}`（白盒可读）。
    

----------

## 6. 收敛判定与循环

-   若指标达标/收敛：**停止**并输出最终策略与日志。
    
-   否则：返回 **步骤 2**，继续迭代。
    

----------

## 数据与调用关系

-   **Knowledge → 策略生成**（静态保合规，动态给方向）
    
-   **策略执行 → 写 Memory**（只追加事实数据）
    
-   **Memory → Reflection**（读数据做评估与诊断）
    
-   **Reflection → Knowledge(动态)**（把“教训/启发”沉淀为可复用经验）
    
-   **Knowledge(含最新动态) + Reflection(edit) → 策略微调**（生成 `v_{k+1}`）
    
