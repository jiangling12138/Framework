{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bea4ab4-3145-423e-97f5-b4d719533dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Running Acrobot-v1 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 show a consistent failure pattern: each episode ends with the minimum possible return of -500.0 and the maximum episode length of 500 steps. This indicates that the agent repeatedly fails to achieve the environment's goal (swinging the end-effector above a target height) within the allowed time. The uniformity suggests the policy is ineffective, likely resulting in repetitive or uninformative actions (e.g., always choosing the same action or random actions with no learning). State characteristics likely remain in low-energy or unproductive regions, with the agent unable to generate sufficient momentum. The return pattern confirms persistent failure without any sign of improvement or exploration.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 consistently show a return of -500.0 and a maximum episode length of 500 steps, indicating that the agent repeatedly fails to solve the task within the allowed time. This pattern suggests the agent is unable to swing the end-effector to the required height, likely due to ineffective exploration or poor policy learning. The persistent minimum return and maximum length imply the agent may be stuck in suboptimal states, possibly oscillating near the starting configuration without generating sufficient momentum. Action selection is likely unvaried or misdirected, failing to exploit the environment's dynamics to achieve the goal. Overall, the key failure pattern is an inability to escape early states and achieve upward movement, resulting in consistently poor performance.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: All three Acrobot-v1 episodes terminated at the maximum length of 500 steps with the lowest possible return of -500.0, indicating that the agent consistently failed to solve the task. This pattern suggests the policy (version 3) is unable to swing the tip above the target line, likely resulting in repetitive or ineffective actions. The agent may be stuck in suboptimal states, such as low-energy oscillations or failing to generate sufficient momentum. The uniformity in returns and episode lengths points to a lack of exploration or learning, possibly due to poor action selection or inadequate state representation.\n",
      "=== Iteration 4 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 indicate a consistent failure pattern: each episode reaches the maximum length of 500 steps with a return of -500.0, suggesting the agent fails to solve the task and never achieves the goal state. This typically means the agent is either stuck in unproductive state trajectories—such as low or oscillating joint angles without sufficient upward momentum—or is taking ineffective or random actions that do not contribute to swinging the end-effector above the required threshold. The uniformity in returns and episode lengths points to a policy that is not learning or exploring effectively, likely repeating the same suboptimal behavior throughout each episode.\n",
      "=== Iteration 5 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes terminated at the maximum length of 500 steps with the minimum possible return of -500.0, indicating the agent consistently failed to solve the task. This pattern suggests the policy is unable to achieve the environment's goal (swinging the Acrobot to the target height). Likely failure causes include the agent remaining stuck in unproductive state regions (e.g., low energy, limited joint movement), repeatedly selecting ineffective or default actions, and lacking exploration or learning progress. The uniformity of poor returns and episode lengths points to a policy that is either untrained, poorly initialized, or not receiving useful feedback from the environment.\n",
      "=== Iteration 6 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 consistently show a return of -500.0 and a maximum episode length of 500 steps, indicating that the agent fails to solve the task in every episode and always reaches the time limit without success. This pattern suggests persistent failure to swing the acrobot to the goal state. Common failure characteristics likely include the agent remaining in suboptimal or stagnant states, possibly oscillating or failing to generate sufficient upward momentum. Action selection may be ineffective or repetitive, lacking the necessary exploration or torque to progress toward the goal. The uniformity in returns and episode lengths highlights a lack of learning or policy improvement at policy_version 6.\n",
      "=== Iteration 7 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 all show a return of -500.0 and a maximum episode length of 500 steps, indicating that the agent consistently fails to solve the task within the allowed time. This pattern suggests the agent is unable to swing the acrobot to the goal state, likely remaining in suboptimal or stagnant state regions (e.g., low vertical displacement or oscillating near the starting position). The repeated maximum negative return and episode length imply ineffective or random action selection, possibly due to poor policy learning or insufficient exploration. Overall, the key failure pattern is persistent inability to reach the goal, reflected in unchanging, minimal returns and episode lengths capped at the environment’s limit.\n",
      "=== Iteration 8 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 all show a return of -500.0 and a maximum episode length of 500 steps, indicating that the agent consistently fails to solve the task within the allowed time. This suggests the agent is unable to swing the acrobot to the target height, likely remaining in suboptimal states near the bottom of the state space. The repeated maximum negative return and episode length imply ineffective exploration or poor policy learning, possibly due to repetitive or uncoordinated actions that do not generate sufficient upward momentum. Overall, the key failure pattern is persistent stagnation in low-reward states with no successful episode completions.\n",
      "=== Iteration 9 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes terminated at the maximum length of 500 steps with the minimum possible return of -500.0, indicating the agent consistently failed to solve the task or reach the goal state. This pattern suggests the policy is unable to move the Acrobot to the target position, likely due to poor exploration or ineffective action selection—possibly repeating suboptimal or no-op actions. The state trajectories are likely confined to regions far from the goal, with little progress toward swinging the end-effector upward. The uniformity of returns and episode lengths highlights a lack of learning or adaptation in the current policy version.\n",
      "=== Iteration 10 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 all show a return of -500.0 and a maximum episode length of 500 steps, indicating that the agent consistently fails to solve the task and never achieves the goal state (swinging the end-effector above a target height). This pattern suggests the agent is either stuck in unproductive state trajectories (e.g., low-energy oscillations near the bottom) or repeatedly takes ineffective actions that do not build sufficient momentum. The uniformity in returns and episode lengths implies a lack of exploration or learning progress, possibly due to poor policy initialization or insufficient reward signal shaping.\n",
      "Consecutive truncated >= 10. Regenerating new policy via LLM.\n",
      "==== Running CartPole-v1 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: 381.67, Success Rate: 0.67\n",
      "Failure Pattern: In the provided CartPole-v1 episode summaries, two episodes reached the maximum return and length (500), indicating successful pole balancing throughout. However, one episode ended prematurely at a return and length of 145, suggesting a failure to maintain balance. The most common failure pattern appears to be occasional early termination, likely due to the pole falling or the cart moving out of bounds. This may be attributed to suboptimal action selection in certain states, such as delayed or incorrect corrective actions when the pole angle or cart position deviates from the center. Overall, while the policy is often effective, it sometimes fails to recover from destabilizing states, leading to shorter episodes and lower returns.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: 362.67, Success Rate: 0.67\n",
      "Failure Pattern: Among the three CartPole-v1 episodes, two achieved the maximum return and length (500), indicating successful pole balancing throughout. The single failure episode ended early with a return and length of 88, suggesting a loss of balance well before the maximum. This pattern points to rare but abrupt failures. Common failure characteristics likely include the pole angle or cart position exceeding safe thresholds, possibly due to delayed or incorrect action selection (e.g., choosing the wrong direction to move the cart when the pole is tilting). The return pattern shows that failures are infrequent but significant when they occur, with most episodes achieving perfect performance and occasional episodes ending prematurely due to a critical misstep in state-action response.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: 500.00, Success Rate: 1.00\n",
      "Failure Pattern: Based on the provided episode summaries, all episodes in the CartPole-v1 environment achieved the maximum return and length (500.0 and 500, respectively) with policy_version 3. This indicates that there were no observable failures in these runs: the agent consistently balanced the pole for the entire episode duration. As a result, there are no evident failure patterns in terms of state characteristics (e.g., pole angle or cart position deviations), action selection issues (e.g., suboptimal or erratic moves), or return patterns (e.g., early termination or low scores). The agent demonstrates stable and optimal performance across all episodes provided.\n",
      "Converged! Stop training. Moving Avg Return=500.00, Success Rate=1.00\n",
      "==== Running MountainCarContinuous-v0 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: -99.90, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 show a consistent failure pattern: each episode reaches the maximum length of 999 steps with a low return of approximately -99.9. This indicates that the agent repeatedly fails to reach the goal within the allowed steps, likely oscillating near the starting position without generating enough momentum to climb the hill. The uniformity in returns and episode lengths suggests the policy is either producing weak or nearly zero actions, or is stuck in a local minimum, unable to exploit the environment's dynamics. The key issues are insufficient exploration and ineffective action selection, preventing the agent from escaping the initial valley and achieving higher rewards.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: 25.08, Success Rate: 0.33\n",
      "Failure Pattern: The most common failure pattern in these MountainCarContinuous-v0 episodes is characterized by long episode lengths (e.g., 999 steps) and highly negative returns (e.g., -99.9), indicating the agent fails to reach the goal within the time limit. This suggests issues such as insufficient acceleration or poor exploration, where the agent may oscillate near the valley without building enough momentum to climb the hill. In contrast, successful episodes are much shorter (101–210 steps) and yield high positive returns (83.5–91.6), implying effective state transitions and well-timed, appropriately scaled actions. Overall, failures are marked by prolonged stagnation in low-potential states and suboptimal action selection, preventing goal achievement.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: 90.81, Success Rate: 1.00\n",
      "Failure Pattern: Across the provided MountainCarContinuous-v0 episode summaries, the returns are consistently high (around 90.6–90.93) with episode lengths between 108 and 112 steps, indicating the agent reliably reaches the goal but not in minimal time. Common failure patterns likely include suboptimal acceleration phases, where the agent may hesitate or oscillate near the valley before building enough momentum, reflecting inefficient state transitions around low velocities and mid-position states. Action issues may involve conservative or poorly timed thrusts, leading to longer episode durations. The return pattern suggests the agent avoids catastrophic failures but does not fully optimize for speed, hinting at a policy that prioritizes safety or stability over aggressive progress.\n",
      "Converged! Stop training. Moving Avg Return=90.81, Success Rate=1.00\n",
      "==== Running MountainCar-v0 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a maximum episode length of 200 steps, indicating the agent consistently fails to reach the goal. This pattern suggests the agent is likely stuck in suboptimal states, such as low velocity and position near the valley, without generating enough momentum to climb the hill. The repeated failure implies the policy is either selecting actions that do not alternate effectively between left and right to build speed, or is overly deterministic (e.g., always choosing the same action). The uniform poor returns and episode lengths highlight a lack of exploration or learning, with the agent unable to escape the initial state region and achieve the environment’s objective.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a length of 200, indicating that the agent consistently fails to reach the goal within the maximum allowed steps. This suggests a common failure pattern where the agent is either unable to generate sufficient momentum or is stuck in suboptimal states, likely oscillating near the bottom of the valley. The uniformity in returns and episode lengths points to ineffective exploration or poor action selection, such as repeatedly choosing actions that do not build enough speed to escape the valley. Overall, the agent’s policy fails to exploit the environment’s dynamics, resulting in persistent, unproductive behavior across episodes.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes in MountainCar-v0 terminated with the minimum return of -200.0 and maximum length of 200 steps, indicating the agent consistently failed to reach the goal. This pattern suggests the agent is likely stuck in suboptimal states, such as low velocity or insufficient momentum near the valley, and is unable to escape due to ineffective action choices (e.g., alternating or repeating actions without building enough speed). The uniform poor returns and episode lengths point to a persistent failure in exploration or policy learning, with the agent unable to exploit the environment dynamics to achieve positive progress.\n",
      "=== Iteration 4 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 consistently show a return of -200.0 and maximum episode length of 200 steps, indicating the agent repeatedly fails to reach the goal and terminates only when the time limit is reached. This pattern suggests the agent is likely stuck in suboptimal states, such as oscillating near the starting position or failing to build enough momentum to ascend the hill. Action selection may be ineffective, possibly alternating between left and right without strategic acceleration, resulting in no progress. The uniform negative return and episode length highlight a persistent inability to solve the environment, pointing to issues in exploration, policy learning, or reward signal utilization.\n",
      "=== Iteration 5 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a maximum episode length of 200 steps, indicating that the agent consistently fails to reach the goal within the allowed time. This pattern suggests the agent is likely getting stuck in local minima, such as oscillating near the bottom of the valley without building enough momentum to reach the flag. Common failure characteristics include insufficient exploration or suboptimal action selection, such as not alternating acceleration directions effectively to gain speed. The uniformity in returns and episode lengths highlights a persistent inability to solve the task, likely due to poor policy learning or inadequate state-action value estimation.\n",
      "=== Iteration 6 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a length of 200 steps, indicating the agent consistently fails to reach the goal within the maximum allowed steps. This pattern suggests the agent is likely stuck in suboptimal states, such as low velocity or insufficient position near the starting point, and is unable to generate enough momentum to climb the hill. Action selection issues may include repetitive or ineffective actions (e.g., alternating or sticking to a single direction) that do not exploit the environment's dynamics. The uniform negative return and episode length highlight a lack of learning progress or exploration, with the agent failing to escape the initial valley in every episode.\n",
      "=== Iteration 7 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and episode lengths of 200, indicating that the agent consistently fails to reach the goal within the maximum allowed steps. This pattern suggests the agent is likely getting stuck in local minima, such as oscillating near the bottom of the valley without building enough momentum to reach the flag. Common state characteristics in such failures include low velocities and positions near the center of the valley. Action issues may involve the agent choosing suboptimal or repetitive actions, such as not alternating acceleration directions effectively to gain momentum. The uniformity in returns and episode lengths highlights a persistent inability to escape the initial region, reflecting a lack of exploration or poor policy learning.\n",
      "=== Iteration 8 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes for policy_version 8 in the MountainCar-v0 environment terminated with the minimum return of -200.0 and the maximum episode length of 200 steps, indicating the agent consistently failed to reach the goal. This suggests a common failure pattern where the agent either remains stuck in local minima (e.g., at the bottom of the valley) or fails to build sufficient momentum to ascend the hill. The repeated poor returns imply ineffective exploration or suboptimal action selection, likely resulting in repetitive, non-progressive actions (such as oscillating without gaining speed). Overall, the agent exhibits persistent inability to escape the initial state region, with no evidence of successful strategies emerging across episodes.\n",
      "=== Iteration 9 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a length of 200 steps, indicating that the agent consistently fails to reach the goal within the maximum allowed steps. This pattern suggests that the agent is likely getting stuck in local minima, unable to build enough momentum to climb the hill. Common failure characteristics in such cases include the agent oscillating near the valley without effective use of left/right actions to gain speed, possibly due to a poor or untrained policy that does not exploit the environment's dynamics. The uniformity in returns and episode lengths further indicates a lack of exploration or learning progress, with the agent repeating the same ineffective behavior across episodes.\n",
      "=== Iteration 10 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes in the MountainCar-v0 environment terminated with the minimum return of -200.0 and the maximum episode length of 200 steps, indicating the agent consistently failed to reach the goal. This pattern suggests the agent is either stuck oscillating between low-velocity states near the valley or repeatedly taking ineffective actions (e.g., not building enough momentum by alternating left and right actions). The lack of variation in returns and lengths points to a persistent failure mode, likely due to poor exploration or a suboptimal policy that does not exploit the environment’s dynamics to escape the initial state region.\n",
      "Consecutive truncated >= 10. Regenerating new policy via LLM.\n",
      "==== Running Pendulum-v1 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: -1255.33, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the consistently low returns (ranging from -1128 to -1501) indicate that the agent frequently fails to keep the pendulum upright and stable, likely spending significant time in states with high angular displacement and velocity. The uniform episode lengths (200 steps) suggest the agent survives the full duration but does not improve control over time. Common failure patterns likely include oscillatory or erratic actions that do not counteract the pendulum’s swing effectively, resulting in persistent energy loss and poor stabilization. Overall, the agent struggles with precise torque application and maintaining near-vertical states, leading to suboptimal performance.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: -1439.05, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the consistently low returns (ranging from approximately -1316 to -1503) indicate frequent failure to keep the pendulum upright and stable, as optimal performance yields much higher (closer to zero) returns. The episodes all reach the maximum length of 200 steps, suggesting the agent is not terminating early but is persistently unable to recover from poor states. Common failure patterns likely include the pendulum swinging far from the upright position, with state characteristics such as high angular displacement and velocity. Action issues may involve insufficient or poorly timed torque applications, failing to counteract the pendulum's momentum. Overall, the agent's policy struggles to stabilize the pendulum, leading to sustained penalties throughout each episode.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: -1132.44, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the consistently low returns (ranging from approximately -974 to -1307) indicate frequent failure to keep the pendulum upright and stable, likely resulting in persistent large angles from the upright position. The fixed episode length of 200 suggests the agent is not terminating early due to success, but rather enduring the full duration with suboptimal control. Common failure patterns likely include poor action selection—such as insufficient or excessive torque—leading to oscillations or inability to recover from deviations. Overall, the agent struggles with precise stabilization, reflected in both the negative returns and lack of improvement across episodes.\n",
      "=== Iteration 4 ===\n",
      "Moving Avg Return: -1311.15, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episode summaries, the consistently low returns (ranging from approximately -1500 to -1100) indicate frequent failure to keep the pendulum upright and stable, a common challenge in this environment. The fixed episode length (200 steps) suggests the agent survives the full duration but struggles to minimize energy loss and maintain the desired upright state. Typical failure patterns likely include the pendulum swinging erratically or remaining far from vertical, with actions either overcorrecting or insufficiently counteracting gravity. These issues result in high negative rewards, reflecting poor control and inefficient stabilization throughout the episodes.\n",
      "=== Iteration 5 ===\n",
      "Moving Avg Return: -1192.91, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 5 show consistently low returns (ranging from -1317.57 to -1120.03) over the maximum episode length (200 steps), indicating persistent difficulty in maintaining the pendulum upright. This suggests common failure patterns such as the agent frequently allowing the pendulum to swing far from the upright position, leading to sustained high negative rewards. Action-wise, the policy likely produces insufficient or poorly timed torques, failing to correct the pendulum’s angle or velocity effectively. The uniform episode lengths imply the agent survives but cannot stabilize the pendulum, reflecting a lack of precise control rather than catastrophic early failures.\n",
      "=== Iteration 6 ===\n",
      "Moving Avg Return: -1130.48, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the consistently low returns (ranging from -1106.76 to -1157.97) indicate persistent failure to keep the pendulum upright and stable. The fixed episode length (200 steps) suggests the agent survives but does not achieve effective control. Common failure patterns likely include frequent large deviations from the upright position (high angle and velocity magnitudes), and suboptimal or oscillatory actions that fail to counteract the pendulum’s momentum. The policy (version 6) appears unable to learn smooth, corrective torque application, resulting in repeated swings and energy loss rather than stabilization, as reflected in the negative returns.\n",
      "=== Iteration 7 ===\n",
      "Moving Avg Return: -1077.58, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the most common failure patterns include consistently low returns (ranging from -1124.48 to -992.51), indicating the agent struggles to keep the pendulum upright and near its target state. The episode lengths are all at the maximum (200 steps), suggesting the agent does not terminate early but fails to achieve significant improvement within the allowed time. Typical state characteristics likely involve the pendulum swinging far from the upright position, with high angular velocities and large deviations from the desired angle. Action issues may include insufficient or poorly timed torque applications, leading to ineffective corrections and persistent instability. Overall, the agent’s policy (version 7) demonstrates limited control, reflected in the consistently poor return values and lack of successful stabilization.\n",
      "=== Iteration 8 ===\n",
      "Moving Avg Return: -1291.10, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes summarized, the most common failure patterns are reflected in consistently low returns (ranging from -1381 to -1141), indicating the agent frequently fails to keep the pendulum upright and stable. State characteristics likely involve the pendulum swinging far from the vertical position, with high angular velocities persisting throughout episodes. Action issues may include insufficient or poorly timed torque applications, preventing effective correction of the pendulum's angle. The uniform episode length (200 steps) suggests the agent survives the full duration but fails to achieve meaningful control, as evidenced by the negative returns clustering around -1300, a sign of repeated suboptimal behavior rather than catastrophic early failures.\n",
      "=== Iteration 9 ===\n",
      "Moving Avg Return: -1381.65, Success Rate: 0.00\n",
      "Failure Pattern: Across the three Pendulum-v1 episodes, all with policy_version 9 and maximum length (200 steps), the returns are consistently poor (ranging from -1150.6 to -1499.0), indicating the agent struggles to keep the pendulum upright and stable. The high negative returns suggest frequent large deviations from the upright position, likely with the pendulum swinging or oscillating rather than balancing. Common failure patterns likely include the agent applying insufficient or inconsistent torque, leading to poor correction of the pendulum's angle and velocity. The lack of improvement across episodes implies the policy is not effectively learning to counteract the pendulum's fall, possibly due to suboptimal action selection or inadequate exploration.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-inf\n",
      "=== Iteration 10 ===\n",
      "Moving Avg Return: -1533.29, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 10 show consistently low returns (ranging from approximately -1397 to -1742) over the maximum episode length of 200 steps, indicating persistent difficulty in maintaining the pendulum upright. Common failure patterns likely include the agent's inability to apply precise torque to counteract the pendulum's falling motion, resulting in oscillatory or erratic actions that fail to stabilize the system. The consistently negative returns suggest that the agent spends most of the episode with the pendulum far from the upright position, possibly swinging back and forth without effective correction. This points to issues in both state estimation (not accurately recognizing pendulum angle and velocity) and suboptimal action selection (applying insufficient or mistimed torques).\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-inf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "changed part:\n",
    "1.when dealing with the Pendulum task, won't get new policy if truncated.\n",
    "2.Add core physical formulas into STATIC_KNOWLEDGE to improve policy generation.\n",
    "3.Introduce weights for dynamic knowledge, and periodically filter out outdated or already-resolved entries.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import inspect\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "# ----------------------------\n",
    "# LLM HTTP call function\n",
    "# ----------------------------\n",
    "LLM_URL = 'https://api.yesapikey.com/v1/chat/completions'\n",
    "LLM_HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer sk-DygYzdGba7V5ggRwDf0d28B193D84c90Af2eE34b68C1C892'\n",
    "}\n",
    "\n",
    "def call_llm(prompt, model=\"gpt-4.1-2025-04-14\", temperature=0.2, max_tokens=1024):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(LLM_URL, json=data, headers=LLM_HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                resp_json = response.json()\n",
    "                if 'choices' in resp_json and resp_json['choices']:\n",
    "                    content = resp_json['choices'][0].get('message', {}).get('content')\n",
    "                    return content\n",
    "        except Exception as e:\n",
    "            print(\"LLM call exception:\", e)\n",
    "        time.sleep(2)\n",
    "\n",
    "# ----------------------------\n",
    "# Environment documentation mapping\n",
    "# ----------------------------\n",
    "ENV_DOC_URL = {\n",
    "    \"Acrobot-v1\": \"https://gymnasium.farama.org/environments/classic_control/acrobot/\",\n",
    "    \"CartPole-v1\": \"https://gymnasium.farama.org/environments/classic_control/cart_pole/\",\n",
    "    \"MountainCarContinuous-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\",\n",
    "    \"MountainCar-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car/\",\n",
    "    \"Pendulum-v1\": \"https://gymnasium.farama.org/environments/classic_control/pendulum/\"\n",
    "}\n",
    "\n",
    "def get_env_doc_url(env_id: str) -> str:\n",
    "    return ENV_DOC_URL.get(env_id, \"https://gymnasium.farama.org/\")\n",
    "\n",
    "# ----------------------------\n",
    "# Static Knowledge (with physics formulas)\n",
    "# ----------------------------\n",
    "STATIC_KNOWLEDGE = {\n",
    "    \"CartPole-v1\": {\n",
    "        \"state_dim\": 4,\n",
    "        \"state_vars\": [\"cart_position\", \"cart_velocity\", \"pole_angle\", \"pole_velocity\"],\n",
    "        \"state_ranges\": [(-4.8, 4.8), (-float(\"inf\"), float(\"inf\")), (-0.418, 0.418), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1],\n",
    "        \"reward_threshold\": 475,\n",
    "        \"action_type\": \"discrete\",\n",
    "        \"physics\": {\n",
    "            \"theta_accel\": \"theta_accel = g*sin(theta) + cos(theta)*(-F - m*l*theta_dot^2*sin(theta))/(M+m)\",\n",
    "            \"cart_accel\": \"x_accel = (F + m*l*(theta_dot^2*sin(theta) - theta_accel*cos(theta))) / (M+m)\"\n",
    "        }\n",
    "    },\n",
    "    \"Acrobot-v1\": {\n",
    "        \"state_dim\": 6,\n",
    "        \"state_vars\": [\"cos_theta1\", \"sin_theta1\", \"cos_theta2\", \"sin_theta2\", \"theta1_dot\", \"theta2_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\")), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -100,\n",
    "        \"action_type\": \"discrete\",\n",
    "        \"physics\": {\n",
    "            \"theta1_accel\": \"theta1_ddot = -(d2*(theta2_dot^2*sin(theta2)+g*sin(theta1+theta2)+...) ) / denominator\",\n",
    "            \"theta2_accel\": \"theta2_ddot = formula depends on theta1, theta1_dot, theta2, theta2_dot, torque\"\n",
    "        }\n",
    "    },\n",
    "    \"MountainCar-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -110,\n",
    "        \"action_type\": \"discrete\",\n",
    "        \"physics\": {\n",
    "            \"velocity_update\": \"v_next = v + 0.001*action - 0.0025*cos(3*position)\",\n",
    "            \"position_update\": \"pos_next = pos + v_next\"\n",
    "        }\n",
    "    },\n",
    "    \"MountainCarContinuous-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [-1.0, 1.0],\n",
    "        \"reward_threshold\": 90,\n",
    "        \"action_type\": \"continuous\",\n",
    "        \"physics\": {\n",
    "            \"velocity_update\": \"v_next = v + 0.001*force - 0.0025*cos(3*position)\",\n",
    "            \"position_update\": \"pos_next = pos + v_next\"\n",
    "        }\n",
    "    },\n",
    "    \"Pendulum-v1\": {\n",
    "        \"state_dim\": 3,\n",
    "        \"state_vars\": [\"cos_theta\", \"sin_theta\", \"theta_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [-2.0, 2.0],\n",
    "        \"reward_threshold\": -200,\n",
    "        \"action_type\": \"continuous\",\n",
    "        \"physics\": {\n",
    "            \"theta_accel\": \"theta_ddot = (-3*g/(2*l)*sin(theta + pi) + 3./(m*l^2)*torque)\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Knowledge module with timestamp, validity, weight\n",
    "# ----------------------------\n",
    "class Knowledge:\n",
    "    def __init__(self):\n",
    "        self.static_knowledge = {}\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def load_static_knowledge(self, env_id):\n",
    "        if env_id not in STATIC_KNOWLEDGE:\n",
    "            raise ValueError(\"Unsupported environment\")\n",
    "        self.static_knowledge = STATIC_KNOWLEDGE[env_id]\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def add_dynamic_entry(self, entry, weight=1.0):\n",
    "        entry_copy = entry.copy()\n",
    "        entry_copy.update({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"valid\": True,\n",
    "            \"weight\": weight\n",
    "        })\n",
    "        self.dynamic_knowledge.append(entry_copy)\n",
    "\n",
    "    def filter_dynamic_knowledge(self, consecutive_inactive=3):\n",
    "        # Mark entries invalid if they haven't appeared in last consecutive_inactive rounds\n",
    "        valid_entries = []\n",
    "        for entry in self.dynamic_knowledge:\n",
    "            if \"last_seen_iter\" not in entry:\n",
    "                entry[\"last_seen_iter\"] = 0\n",
    "            if entry[\"valid\"]:\n",
    "                valid_entries.append(entry)\n",
    "        self.dynamic_knowledge = valid_entries\n",
    "\n",
    "    def get_dynamic_guidance(self, env_id):\n",
    "        valid_entries = [e for e in self.dynamic_knowledge if e.get(\"valid\", True)]\n",
    "        prompt = f\"\"\"\n",
    "I am generating a policy in environment {env_id}.\n",
    "Current valid dynamic knowledge entries: {valid_entries}\n",
    "\n",
    "Focus on environment **principles, physics, and dynamics**, not superficial patterns.\n",
    "Please provide concise heuristic suggestions for policy generation based on this knowledge, such as:\n",
    "- State ranges to prioritize\n",
    "- Common failing action patterns\n",
    "- Recommended threshold adjustments\n",
    "\n",
    "Return a short, structured bullet list (no prose).\n",
    "\"\"\"\n",
    "        guidance = call_llm(prompt)\n",
    "        return guidance\n",
    "\n",
    "# ----------------------------\n",
    "# Memory module\n",
    "# ----------------------------\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.episodes = []\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.episodes.append({\"steps\": [], \"summary\": None})\n",
    "\n",
    "    def add_step(self, s, a, r, done):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"Please call start_episode() before adding steps!\")\n",
    "        self.episodes[-1][\"steps\"].append({\"s\": s, \"a\": a, \"r\": r, \"done\": done})\n",
    "\n",
    "    def add_episode_summary(self, env_id, policy_version):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"No running episode!\")\n",
    "        steps = self.episodes[-1][\"steps\"]\n",
    "        total_reward = sum(step[\"r\"] for step in steps)\n",
    "        length = len(steps)\n",
    "        self.episodes[-1][\"summary\"] = {\n",
    "            \"env_id\": env_id,\n",
    "            \"policy_version\": policy_version,\n",
    "            \"return\": total_reward,\n",
    "            \"length\": length\n",
    "        }\n",
    "\n",
    "    def get_recent_episodes(self, n=5):\n",
    "        summaries = [ep[\"summary\"] for ep in self.episodes if ep[\"summary\"] is not None]\n",
    "        return summaries[-n:]\n",
    "\n",
    "# ----------------------------\n",
    "# Reflection module\n",
    "# ----------------------------\n",
    "class Reflection:\n",
    "    def __init__(self, knowledge: Knowledge):\n",
    "        self.knowledge = knowledge\n",
    "\n",
    "    def metrics(self, recent_episodes):\n",
    "        returns = [ep[\"return\"] for ep in recent_episodes]\n",
    "        lengths = [ep[\"length\"] for ep in recent_episodes]\n",
    "        avg_return = np.mean(returns) if returns else 0\n",
    "        avg_length = np.mean(lengths) if lengths else 0\n",
    "        threshold = self.knowledge.static_knowledge.get(\"reward_threshold\", 0)\n",
    "        success_count = sum(1 for ep in recent_episodes if ep[\"return\"] >= threshold)\n",
    "        success_rate = success_count / len(recent_episodes) if recent_episodes else 0\n",
    "        return {\"avg_return\": avg_return, \"avg_length\": avg_length, \"success_rate\": success_rate}\n",
    "\n",
    "    def failure_pattern(self, recent_episodes, env_id, iter_idx=None):\n",
    "        prompt = f\"\"\"\n",
    "I have the following {env_id} environment episode summaries: {recent_episodes}\n",
    "Please analyze the most common failure patterns, including state characteristics, action issues, and return patterns. Focus only on key points.\n",
    "Return a concise paragraph.\n",
    "\"\"\"\n",
    "        pattern = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"failure_pattern\": pattern, \"env_id\": env_id, \"last_seen_iter\": iter_idx}, weight=1.0)\n",
    "        return pattern\n",
    "\n",
    "    def edit_suggestion(self, recent_episodes, env_id, iter_idx=None):\n",
    "        prompt = f\"\"\"\n",
    "Based on recent episode data from environment {env_id}: {recent_episodes}\n",
    "Generate one policy editing suggestion in one of the following formats:\n",
    "- add_rule(condition -> action)\n",
    "- modify_threshold(variable, old_value, new_value)\n",
    "- reprioritize(rule_i over rule_j)\n",
    "\n",
    "Return exactly one line with one edit.\n",
    "\"\"\"\n",
    "        suggestion = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"edit_suggestion\": suggestion, \"env_id\": env_id, \"last_seen_iter\": iter_idx}, weight=1.0)\n",
    "        return suggestion\n",
    "\n",
    "# ----------------------------\n",
    "# Safe policy call\n",
    "# ----------------------------\n",
    "def safe_policy_call(state, policy_fn, sk):\n",
    "    try:\n",
    "        a = policy_fn(state)\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            a = sk[\"action_space\"][0]\n",
    "        else:\n",
    "            a = (sk[\"action_space\"][0] + sk[\"action_space\"][1]) / 2.0\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        a = np.clip(a, sk[\"action_space\"][0], sk[\"action_space\"][1])\n",
    "    return a\n",
    "\n",
    "# ----------------------------\n",
    "# Safe step wrapper\n",
    "# ----------------------------\n",
    "def safe_step(env, action):\n",
    "    sk = STATIC_KNOWLEDGE[env.unwrapped.spec.id]\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        action = np.array([np.clip(action, sk[\"action_space\"][0], sk[\"action_space\"][1])]) if np.isscalar(action) else np.clip(np.array(action), sk[\"action_space\"][0], sk[\"action_space\"][1])\n",
    "    return env.step(action)\n",
    "\n",
    "# ----------------------------\n",
    "# Policy generation helper with rule-based constraint\n",
    "# ----------------------------\n",
    "def _action_constraints_text(static_knowledge: dict) -> str:\n",
    "    a = static_knowledge[\"action_space\"]\n",
    "    if static_knowledge.get(\"action_type\") == \"discrete\":\n",
    "        return f\"Discrete actions; valid actions are exactly the integers in {a}.\"\n",
    "    else:\n",
    "        lo, hi = a[0], a[1]\n",
    "        return f\"Continuous action; return a single float within [{lo}, {hi}]. Clip if necessary.\"\n",
    "\n",
    "def generate_base_policy(env_id, knowledge: Knowledge):\n",
    "    guidance = knowledge.get_dynamic_guidance(env_id) or \"\"\n",
    "    doc_url = get_env_doc_url(env_id)\n",
    "    sk = knowledge.static_knowledge\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "    state_vars_text = \"\\n\".join([f\"- {name} in range {rng}\" for name, rng in zip(sk[\"state_vars\"], sk[\"state_ranges\"])])\n",
    "    physics_text = \"\\n\".join([f\"- {k}: {v}\" for k,v in sk.get(\"physics\", {}).items()])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are writing a deterministic, white-box **rule-based policy** for Gymnasium environment \"{env_id}\".\n",
    "Focus on **environment principles, physics, and dynamics**, not superficial patterns.\n",
    "The policy must be based on simple if-else statements or threshold comparisons using state variables.\n",
    "Environment documentation: {doc_url}\n",
    "\n",
    "Observation (state vector):\n",
    "{state_vars_text}\n",
    "\n",
    "Physics references:\n",
    "{physics_text}\n",
    "\n",
    "Action constraints:\n",
    "- {action_desc}\n",
    "- May import 'math' if needed\n",
    "- Must be deterministic\n",
    "- Do not use loops, functions, or external libraries except math\n",
    "- Example: if state[2] > 0: return 1 else: return 0\n",
    "\n",
    "Dynamic guidance:\n",
    "{guidance}\n",
    "\n",
    "Output requirements:\n",
    "- Only one Python function: def policy(state): ...\n",
    "- No explanations, no markdown, no print\n",
    "- Returned action strictly satisfies constraints\n",
    "\"\"\"\n",
    "    policy_code = call_llm(prompt)\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    try:\n",
    "        exec(policy_code, local_vars)\n",
    "        policy_fn = local_vars.get(\"policy\")\n",
    "        if policy_fn is None:\n",
    "            if sk[\"action_type\"] == \"discrete\":\n",
    "                def policy_fn(state): return sk[\"action_space\"][0]\n",
    "            else:\n",
    "                lo, hi = sk[\"action_space\"]\n",
    "                def policy_fn(state): return (lo + hi)/2.0\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            def policy_fn(state): return sk[\"action_space\"][0]\n",
    "        else:\n",
    "            lo, hi = sk[\"action_space\"]\n",
    "            def policy_fn(state): return (lo + hi)/2.0\n",
    "    return policy_fn\n",
    "\n",
    "def apply_edit(policy_fn, edit_text, knowledge: Knowledge):\n",
    "    sk = knowledge.static_knowledge\n",
    "    try:\n",
    "        existing_src = inspect.getsource(policy_fn)\n",
    "    except Exception:\n",
    "        existing_src = \"def policy(state):\\n    return \" + (str(sk[\"action_space\"][0]) if sk[\"action_type\"]==\"discrete\" else str((sk[\"action_space\"][0]+sk[\"action_space\"][1])/2.0))\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "\n",
    "    doc_url = get_env_doc_url(knowledge.dynamic_knowledge[0].get(\"env_id\", \"\"))\n",
    "    prompt = f\"\"\"\n",
    "Revise deterministic, **rule-based** policy for environment at {doc_url}.\n",
    "Focus on physics, dynamics, and environment principles.\n",
    "Constraints: {action_desc}\n",
    "Current policy:\n",
    "{existing_src}\n",
    "Edit suggestion: {edit_text}\n",
    "You may use 'math' module.\n",
    "- Must remain if-else or threshold based\n",
    "\n",
    "Output only a valid Python function def policy(state): ...\n",
    "\"\"\"\n",
    "    policy_code = call_llm(prompt)\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    try:\n",
    "        exec(policy_code, local_vars)\n",
    "        new_policy_fn = local_vars.get(\"policy\")\n",
    "        return new_policy_fn if new_policy_fn else policy_fn\n",
    "    except Exception:\n",
    "        return policy_fn\n",
    "\n",
    "# ----------------------------\n",
    "# Main closed-loop training\n",
    "# ----------------------------\n",
    "def run_env_loop(env_id, max_iters=10, episodes_per_iter=10, ma_window=3,\n",
    "                 success_rate_threshold=0.8, rollback_window=3,\n",
    "                 truncated_threshold=10):\n",
    "    knowledge = Knowledge()\n",
    "    knowledge.load_static_knowledge(env_id)\n",
    "    memory = Memory()\n",
    "    reflection = Reflection(knowledge)\n",
    "    policy_version = 0\n",
    "    first_iter = True\n",
    "    policy_fn = None\n",
    "    best_policy_fn = None\n",
    "    best_metrics = {\"avg_return\": -np.inf, \"success_rate\": 0}\n",
    "    recent_metrics_history = []\n",
    "    consecutive_truncated = 0\n",
    "\n",
    "    for iter_idx in range(max_iters):\n",
    "        policy_version += 1\n",
    "        print(f\"=== Iteration {iter_idx+1} ===\")\n",
    "\n",
    "        if first_iter:\n",
    "            policy_fn = generate_base_policy(env_id, knowledge)\n",
    "            first_iter = False\n",
    "        else:\n",
    "            recent_episodes = memory.get_recent_episodes()\n",
    "            suggestion = reflection.edit_suggestion(recent_episodes, env_id, iter_idx)\n",
    "            policy_fn = apply_edit(policy_fn, suggestion, knowledge)\n",
    "\n",
    "        env = gym.make(env_id)\n",
    "        iteration_returns = []\n",
    "        iteration_truncated = False\n",
    "\n",
    "        for ep in range(episodes_per_iter):\n",
    "            s, _ = env.reset()\n",
    "            done = False\n",
    "            memory.start_episode()\n",
    "\n",
    "            while not done:\n",
    "                a = safe_policy_call(s, policy_fn, knowledge.static_knowledge)\n",
    "                s_next, r, terminated, truncated, info = safe_step(env, a)\n",
    "                if truncated:\n",
    "                    iteration_truncated = True\n",
    "                done = terminated or truncated\n",
    "                memory.add_step(s, a, r, done)\n",
    "                s = s_next\n",
    "\n",
    "            memory.add_episode_summary(env_id, policy_version)\n",
    "            iteration_returns.append(memory.episodes[-1][\"summary\"][\"return\"])\n",
    "\n",
    "        recent_episodes_ma = memory.get_recent_episodes(n=ma_window)\n",
    "        metrics = reflection.metrics(recent_episodes_ma)\n",
    "        print(f\"Moving Avg Return: {metrics['avg_return']:.2f}, Success Rate: {metrics['success_rate']:.2f}\")\n",
    "\n",
    "        # Update failure pattern\n",
    "        pattern = reflection.failure_pattern(recent_episodes_ma, env_id, iter_idx)\n",
    "        print(\"Failure Pattern:\", pattern)\n",
    "\n",
    "        # Filter dynamic knowledge periodically\n",
    "        knowledge.filter_dynamic_knowledge(consecutive_inactive=3)\n",
    "\n",
    "        # Update best policy if improved\n",
    "        if metrics[\"avg_return\"] > best_metrics[\"avg_return\"] and metrics[\"success_rate\"] >= success_rate_threshold:\n",
    "            best_metrics = metrics.copy()\n",
    "            best_policy_fn = policy_fn\n",
    "\n",
    "        recent_metrics_history.append(metrics[\"avg_return\"])\n",
    "        if len(recent_metrics_history) > rollback_window:\n",
    "            recent_metrics_history.pop(0)\n",
    "\n",
    "        # Check rollback: if consecutive rollback_window drops, restore best policy\n",
    "        if len(recent_metrics_history) == rollback_window:\n",
    "            if all(recent_metrics_history[i] < recent_metrics_history[i-1] for i in range(1, rollback_window)):\n",
    "                print(f\"Rollback triggered. Restoring previous best policy with Avg Return={best_metrics['avg_return']:.2f}\")\n",
    "                policy_fn = best_policy_fn\n",
    "\n",
    "        # Check convergence based on moving average over ma_window\n",
    "        if len(memory.episodes) >= ma_window * episodes_per_iter:\n",
    "            if metrics[\"avg_return\"] >= knowledge.static_knowledge.get(\"reward_threshold\", 0) and metrics[\"success_rate\"] >= success_rate_threshold:\n",
    "                print(f\"Converged! Stop training. Moving Avg Return={metrics['avg_return']:.2f}, Success Rate={metrics['success_rate']:.2f}\")\n",
    "                break\n",
    "\n",
    "        # Truncated handling: only trigger if consecutive_truncated >= threshold\n",
    "        # Truncated handling (skip for Pendulum-v1)\n",
    "        if env_id != \"Pendulum-v1\":\n",
    "            if iteration_truncated:\n",
    "                consecutive_truncated += 1\n",
    "                if consecutive_truncated >= truncated_threshold:\n",
    "                    print(f\"Consecutive truncated >= {truncated_threshold}. Regenerating new policy via LLM.\")\n",
    "                    policy_fn = generate_base_policy(env_id, knowledge)\n",
    "                    consecutive_truncated = 0\n",
    "            else:\n",
    "                consecutive_truncated = 0\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run multiple control tasks\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    env_list = [\n",
    "        \"Acrobot-v1\",\n",
    "        \"CartPole-v1\",\n",
    "        \"MountainCarContinuous-v0\",\n",
    "        \"MountainCar-v0\",\n",
    "        \"Pendulum-v1\"\n",
    "    ]\n",
    "    for env_id in env_list:\n",
    "        print(f\"==== Running {env_id} ====\")\n",
    "        run_env_loop(env_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2cfdd3-ebcc-4c86-b03c-7dc12b0d978c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485905d3-f96a-433c-a687-f101c3be4d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_rl",
   "language": "python",
   "name": "dl_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
