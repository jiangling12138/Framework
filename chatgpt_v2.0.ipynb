{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb49a43-0723-4a6f-bbe5-6ea5a7237b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Running CartPole-v1 ====\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "changed part:\n",
    "1.Chinese to English : there seems to be little difference in reward.\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# LLM HTTP call function\n",
    "# ----------------------------\n",
    "LLM_URL = 'https://api.yesapikey.com/v1/chat/completions'\n",
    "LLM_HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer ssk-DygYzdGba7V5ggRwDf0d28B193D84c90Af2eE34b68C1C892'\n",
    "}\n",
    "\n",
    "def call_llm(prompt, model=\"gpt-4.1-2025-04-14\", temperature=0.7, max_tokens=1024):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(LLM_URL, json=data, headers=LLM_HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                resp_json = response.json()\n",
    "                if 'choices' in resp_json and resp_json['choices']:\n",
    "                    content = resp_json['choices'][0].get('message', {}).get('content')\n",
    "                    return content\n",
    "        except Exception as e:\n",
    "            print(\"LLM call exception:\", e)\n",
    "        time.sleep(2)\n",
    "\n",
    "# ----------------------------\n",
    "# Knowledge module\n",
    "# ----------------------------\n",
    "class Knowledge:\n",
    "    def __init__(self):\n",
    "        self.static_knowledge = {}\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def load_static_knowledge(self, env_id):\n",
    "        env_map = {\n",
    "            \"Acrobot-v1\": {\"state_dim\": 6, \"action_space\": [0,1,2], \"reward_threshold\": -100},\n",
    "            \"CartPole-v1\": {\"state_dim\": 4, \"action_space\": [0,1], \"reward_threshold\": 475},\n",
    "            \"MountainCarContinuous-v0\": {\"state_dim\": 2, \"action_space\": [-1.0,1.0], \"reward_threshold\": 90},\n",
    "            \"MountainCar-v0\": {\"state_dim\": 2, \"action_space\": [0,1,2], \"reward_threshold\": -110},\n",
    "            \"Pendulum-v1\": {\"state_dim\": 3, \"action_space\": [-2.0,2.0], \"reward_threshold\": -200}\n",
    "        }\n",
    "        if env_id not in env_map:\n",
    "            raise ValueError(\"Unsupported environment\")\n",
    "        self.static_knowledge = env_map[env_id]\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def add_dynamic_entry(self, entry):\n",
    "        self.dynamic_knowledge.append(entry)\n",
    "\n",
    "    def get_dynamic_guidance(self, env_id):\n",
    "        prompt = f\"\"\"\n",
    "        I am generating a policy in environment {env_id}.\n",
    "        Current known dynamic knowledge: {self.dynamic_knowledge}\n",
    "        Please provide heuristic suggestions for policy generation based on this knowledge, such as:\n",
    "        - State ranges to prioritize\n",
    "        - Common failing action patterns\n",
    "        - Recommended threshold adjustments\n",
    "        Return in structured text.\n",
    "        \"\"\"\n",
    "        guidance = call_llm(prompt)\n",
    "        return guidance\n",
    "\n",
    "# ----------------------------\n",
    "# Memory module\n",
    "# ----------------------------\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.episodes = []\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.episodes.append({\"steps\": [], \"summary\": None})\n",
    "\n",
    "    def add_step(self, s, a, r, done):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"Please call start_episode() before adding steps!\")\n",
    "        self.episodes[-1][\"steps\"].append({\"s\": s, \"a\": a, \"r\": r, \"done\": done})\n",
    "\n",
    "    def add_episode_summary(self, env_id, policy_version):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"No running episode!\")\n",
    "        steps = self.episodes[-1][\"steps\"]\n",
    "        total_reward = sum(step[\"r\"] for step in steps)\n",
    "        length = len(steps)\n",
    "        self.episodes[-1][\"summary\"] = {\n",
    "            \"env_id\": env_id,\n",
    "            \"policy_version\": policy_version,\n",
    "            \"return\": total_reward,\n",
    "            \"length\": length\n",
    "        }\n",
    "\n",
    "    def get_recent_episodes(self, n=5):\n",
    "        summaries = [ep[\"summary\"] for ep in self.episodes if ep[\"summary\"] is not None]\n",
    "        return summaries[-n:]\n",
    "\n",
    "# ----------------------------\n",
    "# Reflection module\n",
    "# ----------------------------\n",
    "class Reflection:\n",
    "    def __init__(self, knowledge: Knowledge):\n",
    "        self.knowledge = knowledge\n",
    "\n",
    "    def metrics(self, recent_episodes):\n",
    "        returns = [ep[\"return\"] for ep in recent_episodes]\n",
    "        lengths = [ep[\"length\"] for ep in recent_episodes]\n",
    "        avg_return = np.mean(returns) if returns else 0\n",
    "        avg_length = np.mean(lengths) if lengths else 0\n",
    "        return {\"avg_return\": avg_return, \"avg_length\": avg_length}\n",
    "\n",
    "    def failure_pattern(self, recent_episodes, env_id):\n",
    "        prompt = f\"\"\"\n",
    "        I have the following {env_id} environment episode data: {recent_episodes}\n",
    "        Please analyze the most common failure patterns in detail, including state characteristics, action issues, and return patterns. Focus on the key points, minimize unimportant parts.\n",
    "        \"\"\"\n",
    "        pattern = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"failure_pattern\": pattern})\n",
    "        return pattern\n",
    "\n",
    "    def edit_suggestion(self, recent_episodes, env_id):\n",
    "        prompt = f\"\"\"\n",
    "        Based on the recent episode data from environment {env_id}: {recent_episodes}\n",
    "        Generate a policy editing suggestion. The format must be:\n",
    "        - add_rule(condition -> action)\n",
    "        - modify_threshold(variable, old_value, new_value)\n",
    "        - reprioritize(rule_i over rule_j)\n",
    "        \"\"\"\n",
    "        suggestion = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"edit_suggestion\": suggestion})\n",
    "        return suggestion\n",
    "\n",
    "# ----------------------------\n",
    "# Policy generation and refinement\n",
    "# ----------------------------\n",
    "def generate_base_policy(env_id, knowledge: Knowledge):\n",
    "    guidance = knowledge.get_dynamic_guidance(env_id)\n",
    "    prompt = f\"\"\"\n",
    "    Generate an initial executable white-box policy function in Python code:\n",
    "    def policy(state): ...\n",
    "    Environment: {env_id}\n",
    "    Action space: {knowledge.static_knowledge['action_space']}\n",
    "    Guidance: {guidance}\n",
    "    \"\"\"\n",
    "    policy_code = call_llm(prompt)\n",
    "    local_vars = {}\n",
    "    try:\n",
    "        exec(policy_code, {}, local_vars)\n",
    "        policy_fn = local_vars.get(\"policy\")\n",
    "        if policy_fn is None:\n",
    "            def policy_fn(state):\n",
    "                return random.choice(knowledge.static_knowledge['action_space'])\n",
    "    except:\n",
    "        def policy_fn(state):\n",
    "            return random.choice(knowledge.static_knowledge['action_space'])\n",
    "    return policy_fn\n",
    "\n",
    "def apply_edit(policy_fn, edit_text, knowledge: Knowledge):\n",
    "    \"\"\"Use LLM to rewrite the policy function according to edit_text\"\"\"\n",
    "    code_prompt = f\"\"\"\n",
    "    Existing policy function:\n",
    "    {policy_fn.__code__ if hasattr(policy_fn, '__code__') else 'def policy(state): pass'}\n",
    "    Editing suggestion: {edit_text}\n",
    "    Please output a complete executable Python function: def policy(state): ...\n",
    "    \"\"\"\n",
    "    policy_code = call_llm(code_prompt)\n",
    "    local_vars = {}\n",
    "    try:\n",
    "        exec(policy_code, {}, local_vars)\n",
    "        new_policy_fn = local_vars.get(\"policy\")\n",
    "        if new_policy_fn is None:\n",
    "            return policy_fn\n",
    "        return new_policy_fn\n",
    "    except:\n",
    "        return policy_fn\n",
    "\n",
    "# ----------------------------\n",
    "# Main closed-loop training logic\n",
    "# ----------------------------\n",
    "def run_env_loop(env_id, max_iters=50, episodes_per_iter=5):\n",
    "    knowledge = Knowledge()\n",
    "    knowledge.load_static_knowledge(env_id)\n",
    "    memory = Memory()\n",
    "    reflection = Reflection(knowledge)\n",
    "\n",
    "    policy_version = 0\n",
    "    first_iter = True\n",
    "\n",
    "    policy_fn = None\n",
    "\n",
    "    for iter_idx in range(max_iters):\n",
    "        policy_version += 1\n",
    "\n",
    "        if first_iter:\n",
    "            policy_fn = generate_base_policy(env_id, knowledge)\n",
    "            first_iter = False\n",
    "        else:\n",
    "            # Refine policy\n",
    "            recent_episodes = memory.get_recent_episodes()\n",
    "            suggestion = reflection.edit_suggestion(recent_episodes, env_id)\n",
    "            policy_fn = apply_edit(policy_fn, suggestion, knowledge)\n",
    "\n",
    "        env = gym.make(env_id)\n",
    "        episode_rewards = []\n",
    "\n",
    "        for ep in range(episodes_per_iter):\n",
    "            s, _ = env.reset()\n",
    "            done = False\n",
    "            memory.start_episode()\n",
    "\n",
    "            while not done:\n",
    "                a = policy_fn(s)\n",
    "                s_next, r, terminated, truncated, info = env.step(a)\n",
    "                done = terminated or truncated\n",
    "                memory.add_step(s, a, r, done)\n",
    "                s = s_next\n",
    "            memory.add_episode_summary(env_id, policy_version)\n",
    "            episode_rewards.append(memory.episodes[-1][\"summary\"][\"return\"])\n",
    "\n",
    "        recent_episodes = memory.get_recent_episodes()\n",
    "        metrics = reflection.metrics(recent_episodes)\n",
    "        print(f\"Iteration {iter_idx+1}, Avg Return: {metrics['avg_return']}\")\n",
    "\n",
    "        pattern = reflection.failure_pattern(recent_episodes, env_id)\n",
    "        print(\"Failure Pattern:\", pattern)\n",
    "\n",
    "        threshold = knowledge.static_knowledge.get(\"reward_threshold\", 0)\n",
    "        if metrics[\"avg_return\"] >= threshold:\n",
    "            print(f\"Convergence reached, stopping training, Avg Return={metrics['avg_return']}\")\n",
    "            break\n",
    "\n",
    "# ----------------------------\n",
    "# Run five control tasks\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    env_list = [\n",
    "        #\"Acrobot-v1\",\n",
    "        \"CartPole-v1\",\n",
    "        \"MountainCarContinuous-v0\",\n",
    "        \"MountainCar-v0\",\n",
    "        \"Pendulum-v1\"\n",
    "    ]\n",
    "    for env_id in env_list:\n",
    "        print(f\"==== Running {env_id} ====\")\n",
    "        run_env_loop(env_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844fbc14-1b47-424f-a214-880ede63ea26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_rl",
   "language": "python",
   "name": "dl_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
