{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c81a62-a335-4f94-b54f-5c3914495b27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert files found:\n",
      "  Acrobot-v1: experts/Acrobot-v1_best_expert.cleanrl_model\n",
      "  CartPole-v1: experts/CartPole-v1_best_expert.cleanrl_model\n",
      "  MountainCarContinuous-v0: experts/MountainCarContinuous-v0_best_expert.cleanrl_model\n",
      "  MountainCar-v0: experts/MountainCar-v0_best_expert.cleanrl_model\n",
      "  Pendulum-v1: experts/Pendulum-v1_best_expert.cleanrl_model\n",
      "Starting closed-loop runs (this may take long).\n",
      "\n",
      "==== Running Acrobot-v1 ====\n",
      "=== Iteration 1 ===\n",
      "[Info] loading expert from experts/Acrobot-v1_best_expert.cleanrl_model\n",
      "[Info] Model keys: ['network.0.weight', 'network.0.bias', 'network.2.weight', 'network.2.bias', 'network.4.weight', 'network.4.bias']\n",
      "[Info] Detected checkpoint type: DQN\n",
      "[Info] Expert loaded (dqn)\n",
      "[Selected idx 0 v1] MA Return=-500.00  SR=0.00  ExpertMean=-78.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_477/342666494.py:295: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(self.model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure Pattern: The Acrobot-v1 episode summaries indicate consistent failure, with all episodes reaching the maximum length of 500 steps and receiving the minimum possible return of -500.0. This pattern suggests the agent is unable to achieve the task objective (raising the end-effector above a target height) and is likely stuck in unproductive state trajectories, possibly oscillating or remaining in low-energy configurations. The agent’s actions may be ineffective, such as repeating the same or random actions without exploiting the environment’s dynamics. Compared to expert performance (mean return -62.0), the agent’s returns are drastically worse, highlighting a lack of learning or exploration. Overall, the key failure patterns are persistent inability to escape poor states, ineffective action selection, and no progress toward the goal.\n",
      "Edit Suggestion: add_rule(if steps_without_progress > 100 -> increase exploration rate)\n",
      "Replaced worst idx 0 with new v4: avg_return=-500.00\n",
      "=== Iteration 2 ===\n",
      "[Selected idx 0 v4] MA Return=-500.00  SR=0.00  ExpertMean=-71.00\n",
      "Failure Pattern: The Acrobot-v1 episodes all terminate at the maximum length of 500 steps with the lowest possible return of -500.0, indicating that the agent consistently fails to solve the task and is unable to swing the pendulum to the goal state. This persistent failure suggests the agent is likely stuck in suboptimal or repetitive state trajectories, possibly oscillating near the bottom without gaining sufficient momentum. Action selection appears ineffective, either due to poor exploration or a policy that fails to exploit the environment’s dynamics. In contrast, expert performance achieves a mean return of -86.0, highlighting a significant gap. The key failure pattern is the agent’s inability to escape low-reward states and make progress toward the goal, resulting in uniformly poor returns and episode lengths.\n",
      "Edit Suggestion: modify_threshold(max_steps, 500, 300)\n",
      "Replaced worst idx 0 with new v5: avg_return=-500.00\n",
      "=== Iteration 3 ===\n",
      "[Selected idx 0 v5] MA Return=-500.00  SR=0.00  ExpertMean=-63.00\n",
      "Failure Pattern: The Acrobot-v1 episodes consistently show poor performance, with all returns at -500.0 and maximum episode lengths of 500 steps, indicating the agent repeatedly fails to solve the task (i.e., it never reaches the goal state). Compared to expert performance (mean return -62.0), the agent's policy is significantly suboptimal. Common failure patterns likely include the agent getting stuck in unproductive state cycles, such as swinging without gaining sufficient momentum or failing to coordinate joint actions effectively. Action selection issues may involve repetitive or random moves that do not exploit the environment's dynamics, resulting in no progress toward the objective. The uniformity of the negative returns and episode lengths suggests a lack of learning or exploration, with the agent unable to escape poor trajectories or adapt its behavior.\n",
      "Edit Suggestion: add_rule(if episode_length == 500 and return == -500.0 -> increase exploration rate)\n",
      "Replaced worst idx 0 with new v6: avg_return=-500.00\n",
      "=== Iteration 4 ===\n",
      "[Selected idx 0 v6] MA Return=-500.00  SR=0.00  ExpertMean=-63.00\n",
      "Failure Pattern: The Acrobot-v1 episode summaries indicate consistent failure, with all episodes reaching the maximum length of 500 steps and the lowest possible return of -500.0, suggesting the agent never succeeded in solving the task (i.e., it failed to swing the end-effector above the target line). This contrasts sharply with expert performance (mean return -62.0), highlighting a significant performance gap. Common failure patterns likely include the agent remaining in suboptimal states, such as the pendulum hanging down or oscillating without gaining sufficient momentum. Action selection appears ineffective, possibly repeating unproductive or random actions rather than coordinated swings. The uniformity in episode length and return further suggests the policy is either stuck in a local minimum or not learning meaningful state-action associations.\n",
      "Edit Suggestion: add_rule(if episode_length >= 490 -> increase exploration rate)\n",
      "Replaced worst idx 0 with new v7: avg_return=-156.80\n",
      "=== Iteration 5 ===\n",
      "[Selected idx 0 v7] MA Return=-88.67  SR=1.00  ExpertMean=-62.00\n",
      "Failure Pattern: The episodes for Acrobot-v1 with policy_version 7 consistently yield returns between -87.0 and -92.0 over lengths of 88–93 steps, outperforming the expert mean return of -98.0. This suggests the agent reliably achieves the goal faster than the expert, but the returns remain negative, indicating frequent failures to reach the terminal state optimally. Common failure patterns likely include inefficient swinging or delayed momentum buildup, as Acrobot requires precise timing to elevate the end-effector. State characteristics may show repeated oscillations near the bottom, with actions possibly lacking the necessary torque direction changes to accelerate upward. The return pattern’s narrow range implies the agent is consistent but not yet exploiting optimal trajectories or action sequences to minimize episode length and maximize return.\n",
      "Edit Suggestion: modify_threshold(max_episode_length, 100, 80)\n",
      "Replaced worst idx 1 with new v8: avg_return=-500.00\n",
      "Converged! MA Return=-88.67, SR=1.00\n",
      "\n",
      "==== Running CartPole-v1 ====\n",
      "=== Iteration 1 ===\n",
      "[Info] loading expert from experts/CartPole-v1_best_expert.cleanrl_model\n",
      "[Info] Model keys: ['network.0.weight', 'network.0.bias', 'network.2.weight', 'network.2.bias', 'network.4.weight', 'network.4.bias']\n",
      "[Info] Detected checkpoint type: DQN\n",
      "[Info] Expert loaded (dqn)\n",
      "[Selected idx 0 v1] MA Return=500.00  SR=1.00  ExpertMean=500.00\n",
      "Failure Pattern: Based on the provided episode summaries and expert performance data, all episodes achieved the maximum possible return (500.0) and length (500), matching expert-level performance. This indicates no observable failure patterns in terms of episode termination, state instability, or suboptimal actions; the policy consistently maintained the pole upright for the full duration. There are no indications of problematic state characteristics (e.g., large pole angles or cart positions), nor any action selection issues (e.g., repeated missteps leading to early failure). The return pattern is uniformly optimal, suggesting robust and reliable policy behavior without common failure modes in these episodes.\n",
      "Edit Suggestion: add_rule(if pole angle > 0.05 then move left)\n",
      "Replaced worst idx 2 with new v4: avg_return=9.10\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "=== Iteration 2 ===\n",
      "[Selected idx 1 v2] MA Return=496.33  SR=1.00  ExpertMean=500.00\n",
      "Failure Pattern: The episode summaries for CartPole-v1 show that the agent achieves near-perfect or perfect returns (489.0, 500.0, 500.0), with two episodes reaching the environment's maximum length and return, matching expert performance. The only suboptimal episode ends at 489 steps, suggesting rare but possible failures just before the maximum. This indicates the agent is highly reliable, with failures likely due to occasional instability in balancing near the episode's end rather than persistent state or action issues. No consistent action selection errors or problematic state regions are evident, as performance is consistently optimal except for infrequent, late-episode terminations. Return patterns confirm that failures are rare and not systematic, aligning closely with expert-level behavior.\n",
      "Edit Suggestion: modify_threshold(max_episode_length, 500, 400)\n",
      "Replaced worst idx 2 with new v5: avg_return=472.50\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "=== Iteration 3 ===\n",
      "[Selected idx 2 v5] MA Return=258.67  SR=0.33  ExpertMean=500.00\n",
      "Failure Pattern: The episode summaries for CartPole-v1 show a mix of perfect (500.0 return, max length) and suboptimal episodes (returns of 90.0 and 186.0), indicating inconsistent policy performance compared to the expert's consistently optimal return of 500.0. The most common failure pattern is early episode termination, likely due to the pole falling or the cart moving out of bounds before reaching the maximum step limit. These failures suggest issues with the policy's ability to recover from destabilized states, possibly due to inadequate corrective actions when the pole angle or cart position deviates from the center. The variability in returns highlights that while the policy can sometimes match expert performance, it lacks robustness in handling challenging or off-nominal states.\n",
      "Edit Suggestion: modify_threshold(max_episode_length, 500, 300)\n",
      "Replaced worst idx 2 with new v6: avg_return=355.10\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "=== Iteration 4 ===\n",
      "[Selected idx 2 v6] MA Return=326.67  SR=0.33  ExpertMean=500.00\n",
      "Failure Pattern: The episode summaries for CartPole-v1 with policy_version 6 show variable performance, with returns of 148.0, 332.0, and a perfect 500.0 (matching expert mean). The most common failure pattern is early termination, as seen in episodes with returns of 148.0 and 332.0, suggesting instability in maintaining the pole upright for extended periods. These failures likely stem from state characteristics such as excessive pole angle or cart velocity, leading to loss of balance. Action issues may include inconsistent or delayed corrective actions, causing the environment to reach terminal states prematurely. Overall, while the policy can achieve expert-level performance, it exhibits inconsistency, with frequent episodes ending well before the maximum return, indicating areas for improvement in robustness and stability.\n",
      "Edit Suggestion: modify_threshold(max_pole_angle, 0.15, 0.10)\n",
      "Replaced worst idx 2 with new v7: avg_return=45.00\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "=== Iteration 5 ===\n",
      "[Selected idx 2 v7] MA Return=41.33  SR=0.00  ExpertMean=500.00\n",
      "Failure Pattern: The episodes for CartPole-v1 with policy_version 7 show consistently low returns (38.0, 45.0, 41.0) compared to expert performance (mean return 500.0), indicating early failures. The most common failure pattern is likely an inability to maintain pole balance for extended periods, possibly due to suboptimal action selection such as delayed or incorrect left/right moves when the pole deviates. State characteristics at failure likely include the pole angle exceeding safe thresholds or the cart moving too far from the center. The short episode lengths suggest the policy struggles with rapid corrections, leading to quick termination after brief stabilization.\n",
      "Edit Suggestion: modify_threshold(pole_angle_threshold, 0.2, 0.1)\n",
      "Replaced worst idx 2 with new v8: avg_return=495.00\n",
      "=== Iteration 6 ===\n",
      "[Selected idx 2 v8] MA Return=472.00  SR=0.67  ExpertMean=500.00\n",
      "Failure Pattern: Across the provided CartPole-v1 episode summaries, the agent consistently achieves near-optimal returns (416.0, 500.0, 500.0), with two episodes reaching the environment’s maximum score, matching expert deterministic performance (mean return 500.0). The most common failure pattern is a single episode ending before the maximum length (416 steps), suggesting occasional instability in balancing the pole. This likely arises from brief lapses in state estimation or suboptimal action selection, such as delayed or incorrect left/right moves when the pole angle or cart position approaches critical thresholds. Overall, failures are rare and typically occur when the agent cannot recover from rapidly compounding deviations in pole angle or cart velocity, leading to early termination.\n",
      "Edit Suggestion: modify_threshold(max_episode_length, 500, 600)\n",
      "Replaced worst idx 2 with new v9: avg_return=500.00\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "=== Iteration 7 ===\n",
      "[Selected idx 2 v9] MA Return=500.00  SR=1.00  ExpertMean=500.00\n",
      "Failure Pattern: Based on the provided episode summaries, all episodes achieved the maximum return of 500.0 with episode lengths of 500, indicating no failures occurred during evaluation; the agent consistently performed at expert level. There are no observable failure patterns in state characteristics or action selection, as the agent maintained balance throughout each episode. The return pattern is uniform and optimal, matching deterministic expert performance, suggesting robust policy stability and no evident weaknesses in handling the environment.\n",
      "Edit Suggestion: No policy edit needed: current policy matches expert performance.\n",
      "Replaced worst idx 1 with new v10: avg_return=500.00\n",
      "Converged! MA Return=500.00, SR=1.00\n",
      "\n",
      "==== Running MountainCarContinuous-v0 ====\n",
      "=== Iteration 1 ===\n",
      "[Info] loading expert from experts/MountainCarContinuous-v0_best_expert.cleanrl_model\n",
      "[Info] Model keys: ['actor_logstd', 'critic.0.weight', 'critic.0.bias', 'critic.2.weight', 'critic.2.bias', 'critic.4.weight', 'critic.4.bias', 'actor_mean.0.weight', 'actor_mean.0.bias', 'actor_mean.2.weight']\n",
      "[Info] Detected checkpoint type: PPO\n",
      "[Info] Expert loaded (ppo_policy)\n",
      "[Selected idx 0 v1] MA Return=92.86  SR=1.00  ExpertMean=45.64\n",
      "Failure Pattern: The episodes show consistently high returns (around 92.8–92.9), slightly exceeding the expert mean (90.67), and moderate episode lengths (149–159 steps), indicating successful completion but not optimal speed. Common failure patterns in MountainCarContinuous-v0 typically involve insufficient acceleration at early states (low position, low velocity), leading to delayed momentum buildup and longer episode lengths. Action issues often include suboptimal force application—either too weak to escape the valley quickly or too strong, causing inefficient oscillations. The return patterns suggest the policy reliably reaches the goal but could improve efficiency by better exploiting state transitions near the critical threshold for escape, minimizing unnecessary steps and maximizing reward.\n",
      "Edit Suggestion: modify_threshold(car_velocity_threshold, 0.07, 0.08)\n",
      "Replaced worst idx 2 with new v4: avg_return=90.99\n",
      "=== Iteration 2 ===\n",
      "[Selected idx 1 v2] MA Return=92.10  SR=1.00  ExpertMean=93.78\n",
      "Failure Pattern: The episodes show consistent returns (92.1) and lengths (79), slightly below expert performance (mean return ~93.74), indicating the policy reliably solves the task but with minor inefficiencies. Common failure patterns likely include suboptimal acceleration timing or insufficient exploitation of momentum, causing the car to reach the goal with less energy efficiency than the expert. State characteristics at failure points may involve the car stalling near the hilltop or oscillating before reaching the flag, while action issues could include conservative force application or delayed maximal thrust. The return pattern suggests the policy is stable but not fully optimized, missing out on the highest possible reward due to these subtle inefficiencies.\n",
      "Edit Suggestion: modify_threshold(return, 92.10000000000002, 91.2426745262842)\n",
      "Replaced worst idx 2 with new v5: avg_return=93.47\n",
      "=== Iteration 3 ===\n",
      "[Selected idx 2 v5] MA Return=92.96  SR=1.00  ExpertMean=91.09\n",
      "Failure Pattern: Across the provided MountainCarContinuous-v0 episodes, the agent consistently achieves near-expert returns (92.56–93.46 vs. expert mean 93.84), indicating competent policy performance. However, episode lengths (100–108 steps) suggest occasional inefficiency in reaching the goal compared to optimal trajectories. Common failure patterns likely include suboptimal acceleration near the hill's peak, insufficient momentum buildup in early states, or hesitancy in committing to high-magnitude actions when needed. These issues manifest as slightly lower returns and longer episode durations, implying the agent sometimes struggles with timing or magnitude of actions in critical states, especially when transitioning from low-velocity regions to the goal zone.\n",
      "Edit Suggestion: modify_threshold(return, 92.56118323724625, 93.7429631079253)\n",
      "Replaced worst idx 1 with new v6: avg_return=92.63\n",
      "Converged! MA Return=92.96, SR=1.00\n",
      "\n",
      "==== Running MountainCar-v0 ====\n",
      "=== Iteration 1 ===\n",
      "[Info] loading expert from experts/MountainCar-v0_best_expert.cleanrl_model\n",
      "[Info] Model keys: ['network.0.weight', 'network.0.bias', 'network.2.weight', 'network.2.bias', 'network.4.weight', 'network.4.bias']\n",
      "[Info] Detected checkpoint type: DQN\n",
      "[Info] Expert loaded (dqn)\n",
      "[Selected idx 0 v1] MA Return=-116.67  SR=0.00  ExpertMean=-117.00\n",
      "Failure Pattern: The episodes show consistently poor performance, with returns ranging from -113.0 to -124.0 and episode lengths matching the negative returns, indicating the agent typically takes one action per timestep until termination. This suggests the agent fails to reach the goal and instead hits the maximum step limit, a common failure in MountainCar-v0 due to insufficient momentum-building or suboptimal action selection (e.g., oscillating ineffectively or not coordinating left/right actions to escape the valley). Compared to the expert mean return of -117.0, the agent's performance is similar or slightly worse, reinforcing that it struggles with the environment's core challenge—strategically leveraging momentum. The lack of variation in returns and episode lengths further implies repetitive, non-adaptive behavior rather than learning from prior episodes.\n",
      "Edit Suggestion: modify_threshold(steps_limit, 200, 150)\n",
      "Replaced worst idx 1 with new v4: avg_return=-121.10\n",
      "=== Iteration 2 ===\n",
      "[Selected idx 1 v4] MA Return=-115.00  SR=0.00  ExpertMean=-117.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 4 show returns of -117.0, -115.0, and -113.0, each matching their episode lengths, indicating the agent consistently reaches the goal but takes nearly the maximum allowed steps. Compared to the expert mean return of -110.0, the agent is slightly less efficient. The most common failure pattern is slow progress up the mountain, likely due to suboptimal action selection—possibly hesitating or not building enough momentum early, which is critical in this environment. State-wise, the agent may be spending excessive time in low-velocity or low-position states near the valley, failing to exploit the environment’s physics for faster ascent. Overall, the agent is reliable but inefficient, with returns closely tracking episode length and lagging behind expert performance by a small but consistent margin.\n",
      "Edit Suggestion: modify_threshold(length, 117, 113)\n",
      "Replaced worst idx 2 with new v5: avg_return=-157.90\n",
      "=== Iteration 3 ===\n",
      "[Selected idx 2 v5] MA Return=-176.00  SR=0.00  ExpertMean=-110.00\n",
      "Failure Pattern: The provided MountainCar-v0 episodes, each with a return of -176.0 and length 176, indicate that the agent consistently fails to reach the goal within the maximum allowed steps, receiving the minimum reward per step. Compared to expert performance (mean return -94.0), the agent's returns are significantly worse, suggesting it struggles to build sufficient momentum or select effective actions to escape the valley. The repeated episode length and return pattern imply the agent may be stuck in suboptimal state-action loops, such as oscillating without gaining enough speed, rather than exploiting the environment's dynamics to reach the goal efficiently.\n",
      "Edit Suggestion: modify_threshold(max_steps, 176, 200)\n",
      "Replaced worst idx 2 with new v6: avg_return=-120.60\n",
      "=== Iteration 4 ===\n",
      "[Selected idx 2 v6] MA Return=-121.00  SR=0.00  ExpertMean=-115.00\n",
      "Failure Pattern: The episodes consistently show returns around -122 with lengths matching the return values, indicating the agent typically reaches the time limit without solving the task. This suggests a failure to generate sufficient momentum to reach the goal, likely due to suboptimal action selection—such as not coordinating left/right actions effectively to build speed. Compared to expert performance (mean return -91), the agent’s returns are significantly worse, highlighting inefficient exploration or poor exploitation of the environment’s dynamics. The most common failure pattern is stagnation near the bottom of the hill, with repeated low-reward actions and insufficient progress toward the goal state.\n",
      "Edit Suggestion: modify_threshold(position_threshold, -0.2, -0.1)\n",
      "Replaced worst idx 2 with new v7: avg_return=-200.00\n",
      "=== Iteration 5 ===\n",
      "[Selected idx 2 v7] MA Return=-200.00  SR=0.00  ExpertMean=-93.00\n",
      "Failure Pattern: The episodes all terminate at the maximum length of 200 steps with a return of -200.0, indicating that the agent consistently fails to reach the goal and receives the minimum reward per step. This suggests a persistent failure to escape the initial low-energy state, likely due to insufficient momentum buildup or poor action selection (e.g., oscillating ineffectively or not timing accelerations correctly). Compared to expert performance (mean return -109.0), the agent is far less efficient, highlighting a lack of effective exploration or exploitation of the environment’s dynamics. The key failure pattern is stagnation near the starting position, with actions that do not facilitate the necessary back-and-forth motion to climb the hill.\n",
      "Edit Suggestion: add_rule(position >= 0.5 -> action = stop)\n",
      "Replaced worst idx 2 with new v8: avg_return=-130.20\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "=== Iteration 6 ===\n",
      "[Selected idx 2 v8] MA Return=-121.67  SR=0.00  ExpertMean=-94.00\n",
      "Failure Pattern: The episodes show consistent returns around -121 to -122, indicating the agent typically reaches the goal just before the maximum step limit, reflecting inefficient exploration and suboptimal momentum usage. Compared to the expert mean return of -117.0, the agent’s performance is noticeably worse, suggesting frequent failures to build sufficient speed early or to time actions for climbing the hill effectively. The most common failure pattern is likely inadequate left-right oscillations to gain momentum, resulting in longer episode lengths and lower returns. Action selection may be too conservative or poorly timed, causing the agent to get stuck near the valley or ascend too slowly. Overall, the agent struggles with energy management and strategic action sequencing, leading to returns consistently below expert level.\n",
      "Edit Suggestion: modify_threshold(velocity_threshold, 0.04, 0.05)\n",
      "Replaced worst idx 2 with new v9: avg_return=-119.80\n",
      "=== Iteration 7 ===\n",
      "[Selected idx 2 v9] MA Return=-118.67  SR=0.00  ExpertMean=-109.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 9 show returns of -122.0, -116.0, and -118.0, each corresponding to episode lengths of 122, 116, and 118 steps, respectively. These returns are slightly worse than the expert mean of -113.0, indicating the agent consistently takes close to the maximum allowed steps to reach the goal, if it succeeds at all. Common failure patterns likely include insufficient momentum buildup on the slopes, leading to repeated oscillations near the valley without reaching the flag. Action selection may be suboptimal, with the agent possibly failing to coordinate left and right accelerations effectively to escape the gravitational pull. The consistently high episode lengths and negative returns suggest the agent struggles with timing and sequencing of actions, resulting in slow progress and inefficient trajectories compared to expert performance.\n",
      "Edit Suggestion: modify_threshold(length, 122, 110)\n",
      "Replaced worst idx 2 with new v10: avg_return=-117.50\n",
      "=== Iteration 8 ===\n",
      "[Selected idx 2 v10] MA Return=-116.33  SR=0.00  ExpertMean=-93.00\n",
      "Failure Pattern: The episodes show consistently suboptimal returns (ranging from -114.0 to -121.0), slightly worse than expert performance (-109.0), with episode lengths matching the returns, indicating the agent often reaches the goal near the time limit. This suggests a common failure pattern of slow progress up the mountain, likely due to insufficient momentum-building actions or poor timing in switching between left and right accelerations. The agent may struggle in low-velocity states near the valley, failing to exploit the environment's dynamics efficiently. Overall, the policy exhibits hesitation or indecision in critical states, leading to longer episodes and lower returns compared to the expert.\n",
      "Edit Suggestion: modify_threshold(position_threshold, 0.5, 0.48)\n",
      "Replaced worst idx 0 with new v11: avg_return=-121.10\n",
      "=== Iteration 9 ===\n",
      "[Selected idx 0 v11] MA Return=-119.33  SR=0.00  ExpertMean=-88.00\n",
      "Failure Pattern: The MountainCar-v0 episode summaries indicate that the agent consistently fails to reach the goal efficiently, as shown by episode returns of -114.0 and -122.0 (with episode lengths matching the negative returns), which are notably worse than the expert mean of -109.0. This suggests the agent often takes nearly the maximum allowed steps before termination, implying difficulty in building sufficient momentum to reach the hilltop. Common failure patterns likely include getting stuck oscillating near the bottom of the valley or failing to time accelerations correctly, resulting in suboptimal state transitions and inefficient action choices. The repeated high episode lengths and returns close to the time limit highlight persistent issues with exploration and energy accumulation, rather than isolated mistakes or randomness.\n",
      "Edit Suggestion: modify_threshold(max_steps, 200, 110)\n",
      "Replaced worst idx 0 with new v12: avg_return=-120.30\n",
      "=== Iteration 10 ===\n",
      "[Selected idx 0 v12] MA Return=-119.67  SR=0.00  ExpertMean=-109.00\n",
      "Failure Pattern: The episodes show consistently suboptimal returns (ranging from -115.0 to -122.0), notably worse than expert performance (-92.0), indicating the agent frequently fails to reach the goal efficiently. The episode lengths match the returns, suggesting the agent often takes the maximum steps allowed without escaping the valley, a common failure in MountainCar-v0. This pattern points to insufficient momentum-building, likely due to poor action selection—such as oscillating ineffectively or failing to coordinate left/right actions to gain enough speed. The agent’s state trajectory likely remains near the bottom of the hill, unable to exploit the environment’s dynamics, resulting in repeated, lengthy episodes with low returns.\n",
      "Edit Suggestion: modify_threshold(max_steps, 122, 113)\n",
      "Replaced worst idx 0 with new v13: avg_return=-125.50\n",
      "=== Iteration 11 ===\n",
      "[Selected idx 0 v13] MA Return=-97.00  SR=1.00  ExpertMean=-115.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 13 show returns of -91.0, -90.0, and -110.0, with corresponding episode lengths, indicating that the agent sometimes reaches the goal faster (shorter episodes, higher returns) but is inconsistent. Compared to the expert mean return of -109.0, the agent occasionally outperforms the expert but also matches its performance at times. The most common failure pattern is likely inconsistent momentum building: the agent sometimes fails to coordinate left/right actions effectively to escape the valley, leading to longer episodes and lower returns. This suggests suboptimal action selection, especially in critical low-velocity states near the bottom of the hill, resulting in inefficient progress toward the goal.\n",
      "Edit Suggestion: modify_threshold(length, 110, 90)\n",
      "Replaced worst idx 2 with new v14: avg_return=-119.90\n",
      "Converged! MA Return=-97.00, SR=1.00\n",
      "\n",
      "==== Running Pendulum-v1 ====\n",
      "=== Iteration 1 ===\n",
      "[Info] loading expert from experts/Pendulum-v1_best_expert.cleanrl_model\n",
      "[Info] Model keys: ['action_scale', 'action_bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc_mean.weight', 'fc_mean.bias', 'fc_logstd.weight', 'fc_logstd.bias']\n",
      "[Info] Detected checkpoint type: SAC\n",
      "[Info] Expert loaded (sac_actor)\n",
      "[Selected idx 0 v1] MA Return=-1429.38  SR=0.00  ExpertMean=-215.47\n",
      "Failure Pattern: The provided Pendulum-v1 episode summaries show consistently poor returns (ranging from -1512 to -1254) compared to expert performance (-113), indicating significant failure to keep the pendulum upright and stable. The episodes all reach the maximum length (200 steps), suggesting the agent does not terminate early but struggles throughout. Common failure patterns likely include the agent's inability to apply corrective torques effectively, resulting in the pendulum swinging wildly or remaining far from the upright position. State characteristics during these episodes probably feature large angular deviations and high velocities, while action issues may involve insufficient or poorly timed torque application. Overall, the agent demonstrates a lack of control precision and stability, leading to consistently low returns and failure to approach expert-level performance.\n",
      "Edit Suggestion: modify_threshold(action_torque_limit, current_value, increase_by_20_percent)\n",
      "Replaced worst idx 0 with new v4: avg_return=-1020.10\n",
      "=== Iteration 2 ===\n",
      "[Selected idx 0 v4] MA Return=-1142.78  SR=0.00  ExpertMean=-120.10\n",
      "Failure Pattern: The provided Pendulum-v1 episodes using policy_version 4 consistently yield low returns (ranging from -951 to -1493), which are substantially worse than expert performance (mean return ≈ -116). This indicates persistent failure to keep the pendulum upright and stable. The uniform episode length (200 steps) suggests the agent survives the full duration but accumulates high negative rewards, likely due to frequent large deviations from the upright position and possibly excessive or poorly timed torque actions. Common failure patterns likely include the pendulum swinging wildly or failing to recover from disturbances, with actions that do not effectively counteract the pendulum’s momentum, leading to sustained instability and high energy loss throughout each episode.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increased_value)\n",
      "Replaced worst idx 2 with new v5: avg_return=-1064.43\n",
      "=== Iteration 3 ===\n",
      "[Selected idx 1 v2] MA Return=-974.81  SR=0.00  ExpertMean=-221.37\n",
      "Failure Pattern: The episodes with policy_version 2 in the Pendulum-v1 environment consistently yield low returns (ranging from -836 to -1190), significantly worse than expert performance (mean return ≈ -119.6). This suggests common failure patterns such as poor stabilization of the pendulum near the upright position, likely due to suboptimal action selection—possibly excessive or misdirected torques that fail to counteract gravity effectively. The uniform episode length (200 steps) indicates the agent survives the full duration but accumulates high negative rewards, pointing to persistent deviations from the desired state rather than catastrophic failures. Overall, the agent struggles with precise control and maintaining favorable state characteristics, resulting in consistently poor returns compared to the expert.\n",
      "Edit Suggestion: modify_threshold(action_magnitude_limit, 2.0, 0.5)\n",
      "Replaced worst idx 0 with new v6: avg_return=-1147.98\n",
      "=== Iteration 4 ===\n",
      "[Selected idx 0 v6] MA Return=-1199.86  SR=0.00  ExpertMean=-128.29\n",
      "Failure Pattern: The provided Pendulum-v1 episodes, all using policy_version 6, consistently yield poor returns (ranging from -1491.83 to -1028.19), significantly worse than the expert mean of -121.39. This suggests the agent frequently fails to keep the pendulum upright and near its target angle, likely spending much time in states with large angular deviations and high velocities. The uniform episode length (200 steps) indicates the agent survives the full episode but accumulates negative rewards throughout, pointing to persistent suboptimal control rather than catastrophic failures. Action-wise, the policy likely produces insufficient or poorly timed torques, failing to counteract the pendulum's swing effectively. Overall, the main failure pattern is an inability to stabilize the pendulum, leading to consistently large negative returns.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increased_value)\n",
      "Replaced worst idx 0 with new v7: avg_return=-1147.63\n",
      "=== Iteration 5 ===\n",
      "[Selected idx 0 v7] MA Return=-1325.10  SR=0.00  ExpertMean=-126.87\n",
      "Failure Pattern: The provided Pendulum-v1 episode summaries show consistently poor returns (ranging from -628 to -1795), all significantly worse than expert performance (mean return ≈ -119). This suggests the policy struggles to stabilize the pendulum upright, likely failing to maintain low angular deviation and velocity. Common failure patterns include frequent large negative rewards, indicating the pendulum spends much time far from the upright position. Action-wise, the policy may be producing insufficient or poorly timed torques, unable to counteract gravity effectively. Overall, the episodes reflect a lack of control precision and stability, with the agent failing to achieve sustained upright states compared to the expert.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increased_value)\n",
      "Replaced worst idx 0 with new v8: avg_return=-1131.56\n",
      "=== Iteration 6 ===\n",
      "[Selected idx 0 v8] MA Return=-1041.85  SR=0.00  ExpertMean=-120.78\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 8 show consistently poor returns (ranging from -1504 to -671), far below expert performance (mean return ≈ -115). This suggests the policy struggles to keep the pendulum upright and near its target angle, likely resulting in frequent large negative rewards. Common failure patterns may include the pendulum swinging wildly or remaining far from the upright position, indicating poor state stabilization. Action issues likely involve insufficient or misdirected torque application, failing to counteract gravity effectively. The consistently long episode lengths (200 steps) imply the agent survives but does not improve the pendulum’s state, leading to sustained suboptimal returns rather than early termination.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increase_by_20_percent)\n",
      "Replaced worst idx 2 with new v9: avg_return=-1100.56\n",
      "=== Iteration 7 ===\n",
      "[Selected idx 2 v9] MA Return=-1284.99  SR=0.00  ExpertMean=-116.08\n",
      "Failure Pattern: The provided Pendulum-v1 episode summaries show consistently poor performance, with returns ranging from -1692 to -865, all significantly worse than the expert mean of -114.8. This suggests the policy struggles to keep the pendulum upright and stable, likely resulting in frequent large negative rewards associated with high angular displacement and velocity. The uniform episode length (200 steps) indicates the agent survives the full episode but fails to achieve effective control. Common failure patterns likely include the pendulum swinging erratically or remaining far from the upright position, with actions that are either too weak, too strong, or poorly timed, leading to inefficient energy use and inability to counteract the pendulum’s natural dynamics. Overall, the agent demonstrates inadequate stabilization and control compared to expert behavior.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increase_by_20_percent)\n",
      "Replaced worst idx 2 with new v10: avg_return=-1146.59\n",
      "=== Iteration 8 ===\n",
      "[Selected idx 2 v10] MA Return=-1218.64  SR=0.00  ExpertMean=-118.57\n",
      "Failure Pattern: The provided Pendulum-v1 episode summaries show consistently poor returns (ranging from -961.8 to -1501.6), which are significantly worse than expert performance (mean return ≈ -122.5). This suggests the policy struggles to keep the pendulum upright and stable, likely resulting in frequent large-angle deviations and high angular velocities—key state characteristics associated with low rewards in this environment. The uniform episode length (200 steps) indicates the agent survives the full episode but fails to improve the pendulum's position or velocity. Action-wise, the policy likely issues insufficient or poorly timed torques, failing to counteract the pendulum's momentum effectively. Overall, the main failure patterns are persistent instability, inadequate corrective actions, and consistently low returns compared to the expert.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increased_value)\n",
      "Replaced worst idx 2 with new v11: avg_return=-954.36\n",
      "=== Iteration 9 ===\n",
      "[Selected idx 2 v11] MA Return=-1231.90  SR=0.00  ExpertMean=-215.16\n",
      "Failure Pattern: The episodes from Pendulum-v1 with policy_version 11 consistently show poor performance, as indicated by low returns (ranging from -1479 to -862) compared to expert performance (mean return ≈ -119). The most common failure patterns likely include the agent's inability to keep the pendulum upright, resulting in persistent high negative rewards. State characteristics suggest frequent visits to states with large angles and velocities, far from the upright position. Action issues may involve insufficient or poorly timed torque applications, failing to counteract the pendulum's momentum effectively. Overall, the agent's policy struggles to stabilize the pendulum, leading to consistently suboptimal returns and long episode lengths without meaningful recovery.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increased_value)\n",
      "Replaced worst idx 2 with new v12: avg_return=-976.30\n",
      "=== Iteration 10 ===\n",
      "[Selected idx 2 v12] MA Return=-1299.45  SR=0.00  ExpertMean=-0.35\n",
      "Failure Pattern: The episodes from the Pendulum-v1 environment using policy version 12 consistently show poor performance, with returns ranging from approximately -964 to -1506 over the maximum episode length of 200 steps, indicating persistent failure to stabilize the pendulum. Compared to the expert mean return of -0.90, these results suggest the policy frequently fails to keep the pendulum upright and near its target angle. Common failure patterns likely include the pendulum spending extended periods far from the upright position, resulting in high negative rewards, and possibly erratic or insufficiently corrective actions that do not counteract the pendulum’s momentum. The lack of improvement across episodes suggests the policy struggles with both state estimation and action selection, failing to learn effective control strategies.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increased_value)\n",
      "Replaced worst idx 2 with new v13: avg_return=-1149.59\n",
      "=== Iteration 11 ===\n",
      "[Selected idx 2 v13] MA Return=-1360.10  SR=0.00  ExpertMean=-229.58\n",
      "Failure Pattern: The provided Pendulum-v1 episodes, all using policy version 13, consistently yield poor returns (ranging from -1494.45 to -1149.20), far below expert performance (mean return ≈ -124). This suggests the policy struggles to keep the pendulum upright and stable, likely resulting in frequent large-angle deviations and high angular velocities—states heavily penalized in this environment. The uniform episode lengths (200 steps) indicate the agent survives the full episode but fails to achieve meaningful control. Action-wise, the policy likely produces insufficient or poorly timed torques, failing to counteract the pendulum’s swing and stabilize it near the upright position. Overall, the main failure patterns are persistent instability, lack of corrective action, and consistently poor cumulative rewards.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increased_value)\n",
      "Replaced worst idx 2 with new v14: avg_return=-1025.39\n",
      "=== Iteration 12 ===\n",
      "[Selected idx 2 v14] MA Return=-909.37  SR=0.00  ExpertMean=-120.70\n",
      "Failure Pattern: The provided Pendulum-v1 episodes show consistently poor returns (ranging from -504.7 to -1153.9) compared to expert performance (mean return: -218.3), indicating significant suboptimality. All episodes reach the maximum length (200 steps), suggesting the policy fails to stabilize the pendulum upright within the episode. Common failure patterns likely include the agent's inability to apply precise torque to counteract gravity, resulting in frequent oscillations or the pendulum remaining far from the upright position. Action selection may be erratic or insufficiently targeted, leading to high energy loss and poor control. The wide range in returns also hints at inconsistent policy behavior, possibly due to unstable learning or inadequate exploration of effective state-action pairs.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increased_value)\n",
      "Replaced worst idx 0 with new v15: avg_return=-796.70\n",
      "=== Iteration 13 ===\n",
      "[Selected idx 0 v15] MA Return=-1335.44  SR=0.00  ExpertMean=-119.14\n",
      "Failure Pattern: The episodes from Pendulum-v1 show consistently poor returns (ranging from -1424 to -1277), far below expert performance (-0.23), indicating the policy struggles to keep the pendulum upright and minimize angular velocity. The uniform episode length (200 steps) suggests the agent survives the full duration but fails to achieve stable control, likely oscillating or swinging erratically. Common failure patterns likely include frequent large deviations from the upright position, high angular velocities, and suboptimal or inconsistent torque actions that do not correct the pendulum’s state effectively. Overall, the policy’s actions do not sufficiently counteract the pendulum’s dynamics, resulting in persistent high penalties and low returns.\n",
      "Edit Suggestion: modify_threshold(torque_limit, 2.0, 3.0)\n",
      "Replaced worst idx 0 with new v16: avg_return=-1141.92\n",
      "=== Iteration 14 ===\n",
      "[Selected idx 0 v16] MA Return=-773.13  SR=0.00  ExpertMean=-2.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 16 show consistently suboptimal returns (ranging from -1082.91 to -488.31), all significantly worse than expert performance (mean return -215.38). This suggests frequent failure to keep the pendulum upright and stable, likely resulting in large negative rewards. The uniform episode length (200 steps) indicates the agent survives the full episode but fails to achieve effective control. Common failure patterns likely include poor action selection—such as insufficient or erratic torque application—leading to persistent oscillations or inability to recover from deviations. State characteristics during failures probably involve the pendulum spending extended periods far from the upright position, with high angular velocities and large angles from vertical, reflecting a lack of stabilization and control precision compared to the expert.\n",
      "Edit Suggestion: modify_threshold(action_magnitude_limit, current_value, increase_by_20_percent)\n",
      "Replaced worst idx 1 with new v17: avg_return=-1068.70\n",
      "=== Iteration 15 ===\n",
      "[Selected idx 1 v17] MA Return=-1103.28  SR=0.00  ExpertMean=-118.45\n",
      "Failure Pattern: The provided Pendulum-v1 episode summaries show consistently poor returns (ranging from -1471 to -824), all significantly worse than expert performance (mean return ≈ -119). This suggests the policy struggles to keep the pendulum upright and stable, likely resulting in frequent large-angle deviations and high angular velocities—key failure states in this environment. The uniform episode length (200 steps) indicates the agent survives the full episode but accumulates penalties throughout, pointing to persistent suboptimal control rather than catastrophic failures. Action-wise, the policy may be producing insufficient or poorly timed torques, failing to counteract gravity effectively. Overall, the main failure pattern is an inability to maintain the pendulum near the upright position, leading to consistently high negative returns.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increase_by_20_percent)\n",
      "Replaced worst idx 1 with new v18: avg_return=-1162.37\n",
      "=== Iteration 16 ===\n",
      "[Selected idx 1 v18] MA Return=-1249.64  SR=0.00  ExpertMean=-120.19\n",
      "Failure Pattern: The provided Pendulum-v1 episode summaries show consistently poor returns (ranging from -970 to -1595), far worse than expert performance (mean return ≈ -118). This suggests the policy struggles to keep the pendulum upright and stable, likely resulting in frequent large-angle deviations and high angular velocities—common failure states in this environment. The uniform episode length (200 steps) indicates the agent survives the full episode but accumulates substantial negative reward, pointing to persistent suboptimal actions rather than catastrophic failures. Key issues likely include imprecise or poorly timed torque applications, leading to oscillations or inability to counteract gravity effectively, and a lack of fine control near the upright position.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increased_value)\n",
      "Replaced worst idx 1 with new v19: avg_return=-1189.11\n",
      "=== Iteration 17 ===\n",
      "[Selected idx 1 v19] MA Return=-1301.36  SR=0.00  ExpertMean=-224.60\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 show consistently poor returns (ranging from -1023 to -1498), all significantly worse than expert performance (mean return ≈ -263). This suggests the policy frequently fails to keep the pendulum upright and near its target angle, likely resulting in large negative rewards throughout each episode. The uniform episode length (200 steps) indicates the agent survives the full duration but does not improve its state, possibly oscillating or swinging wildly rather than stabilizing. Common failure patterns likely include poor action selection—such as insufficient or excessive torque—leading to persistent deviation from the upright position. Overall, the policy exhibits a lack of control and precision, with returns clustering far below expert level, highlighting ineffective stabilization and suboptimal action choices.\n",
      "Edit Suggestion: add_rule(angle near zero and angular_velocity near zero -> apply minimal torque)\n",
      "Replaced worst idx 1 with new v20: avg_return=-1131.70\n",
      "=== Iteration 18 ===\n",
      "[Selected idx 1 v20] MA Return=-1436.31  SR=0.00  ExpertMean=-115.85\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 20 show consistently poor returns (ranging from -1171 to -1891), far below expert performance (mean return ≈ -123). This suggests the policy struggles to keep the pendulum upright and near its target angle, likely resulting in frequent large negative rewards. Common failure patterns likely include the pendulum swinging wildly or remaining far from the upright position, with actions that either overcorrect or fail to stabilize the pendulum. The uniform episode length (200 steps) indicates the policy does not terminate early but consistently fails to improve state quality throughout the episode. Overall, the key issues are ineffective action selection and inability to recover from unstable states, leading to persistently poor returns.\n",
      "Edit Suggestion: modify_threshold(angle_threshold, 0.1, 0.01)\n",
      "Replaced worst idx 1 with new v21: avg_return=-1141.01\n",
      "=== Iteration 19 ===\n",
      "[Selected idx 1 v21] MA Return=-1284.14  SR=0.00  ExpertMean=-0.19\n",
      "Failure Pattern: The episodes with policy_version 21 in the Pendulum-v1 environment consistently yield low returns (ranging from -1443.54 to -1102.05), significantly worse than expert performance (-235.21). This suggests persistent failure to maintain the pendulum upright and minimize angular velocity, likely resulting in frequent states with large angles from vertical and high velocities. The uniform episode length (200 steps) indicates the agent survives but fails to stabilize the pendulum. Action issues may include insufficient torque application or poor timing, leading to ineffective corrections. Overall, the key failure pattern is the agent's inability to consistently apply actions that keep the pendulum near the upright position, reflected in the much lower returns compared to the expert.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increased_value)\n",
      "Replaced worst idx 1 with new v22: avg_return=-1236.74\n",
      "=== Iteration 20 ===\n",
      "[Selected idx 1 v22] MA Return=-1190.67  SR=0.00  ExpertMean=-122.64\n",
      "Failure Pattern: The agent's episodes in the Pendulum-v1 environment consistently yield low returns (ranging from -1693.84 to -779.44), indicating poor control over the pendulum compared to expert performance (mean return ≈ -0.19). The most common failure patterns likely involve the agent's inability to keep the pendulum upright and near its target angle, resulting in frequent large negative rewards. State characteristics suggest the pendulum spends significant time far from the upright position, while action issues may include insufficient or poorly timed torque applications, failing to counteract the pendulum's momentum. Overall, the agent's policy struggles to stabilize the pendulum, as reflected by the consistently long episode lengths (maxed at 200 steps) but persistently low returns.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, current_value * 1.2)\n",
      "Replaced worst idx 1 with new v23: avg_return=-844.44\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import inspect\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "\n",
    "# ----------------------------\n",
    "# 配置 LLM（保持你原有的 KEY/URL）\n",
    "# ----------------------------\n",
    "LLM_URL = 'https://api.yesapikey.com/v1/chat/completions'\n",
    "LLM_HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer sk-oUmlgJFV5BaTBy8y7048F0E4Af2b4031AdA7B24037F9Bd71'\n",
    "}\n",
    "\n",
    "def call_llm(prompt, model=\"gpt-4.1-2025-04-14\", temperature=0.2, max_tokens=1024):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(LLM_URL, json=data, headers=LLM_HEADERS, timeout=60)\n",
    "            if response.status_code == 200:\n",
    "                resp_json = response.json()\n",
    "                if 'choices' in resp_json and resp_json['choices']:\n",
    "                    content = resp_json['choices'][0].get('message', {}).get('content')\n",
    "                    return content\n",
    "            else:\n",
    "                print(\"[LLM HTTP]\", response.status_code, response.text[:200])\n",
    "        except Exception as e:\n",
    "            print(\"[LLM Exception]\", e)\n",
    "        time.sleep(2)\n",
    "\n",
    "# ----------------------------\n",
    "# 环境文档映射 & 静态知识\n",
    "# ----------------------------\n",
    "ENV_DOC_URL = {\n",
    "    \"Acrobot-v1\": \"https://gymnasium.farama.org/environments/classic_control/acrobot/ \",\n",
    "    \"CartPole-v1\": \"https://gymnasium.farama.org/environments/classic_control/cart_pole/ \",\n",
    "    \"MountainCarContinuous-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/ \",\n",
    "    \"MountainCar-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car/ \",\n",
    "    \"Pendulum-v1\": \"https://gymnasium.farama.org/environments/classic_control/pendulum/ \"\n",
    "}\n",
    "\n",
    "def get_env_doc_url(env_id: str) -> str:\n",
    "    return ENV_DOC_URL.get(env_id, \"https://gymnasium.farama.org/ \")\n",
    "\n",
    "STATIC_KNOWLEDGE = {\n",
    "    \"CartPole-v1\": {\n",
    "        \"state_dim\": 4,\n",
    "        \"state_vars\": [\"cart_position\", \"cart_velocity\", \"pole_angle\", \"pole_velocity\"],\n",
    "        \"state_ranges\": [(-4.8, 4.8), (-float(\"inf\"), float(\"inf\")), (-0.418, 0.418), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1],\n",
    "        \"reward_threshold\": 475,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"Acrobot-v1\": {\n",
    "        \"state_dim\": 6,\n",
    "        \"state_vars\": [\"cos_theta1\", \"sin_theta1\", \"cos_theta2\", \"sin_theta2\", \"theta1_dot\", \"theta2_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\")), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -100,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"MountainCar-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -110,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"MountainCarContinuous-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [-1.0, 1.0],\n",
    "        \"reward_threshold\": 90,\n",
    "        \"action_type\": \"continuous\"\n",
    "    },\n",
    "    \"Pendulum-v1\": {\n",
    "        \"state_dim\": 3,\n",
    "        \"state_vars\": [\"cos_theta\", \"sin_theta\", \"theta_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [-2.0, 2.0],\n",
    "        \"reward_threshold\": -200,\n",
    "        \"action_type\": \"continuous\"\n",
    "    }\n",
    "}\n",
    "\n",
    "EXPERTS_DIR = \"experts\"\n",
    "\n",
    "# ----------------------------\n",
    "# Knowledge / Memory 类（基本不变）\n",
    "# ----------------------------\n",
    "class Knowledge:\n",
    "    def __init__(self):\n",
    "        self.static_knowledge = {}\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def load_static_knowledge(self, env_id):\n",
    "        if env_id not in STATIC_KNOWLEDGE:\n",
    "            raise ValueError(\"Unsupported environment\")\n",
    "        self.static_knowledge = STATIC_KNOWLEDGE[env_id]\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def add_dynamic_entry(self, entry):\n",
    "        self.dynamic_knowledge.append(entry)\n",
    "\n",
    "    def get_dynamic_guidance(self, env_id):\n",
    "        prompt = f\"\"\"\n",
    "I am generating a policy in environment {env_id}.\n",
    "Current dynamic knowledge entries: {self.dynamic_knowledge}\n",
    "\n",
    "Focus on environment principles, physics, and dynamics, not superficial patterns.\n",
    "Please provide concise heuristic suggestions for policy generation based on this knowledge, such as:\n",
    "- State ranges to prioritize\n",
    "- Common failing action patterns\n",
    "- Recommended threshold adjustments\n",
    "\n",
    "Return a short, structured bullet list (no prose).\n",
    "\"\"\"\n",
    "        guidance = call_llm(prompt)\n",
    "        return guidance\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.episodes = []\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.episodes.append({\"steps\": [], \"summary\": None})\n",
    "\n",
    "    def add_step(self, s, a, r, done):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"Please call start_episode() before adding steps!\")\n",
    "        self.episodes[-1][\"steps\"].append({\"s\": np.array(s).tolist(), \"a\": (np.array(a).tolist() if not np.isscalar(a) else float(a)), \"r\": float(r), \"done\": bool(done)})\n",
    "\n",
    "    def add_episode_summary(self, env_id, policy_version):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"No running episode!\")\n",
    "        steps = self.episodes[-1][\"steps\"]\n",
    "        total_reward = sum(step[\"r\"] for step in steps)\n",
    "        length = len(steps)\n",
    "        self.episodes[-1][\"summary\"] = {\n",
    "            \"env_id\": env_id,\n",
    "            \"policy_version\": policy_version,\n",
    "            \"return\": total_reward,\n",
    "            \"length\": length\n",
    "        }\n",
    "\n",
    "    def get_recent_episodes(self, n=5):\n",
    "        summaries = [ep[\"summary\"] for ep in self.episodes if ep[\"summary\"] is not None]\n",
    "        return summaries[-n:]\n",
    "\n",
    "# ----------------------------\n",
    "# Expert loader / Lightweight wrapper（改进版）\n",
    "# ----------------------------\n",
    "def find_expert_file_for_env(env_id):\n",
    "    patterns = [\n",
    "        f\"{env_id}_best_expert.*\",\n",
    "        f\"expert_{env_id}*\",\n",
    "        f\"{env_id}*cleanrl_model\",\n",
    "        f\"*{env_id}*cleanrl_model\",\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        matches = glob.glob(os.path.join(EXPERTS_DIR, pat))\n",
    "        if matches:\n",
    "            matches.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "            return matches[0]\n",
    "    return None\n",
    "\n",
    "def _unwrap_state_dict(maybe_dict):\n",
    "    \"\"\"If saved the checkpoint was a dict wrapper, try to unwrap known fields.\"\"\"\n",
    "    if isinstance(maybe_dict, dict):\n",
    "        for key in (\"state_dict\", \"model_state_dict\", \"policy_state_dict\", \"params\", \"model\"):\n",
    "            if key in maybe_dict and isinstance(maybe_dict[key], dict):\n",
    "                return maybe_dict[key]\n",
    "        return maybe_dict\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define PPO_Agent, SAC_Actor, and DQN_MLP classes\n",
    "class PPO_Agent(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "            torch.nn.init.orthogonal_(layer.weight, std)\n",
    "            torch.nn.init.constant_(layer.bias, bias_const)\n",
    "            return layer\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(obs_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(obs_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, act_dim), std=0.01),\n",
    "        )\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, act_dim))\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        action_mean = self.actor_mean(x)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        dist = torch.distributions.Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        logprob = dist.log_prob(action).sum(1)\n",
    "        entropy = dist.entropy().sum(1)\n",
    "        value = self.critic(x)\n",
    "        return action, logprob, entropy, value\n",
    "\n",
    "class SAC_Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, action_low, action_high):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mean = nn.Linear(256, act_dim)\n",
    "        self.fc_logstd = nn.Linear(256, act_dim)\n",
    "        action_scale = (action_high - action_low) / 2.0\n",
    "        action_bias = (action_high + action_low) / 2.0\n",
    "        self.register_buffer(\"action_scale\", torch.tensor(action_scale, dtype=torch.float32))\n",
    "        self.register_buffer(\"action_bias\", torch.tensor(action_bias, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        LOG_STD_MAX = 2\n",
    "        LOG_STD_MIN = -5\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self.forward(x)\n",
    "        std = torch.exp(log_std)\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t).sum(1, keepdim=True)\n",
    "        return action, log_prob, torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "\n",
    "class DQN_MLP(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 120), nn.ReLU(),\n",
    "            nn.Linear(120, 84), nn.ReLU(),\n",
    "            nn.Linear(84, act_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class ExpertWrapper:\n",
    "    def __init__(self, env_id, model_path, device=\"cpu\"):\n",
    "        self.env_id = env_id\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.model_type = None\n",
    "        self.policy = None\n",
    "        self.loaded = False\n",
    "        sk = STATIC_KNOWLEDGE[env_id]\n",
    "        self.obs_dim = sk[\"state_dim\"]\n",
    "        self.action_type = sk[\"action_type\"]\n",
    "        self.act_dim = len(sk[\"action_space\"]) if sk[\"action_type\"] == \"discrete\" else 1\n",
    "        self.action_low = sk[\"action_space\"][0] if sk[\"action_type\"] == \"continuous\" else None\n",
    "        self.action_high = sk[\"action_space\"][-1] if sk[\"action_type\"] == \"continuous\" else None\n",
    "\n",
    "    def _load(self):\n",
    "        print(f\"[Info] loading expert from {self.model_path}\")\n",
    "        state = torch.load(self.model_path, map_location=\"cpu\")\n",
    "        keys = list(state.keys())\n",
    "        print(\"[Info] Model keys:\", keys[:10])  # Debug print\n",
    "        if any(k.startswith('fc1') or k.startswith('fc_mean') for k in keys):\n",
    "            self.model_type = \"sac_actor\"\n",
    "            print(\"[Info] Detected checkpoint type: SAC\")\n",
    "            self.policy = SAC_Actor(self.obs_dim, self.act_dim, self.action_low, self.action_high)\n",
    "        elif any(k == 'actor_logstd' or k.startswith('actor_mean') for k in keys):\n",
    "            self.model_type = \"ppo_policy\"\n",
    "            print(\"[Info] Detected checkpoint type: PPO\")\n",
    "            self.policy = PPO_Agent(self.obs_dim, self.act_dim)\n",
    "        elif any(k.startswith('network.') for k in keys):\n",
    "            self.model_type = \"dqn\"\n",
    "            print(\"[Info] Detected checkpoint type: DQN\")\n",
    "            self.policy = DQN_MLP(self.obs_dim, self.act_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown expert checkpoint format: keys={keys[:10]}\")\n",
    "        self.policy.load_state_dict(state, strict=False)  # Use strict=False to tolerate minor mismatches\n",
    "        self.policy.to(self.device)\n",
    "        self.policy.eval()\n",
    "        self.loaded = True\n",
    "        print(f\"[Info] Expert loaded ({self.model_type})\")\n",
    "\n",
    "    def act(self, obs, deterministic=True):\n",
    "        if not self.loaded:\n",
    "            self._load()\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32).to(self.device)\n",
    "        if obs.ndim == 1:\n",
    "            obs = obs.unsqueeze(0)\n",
    "\n",
    "        if self.model_type == \"ppo_policy\":\n",
    "            with torch.no_grad():\n",
    "                action, _, _, _ = self.policy.get_action_and_value(obs)\n",
    "                action = action.cpu().numpy()[0]\n",
    "        elif self.model_type == \"sac_actor\":\n",
    "            with torch.no_grad():\n",
    "                _, _, mean = self.policy.get_action(obs)\n",
    "                action = mean.cpu().numpy()[0] if deterministic else self.policy.get_action(obs)[0].cpu().numpy()[0]\n",
    "        elif self.model_type == \"dqn\":\n",
    "            with torch.no_grad():\n",
    "                q = self.policy(obs)\n",
    "                action = torch.argmax(q, dim=1).cpu().numpy()[0]\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupported expert type\")\n",
    "\n",
    "        return action\n",
    "\n",
    "    def run_episodes(self, episodes=3, render=False):\n",
    "        env = self.make_env_with_wrappers()\n",
    "        if render:\n",
    "            env = gym.make(self.env_id, render_mode=\"human\")\n",
    "        returns = []\n",
    "        for _ in range(episodes):\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            total_r = 0\n",
    "            while not done:\n",
    "                action = self.act(obs, deterministic=True)\n",
    "                obs, r, terminated, truncated, _ = env.step(action)\n",
    "                total_r += r\n",
    "                done = terminated or truncated\n",
    "        returns.append(total_r)\n",
    "        env.close()\n",
    "        return returns\n",
    "\n",
    "    def make_env_with_wrappers(self):\n",
    "        env = gym.make(self.env_id)\n",
    "        if self.model_type == \"ppo_policy\":  # Apply PPO wrappers if PPO\n",
    "            try:\n",
    "                env = gym.wrappers.FlattenObservation(env)\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                env = gym.wrappers.ClipAction(env)\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                env = gym.wrappers.NormalizeObservation(env)\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return env\n",
    "\n",
    "def evaluate_expert(env, expert: ExpertWrapper, n_eval_episodes=3):\n",
    "    \"\"\"Evaluate expert policy with debug output\"\"\"\n",
    "    returns = []\n",
    "    for ep in range(n_eval_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = expert.act(obs, deterministic=True)\n",
    "            if steps == 0:\n",
    "                print(f\"[ExpertEval debug] sample obs: {obs}\")\n",
    "                print(f\"[ExpertEval debug] sample action: {action}\")\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "        returns.append(total_reward)\n",
    "        print(f\"[ExpertEval] Episode {ep+1}: total_reward={total_reward:.3f}, steps={steps}\")\n",
    "    avg_return = sum(returns) / len(returns)\n",
    "    print(f\"[ExpertEval] Avg Return over {n_eval_episodes} eps: {avg_return:.2f}\")\n",
    "    return avg_return\n",
    "\n",
    "# ----------------------------\n",
    "# Reflection (集成 expert)\n",
    "# ----------------------------\n",
    "class Reflection:\n",
    "    def __init__(self, knowledge: Knowledge, use_expert=True):\n",
    "        self.knowledge = knowledge\n",
    "        self.use_expert = use_expert\n",
    "        self._expert_cache = {}\n",
    "\n",
    "    def load_expert_for_env(self, env_id):\n",
    "        if not self.use_expert:\n",
    "            return None\n",
    "        if env_id in self._expert_cache:\n",
    "            return self._expert_cache[env_id]\n",
    "        path = find_expert_file_for_env(env_id)\n",
    "        if path is None:\n",
    "            self._expert_cache[env_id] = None\n",
    "            return None\n",
    "        try:\n",
    "            wrapper = ExpertWrapper(env_id, path)\n",
    "            self._expert_cache[env_id] = wrapper\n",
    "            return wrapper\n",
    "        except Exception as e:\n",
    "            print(\"[Reflection] failed to load expert:\", e)\n",
    "            self._expert_cache[env_id] = None\n",
    "            return None\n",
    "\n",
    "    def metrics(self, recent_episodes):\n",
    "        returns = [ep[\"return\"] for ep in recent_episodes]\n",
    "        lengths = [ep[\"length\"] for ep in recent_episodes]\n",
    "        avg_return = np.mean(returns) if returns else 0.0\n",
    "        avg_length = np.mean(lengths) if lengths else 0.0\n",
    "        threshold = self.knowledge.static_knowledge.get(\"reward_threshold\", 0.0)\n",
    "        success_count = sum(1 for ep in recent_episodes if ep[\"return\"] >= threshold)\n",
    "        success_rate = (success_count / len(recent_episodes)) if recent_episodes else 0.0\n",
    "        return {\"avg_return\": float(avg_return), \"avg_length\": float(avg_length), \"success_rate\": float(success_rate)}\n",
    "\n",
    "    def failure_pattern(self, recent_episodes, env_id):\n",
    "        expert = self.load_expert_for_env(env_id)\n",
    "        expert_summary = None\n",
    "        if expert:\n",
    "            try:\n",
    "                expert_rs = expert.run_episodes(episodes=3, render=False)\n",
    "                expert_summary = {\"expert_returns\": expert_rs, \"expert_mean\": float(np.mean(expert_rs))}\n",
    "            except Exception as e:\n",
    "                expert_summary = {\"error\": str(e)}\n",
    "        prompt = f\"\"\"\n",
    "I have the following {env_id} environment episode summaries: {recent_episodes}\n",
    "Expert performance (deterministic eval) if available: {expert_summary}\n",
    "Please analyze the most common failure patterns, including state characteristics, action issues, and return patterns. Focus only on key points.\n",
    "Return a concise paragraph.\n",
    "\"\"\"\n",
    "        pattern = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"env_id\": env_id, \"failure_pattern\": pattern})\n",
    "        return pattern\n",
    "\n",
    "    def edit_suggestion(self, recent_episodes, env_id):\n",
    "        expert = self.load_expert_for_env(env_id)\n",
    "        expert_summary = None\n",
    "        if expert:\n",
    "            try:\n",
    "                expert_rs = expert.run_episodes(episodes=3, render=False)\n",
    "                expert_summary = {\"expert_returns\": expert_rs, \"expert_mean\": float(np.mean(expert_rs))}\n",
    "            except Exception as e:\n",
    "                expert_summary = {\"error\": str(e)}\n",
    "        prompt = f\"\"\"\n",
    "Based on recent episode data from environment {env_id}: {recent_episodes}\n",
    "Expert performance (deterministic eval) if available: {expert_summary}\n",
    "Generate one policy editing suggestion in one of the following formats:\n",
    "- add_rule(condition -> action)\n",
    "- modify_threshold(variable, old_value, new_value)\n",
    "- reprioritize(rule_i over rule_j)\n",
    "\n",
    "Return exactly one line with one edit.\n",
    "\"\"\"\n",
    "        suggestion = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"env_id\": env_id, \"edit_suggestion\": suggestion})\n",
    "        return suggestion\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: action constraint text, code generation, compilation (保持你原始实现)\n",
    "# ----------------------------\n",
    "def _action_constraints_text(static_knowledge: dict) -> str:\n",
    "    a = static_knowledge[\"action_space\"]\n",
    "    if static_knowledge.get(\"action_type\") == \"discrete\":\n",
    "        return f\"Discrete actions; valid actions are exactly the integers in {a}.\"\n",
    "    else:\n",
    "        lo, hi = a[0], a[1]\n",
    "        return f\"Continuous action; return a single float within [{lo}, {hi}]. Clip if necessary.\"\n",
    "\n",
    "def generate_rule_policy_code(env_id, knowledge: Knowledge):\n",
    "    sk = knowledge.static_knowledge\n",
    "    guidance = knowledge.get_dynamic_guidance(env_id) or \"\"\n",
    "    doc_url = get_env_doc_url(env_id)\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "    state_vars_text = \"\\n\".join([f\"- {name} in range {rng}\" for name, rng in zip(sk[\"state_vars\"], sk[\"state_ranges\"])])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are writing a deterministic, white-box **rule-based policy** for Gymnasium environment \"{env_id}\".\n",
    "Focus on **environment principles, physics, and dynamics**, not superficial patterns.\n",
    "The policy must be based on simple if-else statements or threshold comparisons using state variables.\n",
    "Environment documentation: {doc_url}\n",
    "\n",
    "Observation (state vector):\n",
    "{state_vars_text}\n",
    "\n",
    "Action constraints:\n",
    "- {action_desc}\n",
    "- May import 'math' if needed\n",
    "- Must be deterministic\n",
    "- Do not use loops, functions, or external libraries except math\n",
    "- Example (discrete): if state[2] > 0: return 1 else: return 0\n",
    "- Example (continuous): return max(min(k1*state[1]-k2*state[0], hi), lo)\n",
    "\n",
    "Dynamic guidance:\n",
    "{guidance}\n",
    "\n",
    "Output requirements:\n",
    "- Only one Python function: def policy(state): ...\n",
    "- No explanations, no markdown, no print\n",
    "- Returned action strictly satisfies constraints\n",
    "\"\"\"\n",
    "    code = call_llm(prompt)\n",
    "    return code\n",
    "\n",
    "def compile_policy_or_default(code, sk):\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    try:\n",
    "        exec(code, local_vars)\n",
    "        policy_fn = local_vars.get(\"policy\")\n",
    "        if policy_fn is None:\n",
    "            raise ValueError(\"No function 'policy' found\")\n",
    "        return policy_fn\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            def policy(state): return sk[\"action_space\"][0]\n",
    "        else:\n",
    "            lo, hi = sk[\"action_space\"]\n",
    "            def policy(state): return 0.5 * (lo + hi)\n",
    "        return policy\n",
    "\n",
    "def generate_base_policies(env_id, knowledge: Knowledge, n_candidates=3):\n",
    "    sk = knowledge.static_knowledge\n",
    "    fns = []\n",
    "    for _ in range(n_candidates):\n",
    "        code = generate_rule_policy_code(env_id, knowledge)\n",
    "        fns.append(compile_policy_or_default(code, sk))\n",
    "    return fns\n",
    "\n",
    "def apply_edit(policy_fn, edit_text, knowledge: Knowledge, env_id: str):\n",
    "    sk = knowledge.static_knowledge\n",
    "    try:\n",
    "        existing_src = inspect.getsource(policy_fn)\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            existing_src = \"def policy(state):\\n    return \" + str(sk[\"action_space\"][0])\n",
    "        else:\n",
    "            lo, hi = sk[\"action_space\"]\n",
    "            existing_src = \"def policy(state):\\n    return \" + str(0.5*(lo+hi))\n",
    "\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "    doc_url = get_env_doc_url(env_id)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Revise deterministic, **rule-based** policy for environment at {doc_url}.\n",
    "Focus on physics, dynamics, and environment principles.\n",
    "Constraints: {action_desc}\n",
    "Current policy:\n",
    "{existing_src}\n",
    "\n",
    "Edit suggestion: {edit_text}\n",
    "\n",
    "Rules:\n",
    "- Keep it deterministic and rule-based (if-else / threshold)\n",
    "- Only output a single valid Python function: def policy(state): ...\n",
    "- You may use 'math'\n",
    "\"\"\"\n",
    "    code = call_llm(prompt)\n",
    "    return compile_policy_or_default(code, sk)\n",
    "\n",
    "# ----------------------------\n",
    "# Safe step & Safe policy call（保持原实现）\n",
    "# ----------------------------\n",
    "def safe_step(env, action):\n",
    "    sk = STATIC_KNOWLEDGE[env.unwrapped.spec.id]\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        lo, hi = sk[\"action_space\"]\n",
    "        if np.isscalar(action):\n",
    "            action = np.array([np.clip(action, lo, hi)], dtype=np.float32)\n",
    "        else:\n",
    "            action = np.clip(np.array(action, dtype=np.float32), lo, hi)\n",
    "    return env.step(action)\n",
    "\n",
    "def safe_policy_call(state, policy_fn, sk, env):\n",
    "    try:\n",
    "        a = policy_fn(state)\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            a = random.choice(sk[\"action_space\"])\n",
    "        else:\n",
    "            lo, hi = sk[\"action_space\"]\n",
    "            a = 0.5 * (lo + hi)\n",
    "\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        lo, hi = sk[\"action_space\"]\n",
    "        a = float(np.clip(a, lo, hi))\n",
    "        return a\n",
    "\n",
    "    uw = getattr(env, \"unwrapped\", env)\n",
    "    can_clone = hasattr(uw, \"clone_state\") and hasattr(uw, \"restore_state\")\n",
    "    if not can_clone:\n",
    "        return a\n",
    "\n",
    "    best_a, best_r = a, -float('inf')\n",
    "    try:\n",
    "        snapshot = uw.clone_state()\n",
    "    except Exception:\n",
    "        return a\n",
    "    for cand in sk[\"action_space\"]:\n",
    "        try:\n",
    "            obs2, r, term, trunc, _ = safe_step(env, cand)\n",
    "            uw.restore_state(snapshot)\n",
    "            if r > best_r:\n",
    "                best_r, best_a = r, cand\n",
    "        except Exception:\n",
    "            try:\n",
    "                uw.restore_state(snapshot)\n",
    "            except Exception:\n",
    "                pass\n",
    "            continue\n",
    "    return best_a\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation helpers (eval_policy_once, parallel_eval_candidates)\n",
    "# ----------------------------\n",
    "def eval_policy_once(env_id, policy_fn, episodes=5, use_mcts=True, no_trunc_reset_for=None):\n",
    "    knowledge = Knowledge()\n",
    "    knowledge.load_static_knowledge(env_id)\n",
    "    sk = knowledge.static_knowledge\n",
    "    mem = Memory()\n",
    "\n",
    "    env = gym.make(env_id)\n",
    "    trunc_sensitive = (env_id not in (no_trunc_reset_for or set()))\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        mem.start_episode()\n",
    "        while not done:\n",
    "            if use_mcts:\n",
    "                a = safe_policy_call(s, policy_fn, sk, env)\n",
    "            else:\n",
    "                try:\n",
    "                    a = policy_fn(s)\n",
    "                except Exception:\n",
    "                    if sk[\"action_type\"] == \"discrete\":\n",
    "                        a = random.choice(sk[\"action_space\"])\n",
    "                    else:\n",
    "                        lo, hi = sk[\"action_space\"]\n",
    "                        a = 0.5 * (lo + hi)\n",
    "            s_next, r, terminated, truncated, info = safe_step(env, a)\n",
    "            done = terminated or truncated\n",
    "            mem.add_step(s, a, r, done)\n",
    "            s = s_next\n",
    "        mem.add_episode_summary(env_id, policy_version=0)\n",
    "\n",
    "    env.close()\n",
    "    refl = Reflection(knowledge)\n",
    "    return refl.metrics(mem.get_recent_episodes(n=episodes))\n",
    "\n",
    "def parallel_eval_candidates(env_id, policy_fns, episodes_each=5):\n",
    "    results = []\n",
    "    no_trunc_reset_for = {\"Acrobot-v1\", \"Pendulum-v1\"}\n",
    "    with ThreadPoolExecutor(max_workers=min(len(policy_fns), 3)) as ex:\n",
    "        fut2fn = {\n",
    "            ex.submit(eval_policy_once, env_id, fn, episodes_each, True, no_trunc_reset_for): fn\n",
    "            for fn in policy_fns\n",
    "        }\n",
    "        for fut in as_completed(fut2fn):\n",
    "            fn = fut2fn[fut]\n",
    "            try:\n",
    "                metrics = fut.result()\n",
    "            except Exception as e:\n",
    "                metrics = {\"avg_return\": -1e9, \"avg_length\": 0.0, \"success_rate\": 0.0}\n",
    "                print(\"[Parallel Eval Exception]\", e)\n",
    "            results.append((fn, metrics))\n",
    "    return results\n",
    "\n",
    "# ----------------------------\n",
    "# PolicyPool 类（原始实现）\n",
    "# ----------------------------\n",
    "class PolicyPool:\n",
    "    def __init__(self, max_size=5):\n",
    "        self.policies = []\n",
    "        self.max_size = max_size\n",
    "        self.counts = []\n",
    "        self.values = []\n",
    "\n",
    "    def add_policy(self, policy_fn, version, metrics=None):\n",
    "        entry = {\"fn\": policy_fn, \"version\": version, \"metrics\": metrics}\n",
    "        if len(self.policies) < self.max_size:\n",
    "            self.policies.append(entry)\n",
    "            self.counts.append(0)\n",
    "            self.values.append(0.0)\n",
    "        else:\n",
    "            idx = self.get_worst_policy_idx()\n",
    "            self.policies[idx] = entry\n",
    "            self.counts[idx] = 0\n",
    "            self.values[idx] = 0.0\n",
    "\n",
    "    def select_policy_ucb(self, c=1.0):\n",
    "        if not self.policies:\n",
    "            raise RuntimeError(\"PolicyPool is empty\")\n",
    "        total_counts = sum(self.counts) + 1\n",
    "        ucb_scores = []\n",
    "        for i in range(len(self.policies)):\n",
    "            if self.counts[i] == 0:\n",
    "                ucb_scores.append(float('inf'))\n",
    "            else:\n",
    "                ucb_scores.append(self.values[i] + c * math.sqrt(math.log(total_counts)/self.counts[i]))\n",
    "        idx = int(np.argmax(ucb_scores))\n",
    "        self.counts[idx] += 1\n",
    "        return self.policies[idx][\"fn\"], idx\n",
    "\n",
    "    def update_policy_value(self, idx, reward):\n",
    "        n = self.counts[idx]\n",
    "        if n <= 0:\n",
    "            self.values[idx] = reward\n",
    "        else:\n",
    "            self.values[idx] = ((n-1)/n)*self.values[idx] + (1/n)*reward\n",
    "\n",
    "    def get_worst_policy_idx(self):\n",
    "        vals = []\n",
    "        for p in self.policies:\n",
    "            if p[\"metrics\"] and \"avg_return\" in p[\"metrics\"]:\n",
    "                vals.append(p[\"metrics\"][\"avg_return\"])\n",
    "            else:\n",
    "                vals.append(-float('inf'))\n",
    "        return int(np.argmin(vals))\n",
    "\n",
    "# ----------------------------\n",
    "# 主闭环训练/搜索循环（run_env_loop）\n",
    "# ----------------------------\n",
    "def run_env_loop(env_id, max_iters=10, episodes_per_iter=10, ma_window=3,\n",
    "                 success_rate_threshold=0.8, pool_size=5, n_init_candidates=3):\n",
    "    knowledge = Knowledge()\n",
    "    knowledge.load_static_knowledge(env_id)\n",
    "    memory = Memory()\n",
    "    reflection = Reflection(knowledge, use_expert=True)\n",
    "    policy_version = 0\n",
    "    policy_pool = PolicyPool(max_size=pool_size)\n",
    "\n",
    "    # initial candidates\n",
    "    init_fns = generate_base_policies(env_id, knowledge, n_candidates=n_init_candidates)\n",
    "    evaluated = parallel_eval_candidates(env_id, init_fns, episodes_each=max(2, episodes_per_iter//2))\n",
    "    evaluated.sort(key=lambda t: t[1][\"avg_return\"], reverse=True)\n",
    "    for fn, m in evaluated[:pool_size]:\n",
    "        policy_version += 1\n",
    "        policy_pool.add_policy(fn, policy_version, metrics=m)\n",
    "\n",
    "    for iter_idx in range(max_iters):\n",
    "        print(f\"=== Iteration {iter_idx+1} ===\")\n",
    "        policy_fn, idx = policy_pool.select_policy_ucb()\n",
    "\n",
    "        # Run selected policy\n",
    "        env = gym.make(env_id)\n",
    "        iteration_returns = []\n",
    "        truncated_seen = False\n",
    "        for ep in range(episodes_per_iter):\n",
    "            s, _ = env.reset()\n",
    "            done = False\n",
    "            memory.start_episode()\n",
    "            while not done:\n",
    "                a = safe_policy_call(s, policy_fn, knowledge.static_knowledge, env)\n",
    "                s_next, r, terminated, truncated, info = safe_step(env, a)\n",
    "                done = terminated or truncated\n",
    "                if truncated and env_id not in [\"Acrobot-v1\", \"Pendulum-v1\"]:\n",
    "                    truncated_seen = True\n",
    "                memory.add_step(s, a, r, done)\n",
    "                s = s_next\n",
    "            memory.add_episode_summary(env_id, policy_version=policy_pool.policies[idx][\"version\"])\n",
    "            iteration_returns.append(memory.episodes[-1][\"summary\"][\"return\"])\n",
    "        env.close()\n",
    "\n",
    "        recent_ma = memory.get_recent_episodes(n=ma_window)\n",
    "        metrics = reflection.metrics(recent_ma)\n",
    "        policy_pool.policies[idx][\"metrics\"] = metrics\n",
    "        policy_pool.update_policy_value(idx, metrics[\"avg_return\"])\n",
    "\n",
    "        # compute expert deterministic mean (3 episodes) for display comparison\n",
    "        expert_mean = None\n",
    "        expert_wrapper = reflection.load_expert_for_env(env_id)\n",
    "        if expert_wrapper is not None:\n",
    "            try:\n",
    "                ers = expert_wrapper.run_episodes(episodes=3, render=False)\n",
    "                expert_mean = float(np.mean(ers))\n",
    "            except Exception as e:\n",
    "                expert_mean = None\n",
    "\n",
    "        # print including expert mean\n",
    "        if expert_mean is None:\n",
    "            print(f\"[Selected idx {idx} v{policy_pool.policies[idx]['version']}] MA Return={metrics['avg_return']:.2f}  SR={metrics['success_rate']:.2f}  ExpertMean=N/A\")\n",
    "        else:\n",
    "            print(f\"[Selected idx {idx} v{policy_pool.policies[idx]['version']}] MA Return={metrics['avg_return']:.2f}  SR={metrics['success_rate']:.2f}  ExpertMean={expert_mean:.2f}\")\n",
    "\n",
    "        pattern = reflection.failure_pattern(recent_ma, env_id)\n",
    "        print(\"Failure Pattern:\", pattern)\n",
    "        edit = reflection.edit_suggestion(recent_ma, env_id)\n",
    "        print(\"Edit Suggestion:\", edit)\n",
    "\n",
    "        # Replace worst\n",
    "        worst_idx = policy_pool.get_worst_policy_idx()\n",
    "        seed_fn = policy_fn\n",
    "        new_candidates = []\n",
    "        try:\n",
    "            edited_fn = apply_edit(seed_fn, edit, knowledge, env_id)\n",
    "            new_candidates.append(edited_fn)\n",
    "        except Exception:\n",
    "            pass\n",
    "        fresh = generate_base_policies(env_id, knowledge, n_candidates=2)\n",
    "        new_candidates.extend(fresh)\n",
    "\n",
    "        cand_eval = parallel_eval_candidates(env_id, new_candidates, episodes_each=max(2, episodes_per_iter//2))\n",
    "        cand_eval.sort(key=lambda t: t[1][\"avg_return\"], reverse=True)\n",
    "        best_new_fn, best_new_metrics = cand_eval[0]\n",
    "        policy_version += 1\n",
    "        policy_pool.policies[worst_idx] = {\"fn\": best_new_fn, \"version\": policy_version, \"metrics\": best_new_metrics}\n",
    "        policy_pool.counts[worst_idx] = 0\n",
    "        policy_pool.values[worst_idx] = 0.0\n",
    "        print(f\"Replaced worst idx {worst_idx} with new v{policy_version}: avg_return={best_new_metrics['avg_return']:.2f}\")\n",
    "\n",
    "        if len(memory.episodes) >= ma_window * episodes_per_iter:\n",
    "            thr = knowledge.static_knowledge.get(\"reward_threshold\", 0.0)\n",
    "            if metrics[\"avg_return\"] >= thr and metrics[\"success_rate\"] >= success_rate_threshold:\n",
    "                print(f\"Converged! MA Return={metrics['avg_return']:.2f}, SR={metrics['success_rate']:.2f}\")\n",
    "                break\n",
    "\n",
    "        if truncated_seen and env_id not in [\"Acrobot-v1\", \"Pendulum-v1\"]:\n",
    "            print(\"[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\")\n",
    "\n",
    "# ----------------------------\n",
    "# 脚本入口 — 逐个运行任务\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    env_list = [\n",
    "        \"Acrobot-v1\",\n",
    "        \"CartPole-v1\",\n",
    "        \"MountainCarContinuous-v0\",\n",
    "        \"MountainCar-v0\",\n",
    "        \"Pendulum-v1\"\n",
    "    ]\n",
    "    # 打印发现的 experts\n",
    "    print(\"Expert files found:\")\n",
    "    for env in env_list:\n",
    "        f = find_expert_file_for_env(env)\n",
    "        print(f\"  {env}: {f}\")\n",
    "    print(\"Starting closed-loop runs (this may take long).\")\n",
    "\n",
    "    for env_id in env_list:\n",
    "        print(f\"\\n==== Running {env_id} ====\")\n",
    "        run_env_loop(env_id, max_iters=20, episodes_per_iter=20, ma_window=3,\n",
    "                     success_rate_threshold=0.8, pool_size=5, n_init_candidates=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85432e-d28e-4847-b40e-1f3ff5c70c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_rl",
   "language": "python",
   "name": "dl_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
