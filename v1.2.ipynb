{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb49a43-0723-4a6f-bbe5-6ea5a7237b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Running Acrobot-v1 ====\n",
      "Iteration 1, Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes in the Acrobot-v1 environment terminated with the minimum return of -500.0 and maximum episode length of 500, indicating the agent consistently failed to achieve the task objective (raising the end-effector above a target height) within the allowed steps. This pattern suggests the policy is ineffective, likely resulting in repetitive or unproductive actions such as oscillating or stationary behavior without meaningful progress. Common failure characteristics in such cases include the agent remaining near its initial state, failing to generate sufficient momentum, or selecting actions that do not exploit the environment’s dynamics. The uniformity of returns and episode lengths highlights a lack of learning or exploration, possibly due to poor policy initialization or insufficient reward feedback.\n",
      "Iteration 2, Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 all show a return of -500.0 and a length of 500 steps, indicating the agent consistently fails to solve the environment within the maximum allowed steps. This pattern suggests the agent is likely stuck in unproductive state trajectories, such as swinging without gaining enough momentum to reach the goal. Action selection may be ineffective, possibly repeating suboptimal actions or failing to exploit the environment’s dynamics. The uniformity in returns and episode lengths highlights a persistent inability to improve performance, likely due to poor exploration or insufficient learning from previous episodes.\n",
      "Iteration 3, Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes in the Acrobot-v1 environment terminated with the minimum return of -500.0 and maximum length of 500 steps, indicating the agent consistently failed to solve the task within the allowed time. This pattern suggests the policy (version 3) is unable to swing the Acrobot’s end-effector to the target height, likely due to ineffective action selection—such as repetitive or non-exploratory actions—and poor state transitions that do not progress toward the goal. The uniformity in returns and episode lengths highlights a persistent inability to escape initial or suboptimal states, reflecting a lack of learning or exploration in the agent’s behavior.\n",
      "Iteration 4, Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 all show a return of -500.0 and a maximum episode length of 500, indicating the agent consistently fails to solve the task within the allowed steps. This pattern suggests the agent is unable to swing the end-effector to the required height, likely remaining in suboptimal or stagnant states. Common failure characteristics include the agent getting stuck in local minima, such as low-energy oscillations near the downward position, and failing to generate sufficient momentum. Action selection may be ineffective or repetitive, lacking the variability or timing needed to escape these states. The uniform poor returns and episode lengths highlight a lack of learning progress or exploration, possibly due to insufficient policy updates or poor reward signal utilization.\n",
      "Iteration 5, Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 consistently show a return of -500.0 and maximum episode length (500 steps), indicating the agent repeatedly fails to solve the task and terminates only due to the time limit. This pattern suggests the policy (version 5) is unable to swing the lower link above the required threshold. Common failure characteristics likely include the agent remaining in low-energy or stagnant states, with actions that do not generate sufficient momentum or are poorly timed. The uniformity in returns and episode lengths points to persistent issues in exploration or policy effectiveness, rather than sporadic mistakes.\n",
      "Iteration 6, Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 consistently show a return of -500.0 and maximum episode length (500 steps), indicating the agent repeatedly fails to solve the task and never achieves the goal state. This pattern suggests the agent is likely stuck in unproductive state trajectories, possibly oscillating or remaining in low-reward regions without effective exploration. Action selection may be ineffective or random, failing to generate sufficient torque or coordinated movement to swing the end-effector to the required height. The uniformity in returns and lengths points to a persistent policy failure, likely due to poor state-action mapping or insufficient learning progress.\n",
      "Iteration 7, Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episodes for Acrobot-v1 all terminate with the minimum return of -500.0 and maximum length of 500 steps, indicating the agent consistently fails to solve the task within the allowed time. This pattern suggests the policy (version 7) is unable to swing the acrobot's end-effector to the target height. Common failure characteristics likely include the agent remaining in low-reward states, such as the acrobot hanging below the goal, and possibly repeating ineffective or random actions that do not generate sufficient upward momentum. The uniformity in returns and episode lengths highlights a lack of learning progress or exploration, with the agent stuck in a suboptimal behavioral loop.\n",
      "Iteration 8, Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes for Acrobot-v1 with policy_version 8 show a consistent failure pattern: each episode reaches the maximum length of 500 steps with the minimum possible return of -500.0. This indicates the agent repeatedly fails to solve the task, likely remaining in suboptimal states without achieving the goal (raising the end-effector above the target height). The persistent poor returns suggest ineffective action selection, possibly due to a policy stuck in a loop or producing near-zero torques, resulting in little to no progress. The lack of episode termination before the time limit further implies the agent never escapes initial or low-reward states, highlighting a need for improved exploration or policy updates.\n",
      "Iteration 9, Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes for Acrobot-v1 with policy_version 9 terminate at the maximum length of 500 steps with a return of -500.0, indicating the agent consistently fails to solve the task and never achieves the goal state. This pattern suggests the agent is either stuck in unproductive state trajectories (e.g., low joint velocities or poor link angles) or is taking ineffective or repetitive actions that do not generate sufficient upward momentum. The uniformity in episode length and return points to a lack of exploration or learning progress, possibly due to suboptimal policy initialization, insufficient reward feedback, or inadequate action selection strategies.\n",
      "Iteration 10, Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes in the Acrobot-v1 environment terminated with the minimum return of -500.0 and the maximum episode length of 500 steps, indicating that the agent consistently failed to solve the task within the allowed time. This pattern suggests the policy (version 10) is unable to swing the Acrobot's end-effector to the target height, likely due to ineffective or random action selection. Common failure characteristics likely include the agent remaining in low-energy or oscillatory states without generating sufficient momentum, and possibly repeating suboptimal actions that do not exploit the environment's dynamics. The uniformity of poor returns and episode lengths points to a fundamental issue in policy learning or exploration.\n",
      "==== Running CartPole-v1 ====\n",
      "Iteration 1, Moving Avg Return: 500.00, Success Rate: 1.00\n",
      "Failure Pattern: Based on the provided episode summaries, all episodes in the CartPole-v1 environment achieved the maximum return and episode length (500), indicating that the agent did not fail within the allowed steps. As a result, there are no observable failure patterns in terms of state characteristics, action selection, or return trends. The agent's policy appears robust and consistently maintains balance, suggesting effective handling of both state transitions and action choices throughout each episode.\n",
      "Converged! Stop training. Moving Avg Return=500.00, Success Rate=1.00\n",
      "==== Running MountainCarContinuous-v0 ====\n",
      "Iteration 1, Moving Avg Return: -69.37, Success Rate: 0.00\n",
      "Failure Pattern: The episodes from the MountainCarContinuous-v0 environment consistently end with long episode lengths (999 steps), indicating the agent repeatedly fails to reach the goal within the allowed time. The returns are all strongly negative (around -68 to -71), reflecting persistent inefficiency or inability to escape the valley. This suggests a common failure pattern where the agent likely oscillates or stalls near the bottom of the hill, unable to generate sufficient momentum. The actions chosen may be too weak, inconsistent, or poorly timed, preventing the car from building up the speed needed to reach the goal state. Overall, the agent demonstrates a lack of effective exploration or exploitation of the environment’s dynamics, resulting in prolonged, unproductive episodes.\n",
      "Iteration 2, Moving Avg Return: -69.05, Success Rate: 0.00\n",
      "Failure Pattern: The episodes from the MountainCarContinuous-v0 environment consistently end with maximum length (999 steps) and low returns (around -67 to -72), indicating that the agent repeatedly fails to reach the goal within the allowed time. This suggests a common failure pattern where the agent likely gets stuck oscillating near the bottom of the valley or fails to build enough momentum to climb the hill. The persistent negative returns and episode lengths imply ineffective action selection—possibly weak or poorly timed accelerations—preventing escape from suboptimal states. Overall, the agent struggles with generating the necessary force and timing to solve the task, leading to repeated unsuccessful episodes.\n",
      "Iteration 3, Moving Avg Return: -23.20, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 show that two out of three episodes ended with long durations (999 steps) and significantly negative returns (-72.51 and -69.63), indicating the agent failed to reach the goal within the time limit—this is a common failure pattern in this environment. Such failures typically stem from the agent not generating enough momentum to climb the hill, often due to suboptimal or overly conservative action choices that fail to exploit the environment's physics. The negative returns further suggest the agent spent excessive time and energy without achieving the objective. In contrast, the successful episode (493 steps, return 72.52) demonstrates that when the agent applies more effective, well-timed actions, it can solve the task in fewer steps with a positive return. Overall, the key issues are insufficient exploration or poor action selection, leading to prolonged episodes and negative rewards.\n",
      "Iteration 4, Moving Avg Return: -70.69, Success Rate: 0.00\n",
      "Failure Pattern: The episodes from the MountainCarContinuous-v0 environment consistently reach the maximum length of 999 steps, indicating that the agent repeatedly fails to solve the task within the allowed time. The returns are all strongly negative (around -68 to -72), reflecting persistent inefficiency in progressing toward the goal. This pattern suggests the agent likely struggles with insufficient acceleration or poor timing in applying force, possibly oscillating near the bottom of the valley without gaining enough momentum to reach the flag. The consistent episode length and negative returns point to a policy that is either too conservative or poorly tuned, failing to exploit the environment's dynamics to escape the initial low-energy state.\n",
      "Iteration 5, Moving Avg Return: -70.25, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 all show long episode lengths (999 steps), indicating the agent consistently fails to reach the goal within the time limit. The returns are all negative and clustered closely (-71.29, -71.04, -68.41), suggesting the agent is stuck in suboptimal behavior, likely oscillating near the bottom of the valley without generating enough momentum to reach the flag. This pattern points to issues with insufficient or poorly timed actions, possibly due to inadequate exploration or a policy that fails to learn the necessary force application. The consistent episode length and similar returns highlight a lack of progress across episodes, with the agent repeatedly failing in similar states and action patterns.\n",
      "Iteration 6, Moving Avg Return: -68.97, Success Rate: 0.00\n",
      "Failure Pattern: The episodes from MountainCarContinuous-v0 consistently reach the maximum length of 999 steps, indicating the agent fails to solve the task within the allowed time. The returns are all negative and clustered around -68 to -71, suggesting the agent spends most of its time in low-reward states, likely near the bottom of the valley without generating enough momentum to reach the goal. This pattern points to common failure modes: insufficient exploration or suboptimal action selection, such as weak or poorly timed accelerations, preventing escape from the initial state region. Overall, the agent's policy struggles to generate effective actions for overcoming the environment's core challenge—building momentum to reach the hilltop.\n",
      "Iteration 7, Moving Avg Return: -68.99, Success Rate: 0.00\n",
      "Failure Pattern: The episodes from the MountainCarContinuous-v0 environment consistently end with long episode lengths (999 steps), indicating that the agent fails to reach the goal within the allowed time. The returns are all negative and clustered closely (around -68 to -70), suggesting the agent is unable to generate sufficient momentum to climb the hill, likely oscillating near the valley without escaping. This pattern points to common failures such as insufficient or poorly timed acceleration, suboptimal exploration, and possibly getting stuck in low-velocity states. The agent’s actions may lack the necessary force or directionality to build up speed, resulting in repeated unsuccessful attempts and minimal progress toward the goal.\n",
      "Iteration 8, Moving Avg Return: -69.73, Success Rate: 0.00\n",
      "Failure Pattern: The episodes from the MountainCarContinuous-v0 environment consistently end with long durations (length = 999), indicating that the agent fails to reach the goal within the time limit. The returns are all negative and clustered closely (around -67 to -72), suggesting the agent is unable to generate sufficient momentum to drive up the hill and solve the task. This pattern typically points to common failure modes: the agent likely remains stuck oscillating near the bottom of the valley or takes suboptimal, low-magnitude actions that do not exploit the environment’s physics (i.e., not building enough speed by moving back and forth). The state characteristics in such failures often show the car’s position and velocity fluctuating within a narrow range, never achieving the necessary velocity to escape the valley. Overall, the agent’s policy appears too conservative or poorly tuned to the environment’s dynamics, resulting in repeated timeouts and consistently poor returns.\n",
      "Iteration 9, Moving Avg Return: -69.07, Success Rate: 0.00\n",
      "Failure Pattern: The episodes from the MountainCarContinuous-v0 environment consistently end with long episode lengths (999 steps), indicating the agent repeatedly fails to reach the goal within the allowed time. The returns are all strongly negative (around -69), reflecting persistent inefficiency or inability to make progress up the mountain. This pattern suggests the agent is likely stuck in suboptimal states, such as oscillating at the bottom of the valley, and is unable to generate sufficient momentum to reach the goal. Action-wise, this often results from the agent choosing actions that are too weak, inconsistent, or poorly timed to build up the necessary speed. Overall, the key failure pattern is prolonged stagnation in low-reward states due to ineffective action selection, leading to timeouts and poor returns.\n",
      "Iteration 10, Moving Avg Return: -69.84, Success Rate: 0.00\n",
      "Failure Pattern: The episodes from MountainCarContinuous-v0 all terminate at the maximum length of 999 steps, indicating the agent consistently fails to reach the goal. The returns are tightly clustered around -69 to -71, reflecting prolonged periods of low reward accumulation, typical when the car remains stuck oscillating near the bottom of the valley without generating enough momentum to ascend. This suggests common failure patterns: the agent likely struggles with state transitions requiring coordinated acceleration, possibly due to suboptimal action selection (e.g., insufficient or poorly timed force application). Overall, the policy fails to exploit the environment’s dynamics, resulting in repeated episodes of stagnation and poor returns.\n",
      "==== Running MountainCar-v0 ====\n",
      "Iteration 1, Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a length of 200 steps, indicating that the agent consistently fails to reach the goal within the maximum allowed steps. This pattern suggests the agent is either not learning an effective policy or is taking suboptimal actions, such as oscillating without building enough momentum to reach the hilltop. Common failure characteristics in this environment include the agent remaining stuck in local minima, not exploiting the environment's physics (e.g., not reversing to gain speed), and repeatedly selecting actions that do not increase position effectively. The uniformity of poor returns and maximum episode lengths highlights a lack of exploration or inadequate policy updates in the current policy version.\n",
      "Iteration 2, Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a length of 200, indicating the agent consistently failed to reach the goal within the maximum allowed steps. This pattern suggests the agent is likely stuck in suboptimal states, such as low velocity and poor position, unable to generate enough momentum to climb the hill. The repeated returns and episode lengths imply the policy may be selecting ineffective or repetitive actions, such as oscillating between left and right without strategic acceleration. Overall, the key failure pattern is persistent inability to escape the initial valley, resulting in maximum episode length and minimum return.\n",
      "Iteration 3, Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a length of 200, indicating the agent consistently fails to reach the goal within the maximum allowed steps. This pattern suggests the agent is likely stuck oscillating near the starting position or unable to build enough momentum to climb the hill, a common failure mode in this environment. The repeated maximum episode length and minimum return imply ineffective action selection—possibly favoring suboptimal or random actions that do not exploit the environment's dynamics. Overall, the agent demonstrates poor exploration and lacks a policy capable of leveraging state information to escape the initial valley, resulting in repeated, unproductive episodes.\n",
      "Iteration 4, Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a length of 200, indicating that the agent consistently fails to reach the goal within the maximum allowed steps. This pattern suggests the agent is either stuck in suboptimal states (e.g., unable to build enough momentum to climb the hill) or is taking ineffective actions, such as oscillating without progress or repeatedly choosing actions that do not increase velocity toward the goal. The uniformity in returns and episode lengths highlights a persistent failure to solve the environment, likely due to poor exploration, insufficient policy learning, or an inability to escape local minima in the state space.\n",
      "Iteration 5, Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes for policy_version 5 in the MountainCar-v0 environment terminated with the minimum return of -200.0 and the maximum episode length of 200, indicating the agent consistently failed to reach the goal within the time limit. This suggests a common failure pattern where the agent either remains stuck in local minima or lacks sufficient momentum to ascend the hill, likely due to suboptimal action selection (e.g., oscillating ineffectively or not exploiting the environment's physics). The uniformity in returns and episode lengths highlights a persistent inability to improve performance, pointing to issues in policy learning or exploration.\n",
      "Iteration 6, Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a length of 200, indicating that the agent consistently fails to reach the goal within the maximum allowed steps. This pattern suggests the agent is either stuck in suboptimal states (e.g., unable to build enough momentum to climb the hill) or is taking ineffective actions, such as oscillating without progress or repeatedly choosing actions that do not increase velocity toward the goal. The uniformity in returns and episode lengths highlights a persistent failure to learn or exploit the environment's dynamics, likely due to poor exploration, insufficient policy learning, or inadequate reward signal utilization.\n",
      "Iteration 7, Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a maximum episode length of 200 steps, indicating the agent consistently fails to reach the goal and terminates due to the time limit. This pattern suggests the agent is likely stuck in suboptimal state trajectories, unable to build enough momentum to climb the hill. The repeated poor performance across episodes and identical returns imply the policy (version 7) may be selecting actions that are too conservative or not sufficiently exploiting the environment's dynamics, such as oscillating ineffectively at the bottom of the valley. Overall, the key failure pattern is persistent inability to escape the initial low-energy states, leading to maximum-length episodes with minimal progress.\n",
      "Iteration 8, Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The provided MountainCar-v0 episode summaries all show a return of -200.0 and a maximum episode length of 200 steps, indicating that the agent consistently fails to reach the goal within the time limit. This pattern suggests the agent is either stuck oscillating in a limited region of the state space (e.g., unable to build enough momentum to climb the hill) or is taking suboptimal or repetitive actions that do not contribute to progress. The uniform poor return and episode length imply a lack of exploration or ineffective policy updates, possibly resulting in the agent repeatedly selecting actions that do not exploit the environment's dynamics to escape the valley.\n",
      "Iteration 9, Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a maximum episode length of 200 steps, indicating that the agent consistently fails to reach the goal within the allowed time. This suggests a common failure pattern where the agent either gets stuck in local minima or lacks the momentum-building behavior required to solve the environment. The repeated poor returns imply the policy is likely taking suboptimal or random actions, failing to exploit the environment's dynamics (such as swinging back and forth to gain enough speed). Overall, the agent demonstrates ineffective exploration and insufficient learning, resulting in persistent failure across episodes.\n",
      "Iteration 10, Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: All episodes in the MountainCar-v0 environment show a return of -200.0 and a maximum length of 200 steps, indicating the agent consistently fails to reach the goal within the allowed time. This pattern suggests the agent is likely stuck in suboptimal states, such as low velocity or poor position, and is unable to generate enough momentum to escape the valley. The repeated returns and episode lengths imply the policy is either taking ineffective or repetitive actions, such as oscillating without progress or failing to exploit the environment's dynamics. Overall, the key failure is an inability to learn the necessary action sequence to solve the task, resulting in persistent, unproductive behavior.\n",
      "==== Running Pendulum-v1 ====\n",
      "Iteration 1, Moving Avg Return: -1440.23, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the consistently negative returns (ranging from -1247.88 to -1768.80) indicate persistent failure to keep the pendulum upright and stable, a core objective of the environment. The uniform episode length (200 steps) suggests the agent survives for the full duration but fails to achieve efficient control. Common failure patterns likely include the pendulum frequently swinging away from the upright position, with state characteristics showing high angular displacement and velocity. Action issues may involve insufficient or poorly timed torque application, leading to oscillatory or unstable behavior rather than corrective stabilization. Overall, the agent’s policy struggles to minimize energy loss and maintain balance, as reflected in the substantial negative returns.\n",
      "Iteration 2, Moving Avg Return: -1316.43, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes summarized, the most common failure pattern is consistently low returns (ranging from approximately -1249 to -1450), indicating the agent struggles to keep the pendulum upright and stable. All episodes reach the maximum length (200 steps), suggesting persistent but ineffective actions rather than early termination. The negative returns point to frequent large-angle deviations and high angular velocities, characteristic of poor state control. Action-wise, the agent likely applies insufficient or poorly timed torques, failing to counteract the pendulum’s natural tendency to fall. Overall, the key issues are inadequate stabilization strategies and suboptimal action selection, resulting in sustained poor performance throughout each episode.\n",
      "Iteration 3, Moving Avg Return: -852.23, Success Rate: 0.33\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 show a stark contrast in returns, with two episodes yielding highly negative returns (around -1305 and -1250) and one episode achieving near-optimal performance (return ≈ -0.95). The most common failure pattern is characterized by consistently poor returns, suggesting the policy often fails to keep the pendulum upright and stable. This likely results from frequent visits to states with large angular deviations and velocities, causing the agent to apply ineffective or misdirected torques. Action issues may include insufficient corrective force or delayed responses, preventing recovery from unstable states. Overall, the return pattern indicates that while the policy occasionally succeeds, it predominantly struggles with maintaining balance, leading to repeated failures.\n",
      "Iteration 4, Moving Avg Return: -1376.93, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes summarized, the consistently low returns (ranging from -1249 to -1540) over the maximum episode length (200 steps) indicate persistent failure to stabilize the pendulum upright. Common failure patterns likely include the agent frequently allowing the pendulum to swing far from the vertical position, resulting in high negative rewards. Action issues may involve insufficient or poorly timed torque applications, leading to oscillatory or ineffective control. The lack of improvement across episodes with the same policy version suggests the agent struggles with learning effective state-action mappings, particularly in handling the pendulum's angular velocity and position near unstable equilibria. Overall, the agent fails to counteract the pendulum's momentum and maintain balance, as reflected in the consistently poor returns.\n",
      "Iteration 5, Moving Avg Return: -1610.37, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes summarized, the most common failure pattern is consistently low returns (ranging from -1768 to -1429), indicating the agent struggles to keep the pendulum upright and stable. Each episode reaches the maximum length (200 steps), suggesting persistent difficulty rather than early termination. Typical state characteristics likely include frequent large angular deviations and high angular velocities, reflecting poor balance control. Action issues may involve insufficient or poorly timed torque applications, failing to counteract the pendulum’s momentum. Overall, the agent’s policy (version 5) demonstrates limited effectiveness in stabilizing the pendulum, as evidenced by the consistently negative returns and lack of improvement across episodes.\n",
      "Iteration 6, Moving Avg Return: -1422.25, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes summarized, the most common failure patterns include consistently low returns (ranging from approximately -1767 to -1247), indicating the agent struggles to keep the pendulum upright and near its target angle. The episode lengths are all at the maximum (200 steps), suggesting the agent does not terminate early but fails to achieve effective control throughout. Typical state characteristics likely involve the pendulum swinging far from vertical, with high angular velocities persisting. Action issues may include insufficient or poorly timed torque applications, leading to oscillations rather than stabilization. Overall, the agent’s policy version 6 demonstrates persistent difficulty in achieving stable, energy-efficient control, as reflected in the negative return patterns and lack of improvement across episodes.\n",
      "Iteration 7, Moving Avg Return: -832.49, Success Rate: 0.33\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 show a distinct failure pattern characterized by highly negative returns in two out of three episodes (around -1248), indicating the pendulum was likely swinging far from upright or oscillating wildly, which is penalized in this environment. The single near-zero return (-0.72) suggests rare successful stabilization. Common issues likely include poor action selection—such as applying excessive or poorly timed torques—leading to persistent deviation from the upright position. The consistent episode length (200 steps) implies the agent does not terminate early, but often fails to recover from initial instability, resulting in prolonged poor states and low returns.\n",
      "Iteration 8, Moving Avg Return: -1420.31, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the consistently low returns (ranging from -1246 to -1767) indicate persistent failure to keep the pendulum upright and stable. The episode lengths are all at the maximum (200 steps), suggesting the agent survives but accumulates negative rewards throughout, likely due to oscillating or failing to maintain the pendulum near the upright position. Common failure patterns likely include frequent large angular deviations from vertical, insufficient or poorly timed torque applications, and a lack of corrective actions to counteract momentum. The agent’s policy (version 8) appears unable to effectively minimize the pendulum’s angle and velocity, resulting in sustained suboptimal performance.\n",
      "Iteration 9, Moving Avg Return: -943.58, Success Rate: 0.33\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 show a wide variance in returns, with two episodes yielding highly negative returns (-1584.72 and -1245.14) and one episode achieving near-optimal performance (-0.88), all with the same policy version and episode length. This suggests inconsistent policy behavior, likely due to failure patterns such as poor initial state handling (e.g., starting far from upright), suboptimal action selection (possibly excessive or insufficient torque), and difficulty stabilizing the pendulum. The consistently long episode lengths indicate the agent is not terminating early but struggles to maintain control, leading to large negative rewards in most episodes. The key issue appears to be unreliable policy generalization across different initial states or transitions, resulting in frequent failures to keep the pendulum upright and minimize energy expenditure.\n",
      "Iteration 10, Moving Avg Return: -1558.66, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 10 show consistently negative returns (ranging from -1889 to -1341) and fixed episode lengths (200 steps), indicating the agent struggles to keep the pendulum upright and balanced throughout each episode. The most common failure pattern is the inability to maintain the pendulum near the upright position, likely resulting in frequent large-angle deviations and high energy loss. Action-wise, the policy may be producing insufficient or poorly timed torques, failing to counteract the pendulum's swing effectively. The return pattern—persistently low and not improving—suggests the policy is not learning effective stabilization strategies and may be stuck in a suboptimal regime.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "changed part:\n",
    "1.Have the LLM generate the obtained policy in an if-else format, instead of as a black box.\n",
    "2.The prompt emphasizes environment principles, dynamics, and physical knowledge.\n",
    "3.(failed)Introduce a moving average (ma_window=3) to avoid misjudging convergence due to single-iteration fluctuations.\n",
    "4.Add a success_rate metric, convergence is only considered achieved if the success rate ≥ 0.8.\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import inspect\n",
    "import math\n",
    "\n",
    "# ----------------------------\n",
    "# LLM HTTP call function\n",
    "# ----------------------------\n",
    "LLM_URL = 'https://api.yesapikey.com/v1/chat/completions'\n",
    "LLM_HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer sk-PaTN85cFufotMmPm97Ae2546B0874aA29b6a86Ae069b7b4b'\n",
    "}\n",
    "\n",
    "def call_llm(prompt, model=\"gpt-4.1-2025-04-14\", temperature=0.2, max_tokens=1024):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(LLM_URL, json=data, headers=LLM_HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                resp_json = response.json()\n",
    "                if 'choices' in resp_json and resp_json['choices']:\n",
    "                    content = resp_json['choices'][0].get('message', {}).get('content')\n",
    "                    return content\n",
    "        except Exception as e:\n",
    "            print(\"LLM call exception:\", e)\n",
    "        time.sleep(2)\n",
    "\n",
    "# ----------------------------\n",
    "# Environment documentation mapping\n",
    "# ----------------------------\n",
    "ENV_DOC_URL = {\n",
    "    \"Acrobot-v1\": \"https://gymnasium.farama.org/environments/classic_control/acrobot/\",\n",
    "    \"CartPole-v1\": \"https://gymnasium.farama.org/environments/classic_control/cart_pole/\",\n",
    "    \"MountainCarContinuous-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\",\n",
    "    \"MountainCar-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car/\",\n",
    "    \"Pendulum-v1\": \"https://gymnasium.farama.org/environments/classic_control/pendulum/\"\n",
    "}\n",
    "\n",
    "def get_env_doc_url(env_id: str) -> str:\n",
    "    return ENV_DOC_URL.get(env_id, \"https://gymnasium.farama.org/\")\n",
    "\n",
    "# ----------------------------\n",
    "# Static Knowledge\n",
    "# ----------------------------\n",
    "STATIC_KNOWLEDGE = {\n",
    "    \"CartPole-v1\": {\n",
    "        \"state_dim\": 4,\n",
    "        \"state_vars\": [\"cart_position\", \"cart_velocity\", \"pole_angle\", \"pole_velocity\"],\n",
    "        \"state_ranges\": [(-4.8, 4.8), (-float(\"inf\"), float(\"inf\")), (-0.418, 0.418), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1],\n",
    "        \"reward_threshold\": 475,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"Acrobot-v1\": {\n",
    "        \"state_dim\": 6,\n",
    "        \"state_vars\": [\"cos_theta1\", \"sin_theta1\", \"cos_theta2\", \"sin_theta2\", \"theta1_dot\", \"theta2_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\")), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -100,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"MountainCar-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -110,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"MountainCarContinuous-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [-1.0, 1.0],\n",
    "        \"reward_threshold\": 90,\n",
    "        \"action_type\": \"continuous\"\n",
    "    },\n",
    "    \"Pendulum-v1\": {\n",
    "        \"state_dim\": 3,\n",
    "        \"state_vars\": [\"cos_theta\", \"sin_theta\", \"theta_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [-2.0, 2.0],\n",
    "        \"reward_threshold\": -200,\n",
    "        \"action_type\": \"continuous\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Knowledge module\n",
    "# ----------------------------\n",
    "class Knowledge:\n",
    "    def __init__(self):\n",
    "        self.static_knowledge = {}\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def load_static_knowledge(self, env_id):\n",
    "        if env_id not in STATIC_KNOWLEDGE:\n",
    "            raise ValueError(\"Unsupported environment\")\n",
    "        self.static_knowledge = STATIC_KNOWLEDGE[env_id]\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def add_dynamic_entry(self, entry):\n",
    "        self.dynamic_knowledge.append(entry)\n",
    "\n",
    "    def get_dynamic_guidance(self, env_id):\n",
    "        prompt = f\"\"\"\n",
    "I am generating a policy in environment {env_id}.\n",
    "Current dynamic knowledge entries: {self.dynamic_knowledge}\n",
    "\n",
    "Focus on environment **principles, physics, and dynamics**, not superficial patterns.\n",
    "Please provide concise heuristic suggestions for policy generation based on this knowledge, such as:\n",
    "- State ranges to prioritize\n",
    "- Common failing action patterns\n",
    "- Recommended threshold adjustments\n",
    "\n",
    "Return a short, structured bullet list (no prose).\n",
    "\"\"\"\n",
    "        guidance = call_llm(prompt)\n",
    "        return guidance\n",
    "\n",
    "# ----------------------------\n",
    "# Memory module\n",
    "# ----------------------------\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.episodes = []\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.episodes.append({\"steps\": [], \"summary\": None})\n",
    "\n",
    "    def add_step(self, s, a, r, done):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"Please call start_episode() before adding steps!\")\n",
    "        self.episodes[-1][\"steps\"].append({\"s\": s, \"a\": a, \"r\": r, \"done\": done})\n",
    "\n",
    "    def add_episode_summary(self, env_id, policy_version):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"No running episode!\")\n",
    "        steps = self.episodes[-1][\"steps\"]\n",
    "        total_reward = sum(step[\"r\"] for step in steps)\n",
    "        length = len(steps)\n",
    "        self.episodes[-1][\"summary\"] = {\n",
    "            \"env_id\": env_id,\n",
    "            \"policy_version\": policy_version,\n",
    "            \"return\": total_reward,\n",
    "            \"length\": length\n",
    "        }\n",
    "\n",
    "    def get_recent_episodes(self, n=5):\n",
    "        summaries = [ep[\"summary\"] for ep in self.episodes if ep[\"summary\"] is not None]\n",
    "        return summaries[-n:]\n",
    "\n",
    "# ----------------------------\n",
    "# Reflection module\n",
    "# ----------------------------\n",
    "class Reflection:\n",
    "    def __init__(self, knowledge: Knowledge):\n",
    "        self.knowledge = knowledge\n",
    "\n",
    "    def metrics(self, recent_episodes):\n",
    "        returns = [ep[\"return\"] for ep in recent_episodes]\n",
    "        lengths = [ep[\"length\"] for ep in recent_episodes]\n",
    "        avg_return = np.mean(returns) if returns else 0\n",
    "        avg_length = np.mean(lengths) if lengths else 0\n",
    "        # success_rate: fraction of episodes where return >= reward_threshold\n",
    "        threshold = self.knowledge.static_knowledge.get(\"reward_threshold\", 0)\n",
    "        success_count = sum(1 for ep in recent_episodes if ep[\"return\"] >= threshold)\n",
    "        success_rate = success_count / len(recent_episodes) if recent_episodes else 0\n",
    "        return {\"avg_return\": avg_return, \"avg_length\": avg_length, \"success_rate\": success_rate}\n",
    "\n",
    "    def failure_pattern(self, recent_episodes, env_id):\n",
    "        prompt = f\"\"\"\n",
    "I have the following {env_id} environment episode summaries: {recent_episodes}\n",
    "Please analyze the most common failure patterns, including state characteristics, action issues, and return patterns. Focus only on key points.\n",
    "Return a concise paragraph.\n",
    "\"\"\"\n",
    "        pattern = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"failure_pattern\": pattern})\n",
    "        return pattern\n",
    "\n",
    "    def edit_suggestion(self, recent_episodes, env_id):\n",
    "        prompt = f\"\"\"\n",
    "Based on recent episode data from environment {env_id}: {recent_episodes}\n",
    "Generate one policy editing suggestion in one of the following formats:\n",
    "- add_rule(condition -> action)\n",
    "- modify_threshold(variable, old_value, new_value)\n",
    "- reprioritize(rule_i over rule_j)\n",
    "\n",
    "Return exactly one line with one edit.\n",
    "\"\"\"\n",
    "        suggestion = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"edit_suggestion\": suggestion})\n",
    "        return suggestion\n",
    "\n",
    "# ----------------------------\n",
    "# Safe policy call\n",
    "# ----------------------------\n",
    "def safe_policy_call(state, policy_fn, sk):\n",
    "    try:\n",
    "        a = policy_fn(state)\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            a = sk[\"action_space\"][0]\n",
    "        else:\n",
    "            a = (sk[\"action_space\"][0] + sk[\"action_space\"][1]) / 2.0\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        a = np.clip(a, sk[\"action_space\"][0], sk[\"action_space\"][1])\n",
    "    return a\n",
    "\n",
    "# ----------------------------\n",
    "# Safe step wrapper\n",
    "# ----------------------------\n",
    "def safe_step(env, action):\n",
    "    sk = STATIC_KNOWLEDGE[env.unwrapped.spec.id]\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        action = np.array([np.clip(action, sk[\"action_space\"][0], sk[\"action_space\"][1])]) if np.isscalar(action) else np.clip(np.array(action), sk[\"action_space\"][0], sk[\"action_space\"][1])\n",
    "    return env.step(action)\n",
    "\n",
    "# ----------------------------\n",
    "# Policy generation helper with rule-based constraint\n",
    "# ----------------------------\n",
    "def _action_constraints_text(static_knowledge: dict) -> str:\n",
    "    a = static_knowledge[\"action_space\"]\n",
    "    if static_knowledge.get(\"action_type\") == \"discrete\":\n",
    "        return f\"Discrete actions; valid actions are exactly the integers in {a}.\"\n",
    "    else:\n",
    "        lo, hi = a[0], a[1]\n",
    "        return f\"Continuous action; return a single float within [{lo}, {hi}]. Clip if necessary.\"\n",
    "\n",
    "def generate_base_policy(env_id, knowledge: Knowledge):\n",
    "    guidance = knowledge.get_dynamic_guidance(env_id) or \"\"\n",
    "    doc_url = get_env_doc_url(env_id)\n",
    "    sk = knowledge.static_knowledge\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "    state_vars_text = \"\\n\".join([f\"- {name} in range {rng}\" for name, rng in zip(sk[\"state_vars\"], sk[\"state_ranges\"])])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are writing a deterministic, white-box **rule-based policy** for Gymnasium environment \"{env_id}\".\n",
    "Focus on **environment principles, physics, and dynamics**, not superficial patterns.\n",
    "The policy must be based on simple if-else statements or threshold comparisons using state variables.\n",
    "Environment documentation: {doc_url}\n",
    "\n",
    "Observation (state vector):\n",
    "{state_vars_text}\n",
    "\n",
    "Action constraints:\n",
    "- {action_desc}\n",
    "- May import 'math' if needed\n",
    "- Must be deterministic\n",
    "- Do not use loops, functions, or external libraries except math\n",
    "- Example: if state[2] > 0: return 1 else: return 0\n",
    "\n",
    "Dynamic guidance:\n",
    "{guidance}\n",
    "\n",
    "Output requirements:\n",
    "- Only one Python function: def policy(state): ...\n",
    "- No explanations, no markdown, no print\n",
    "- Returned action strictly satisfies constraints\n",
    "\"\"\"\n",
    "    policy_code = call_llm(prompt)\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    try:\n",
    "        exec(policy_code, local_vars)\n",
    "        policy_fn = local_vars.get(\"policy\")\n",
    "        if policy_fn is None:\n",
    "            if sk[\"action_type\"] == \"discrete\":\n",
    "                def policy_fn(state): return sk[\"action_space\"][0]\n",
    "            else:\n",
    "                lo, hi = sk[\"action_space\"]\n",
    "                def policy_fn(state): return (lo + hi)/2.0\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            def policy_fn(state): return sk[\"action_space\"][0]\n",
    "        else:\n",
    "            lo, hi = sk[\"action_space\"]\n",
    "            def policy_fn(state): return (lo + hi)/2.0\n",
    "    return policy_fn\n",
    "\n",
    "def apply_edit(policy_fn, edit_text, knowledge: Knowledge):\n",
    "    sk = knowledge.static_knowledge\n",
    "    try:\n",
    "        existing_src = inspect.getsource(policy_fn)\n",
    "    except Exception:\n",
    "        existing_src = \"def policy(state):\\n    return \" + (str(sk[\"action_space\"][0]) if sk[\"action_type\"]==\"discrete\" else str((sk[\"action_space\"][0]+sk[\"action_space\"][1])/2.0))\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "\n",
    "    doc_url = get_env_doc_url(knowledge.dynamic_knowledge[0].get(\"env_id\", \"\"))\n",
    "    prompt = f\"\"\"\n",
    "Revise deterministic, **rule-based** policy for environment at {doc_url}.\n",
    "Focus on physics, dynamics, and environment principles.\n",
    "Constraints: {action_desc}\n",
    "Current policy:\n",
    "{existing_src}\n",
    "Edit suggestion: {edit_text}\n",
    "You may use 'math' module.\n",
    "- Must remain if-else or threshold based\n",
    "\n",
    "Output only a valid Python function def policy(state): ...\n",
    "\"\"\"\n",
    "    policy_code = call_llm(prompt)\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    try:\n",
    "        exec(policy_code, local_vars)\n",
    "        new_policy_fn = local_vars.get(\"policy\")\n",
    "        return new_policy_fn if new_policy_fn else policy_fn\n",
    "    except Exception:\n",
    "        return policy_fn\n",
    "\n",
    "# ----------------------------\n",
    "# Main closed-loop training\n",
    "# ----------------------------\n",
    "def run_env_loop(env_id, max_iters=10, episodes_per_iter=10, ma_window=3, success_rate_threshold=0.8):\n",
    "    knowledge = Knowledge()\n",
    "    knowledge.load_static_knowledge(env_id)\n",
    "    memory = Memory()\n",
    "    reflection = Reflection(knowledge)\n",
    "    policy_version = 0\n",
    "    first_iter = True\n",
    "    policy_fn = None\n",
    "\n",
    "    for iter_idx in range(max_iters):\n",
    "        policy_version += 1\n",
    "        if first_iter:\n",
    "            policy_fn = generate_base_policy(env_id, knowledge)\n",
    "            first_iter = False\n",
    "        else:\n",
    "            recent_episodes = memory.get_recent_episodes()\n",
    "            suggestion = reflection.edit_suggestion(recent_episodes, env_id)\n",
    "            policy_fn = apply_edit(policy_fn, suggestion, knowledge)\n",
    "\n",
    "        env = gym.make(env_id)\n",
    "        episode_rewards = []\n",
    "\n",
    "        for ep in range(episodes_per_iter):\n",
    "            s, _ = env.reset()\n",
    "            done = False\n",
    "            memory.start_episode()\n",
    "            while not done:\n",
    "                a = safe_policy_call(s, policy_fn, knowledge.static_knowledge)\n",
    "                s_next, r, terminated, truncated, info = safe_step(env, a)\n",
    "                done = terminated or truncated\n",
    "                memory.add_step(s, a, r, done)\n",
    "                s = s_next\n",
    "            memory.add_episode_summary(env_id, policy_version)\n",
    "            episode_rewards.append(memory.episodes[-1][\"summary\"][\"return\"])\n",
    "\n",
    "        recent_episodes = memory.get_recent_episodes(n=ma_window)\n",
    "        metrics = reflection.metrics(recent_episodes)\n",
    "        print(f\"Iteration {iter_idx+1}, Moving Avg Return: {metrics['avg_return']:.2f}, Success Rate: {metrics['success_rate']:.2f}\")\n",
    "\n",
    "        pattern = reflection.failure_pattern(recent_episodes, env_id)\n",
    "        print(\"Failure Pattern:\", pattern)\n",
    "\n",
    "        threshold = knowledge.static_knowledge.get(\"reward_threshold\", 0)\n",
    "        if metrics[\"avg_return\"] >= threshold and metrics[\"success_rate\"] >= success_rate_threshold:\n",
    "            print(f\"Converged! Stop training. Moving Avg Return={metrics['avg_return']:.2f}, Success Rate={metrics['success_rate']:.2f}\")\n",
    "            break\n",
    "\n",
    "# ----------------------------\n",
    "# Run multiple control tasks\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    env_list = [\n",
    "        \"Acrobot-v1\",\n",
    "        \"CartPole-v1\",\n",
    "        \"MountainCarContinuous-v0\",\n",
    "        \"MountainCar-v0\",\n",
    "        \"Pendulum-v1\"\n",
    "    ]\n",
    "    for env_id in env_list:\n",
    "        print(f\"==== Running {env_id} ====\")\n",
    "        run_env_loop(env_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844fbc14-1b47-424f-a214-880ede63ea26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_rl",
   "language": "python",
   "name": "dl_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
