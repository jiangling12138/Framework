{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607d07f8-98c5-4910-8ea2-a3c954602773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Running Acrobot-v1 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 consistently show a return of -500.0 and a maximum episode length of 500 steps, indicating that the agent repeatedly fails to solve the task within the allowed time. This pattern suggests the agent is likely stuck in suboptimal states, unable to swing the lower link above the required threshold. Common failure characteristics include insufficient exploration or ineffective action selection, possibly resulting in repetitive or non-contributory actions that do not generate upward momentum. The uniformity in returns and episode lengths points to a policy that is either random or poorly trained, unable to escape local minima or learn the necessary dynamics to achieve the goal.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: -382.67, Success Rate: 0.00\n",
      "Failure Pattern: The most common failure pattern in these Acrobot-v1 episodes is the agent consistently reaching the maximum episode length (500 steps) with the lowest possible return (-500.0), indicating it repeatedly fails to achieve the task objective (swinging the end-effector above a certain height). This suggests persistent issues with either ineffective exploration or suboptimal action selection, likely resulting in the agent getting stuck in unproductive state trajectories without making meaningful progress. The occasional higher return (-148.0 over 149 steps) shows sporadic success, but the dominant pattern is stagnation and failure to terminate episodes early by solving the task. Overall, the key issues are insufficiently diverse or effective actions and an inability to escape poor state sequences, as reflected in the return and episode length patterns.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 all show a return of -500.0 and a maximum episode length of 500 steps, indicating the agent consistently fails to solve the task within the allowed time. This pattern suggests the policy (version 3) is unable to swing the acrobot’s end-effector to the target height, likely due to ineffective action selection—such as repeated or random actions that do not exploit the environment’s dynamics. The persistent minimum return and maximum length imply the agent remains stuck in suboptimal states, possibly oscillating near the starting position without generating sufficient momentum. Overall, the key failure points are poor exploration, lack of coordinated actions, and inability to escape low-reward states.\n",
      "=== Iteration 4 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 consistently show a return of -500.0 and maximum episode length (500 steps), indicating the agent repeatedly fails to solve the task and never achieves the goal state. This pattern suggests the agent is either stuck in unproductive state trajectories—such as swinging without gaining enough momentum to reach the target height—or is selecting ineffective or repetitive actions that do not progress toward success. The uniformity in returns and lengths points to a lack of exploration or poor policy learning, with the agent possibly trapped in local minima or default behaviors that do not exploit the environment's dynamics.\n",
      "=== Iteration 5 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episodes for Acrobot-v1 all show a return of -500.0 and maximum length of 500 steps, indicating the agent consistently fails to solve the task within the allowed time. This pattern suggests the policy (version 5) is unable to swing the acrobot to the target height, likely due to ineffective action selection—such as repetitive or suboptimal torque applications. The persistent failure implies the agent may be stuck in unproductive state trajectories, possibly oscillating near the starting position without generating sufficient momentum. Overall, the key issues are stagnant state transitions, poor action choices, and a return pattern reflecting repeated unsuccessful episodes.\n",
      "=== Iteration 6 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 consistently show a return of -500.0 and a maximum episode length of 500 steps, indicating that the agent repeatedly fails to solve the environment within the allowed time. This pattern suggests persistent issues such as the agent getting stuck in suboptimal states, possibly near the bottom of the swing, and failing to generate sufficient upward momentum. The uniformity across episodes and policy version implies that the policy may be deterministic or poorly trained, often selecting ineffective or repetitive actions that do not progress toward the goal. Overall, the key failure pattern is a lack of exploration or learning, resulting in consistently poor returns and no successful episodes.\n",
      "=== Iteration 7 ===\n",
      "Moving Avg Return: -443.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 reveal a recurring failure pattern characterized by frequent maximum-length episodes (length 500) with consistently poor returns (-500.0), indicating the agent often fails to solve the task within the allowed steps. This suggests the policy struggles to escape from initial or suboptimal states, likely getting stuck in low-momentum configurations or failing to generate sufficient swing-up motion. The occasional shorter episode with a better return (-329.0, length 330) hints at sporadic partial progress, but overall, the agent's actions may lack effective exploration or coordination, resulting in repeated inability to reach the goal state. The return pattern highlights a need for improved action selection and state transition strategies.\n",
      "=== Iteration 8 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: All three Acrobot-v1 episodes terminated at the maximum length of 500 steps with the minimum possible return of -500.0, indicating that the agent consistently failed to solve the task or reach the goal state. This pattern suggests the agent is likely stuck in unproductive state trajectories, possibly oscillating or remaining in suboptimal configurations without making upward progress. The uniformity of returns and episode lengths points to a policy that is either taking ineffective or repetitive actions, failing to exploit the environment's dynamics. Overall, the agent exhibits persistent failure to escape poor states, likely due to inadequate exploration or a poorly learned policy.\n",
      "=== Iteration 9 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 all show a return of -500.0 and a maximum episode length of 500 steps, indicating that the agent consistently fails to solve the task within the allowed time. This pattern suggests the agent is unable to swing the lower link above the target line, likely remaining in suboptimal or stagnant states without significant progress. The repeated maximum negative return implies ineffective exploration or poor policy learning, possibly due to consistently choosing non-contributive or oscillatory actions that do not build sufficient momentum. Overall, the agent is stuck in a failure mode characterized by lack of upward progress, repetitive action selection, and no improvement across episodes.\n",
      "=== Iteration 10 ===\n",
      "Moving Avg Return: -500.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 consistently show a return of -500.0 and maximum episode length (500 steps), indicating the agent repeatedly fails to solve the task and never achieves the goal state. This pattern suggests the agent is likely stuck in unproductive state trajectories, possibly oscillating or remaining in low-reward regions without effective exploration. Action selection may be ineffective or deterministic, failing to generate the necessary torque to swing the end-effector to the target height. The uniformity in returns and lengths highlights a lack of learning progress or policy improvement at version 10, with the agent unable to escape persistent failure modes.\n",
      "Consecutive truncated >= 10. Regenerating new policy via LLM.\n",
      "==== Running CartPole-v1 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: 500.00, Success Rate: 1.00\n",
      "Failure Pattern: The provided CartPole-v1 episode summaries all show maximum returns and episode lengths (500.0 and 500), indicating that the agent consistently succeeds without failure. There are no observed failure patterns in these episodes: the agent maintains balance throughout, suggesting effective state management (pole angle and cart position remain within limits) and appropriate action selection (left/right moves prevent pole from falling). The uniformity in returns and lengths implies stable policy performance, with no premature terminations or suboptimal actions. Thus, no common failure states, action errors, or low-return patterns are present in this data.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: 500.00, Success Rate: 1.00\n",
      "Failure Pattern: Based on the provided episode summaries, all episodes in the CartPole-v1 environment achieved the maximum possible return and length (500), indicating that the agent consistently succeeded without any failures. Therefore, there are no observable failure patterns in terms of state characteristics or action issues; the policy (version 2) reliably maintained balance throughout each episode. The return pattern is optimal and uniform, suggesting robust performance and no recurring weaknesses in the agent’s decision-making or environment interaction.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: 500.00, Success Rate: 1.00\n",
      "Failure Pattern: Based on the provided episode summaries, all episodes in the CartPole-v1 environment achieved the maximum return of 500.0 with episode lengths of 500, indicating no failures occurred during these runs. There are no observable failure patterns in terms of state characteristics or action issues, as the agent consistently maintained balance throughout each episode. The uniformity in returns and lengths suggests stable and optimal policy performance, with no evidence of instability, poor action selection, or problematic states.\n",
      "Converged! Stop training. Moving Avg Return=500.00, Success Rate=1.00\n",
      "==== Running MountainCarContinuous-v0 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: -99.90, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 show repeated failures, with each episode reaching the maximum length of 999 steps and a consistently poor return of approximately -99.9. This pattern suggests the agent is unable to reach the goal, likely remaining stuck in low-energy states near the valley without generating sufficient momentum to climb the hill. The actions taken may be too weak, poorly timed, or lack the necessary oscillatory pattern to build up speed, indicating ineffective exploration or suboptimal policy. The uniformity in returns and episode lengths further points to a deterministic failure mode, where the agent repeatedly fails in the same way across episodes.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: -99.90, Success Rate: 0.00\n",
      "Failure Pattern: The episodes from the MountainCarContinuous-v0 environment consistently show a return of approximately -99.9 and reach the maximum length of 999 steps, indicating the agent repeatedly fails to reach the goal. This pattern suggests the agent is likely stuck in low-velocity, low-position states near the valley, unable to generate sufficient momentum to climb the hill. The actions taken are probably too weak or poorly timed, resulting in ineffective exploration and insufficient propulsion. The uniformity in returns and episode lengths highlights a persistent inability to escape the initial state region, reflecting a common failure mode where the policy does not learn the necessary forceful, oscillatory actions required for success in this environment.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: -99.90, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 consistently show a return of approximately -99.9 and maximum episode length (999 steps), indicating the agent repeatedly fails to reach the goal within the allowed time. This pattern suggests the agent is likely stuck in low-velocity states near the bottom of the valley, unable to generate enough momentum to ascend the hill. The actions taken may be too weak, poorly timed, or oscillatory, preventing effective exploration and progress. The uniformity in returns and episode lengths highlights a persistent failure to escape the initial region, reflecting insufficient policy learning or exploration.\n",
      "=== Iteration 4 ===\n",
      "Moving Avg Return: -99.90, Success Rate: 0.00\n",
      "Failure Pattern: The episodes consistently show a return of approximately -99.9 and reach the maximum length of 999 steps, indicating the agent repeatedly fails to solve the MountainCarContinuous-v0 task. This pattern suggests the agent is unable to reach the goal, likely remaining stuck in low-velocity or low-position states near the valley. Action selection may be ineffective, possibly producing insufficient or poorly timed force to escape the valley. The uniformity in returns and episode lengths highlights a persistent failure to learn or execute the necessary strategy for success, such as building momentum by alternating actions.\n",
      "=== Iteration 5 ===\n",
      "Moving Avg Return: -99.90, Success Rate: 0.00\n",
      "Failure Pattern: The episodes from the MountainCarContinuous-v0 environment consistently show a return of approximately -99.9 and reach the maximum episode length of 999 steps, indicating the agent repeatedly fails to reach the goal. This pattern suggests the agent is likely stuck in low-velocity states near the valley, unable to generate enough momentum to climb the hill. Action selection may be suboptimal, possibly oscillating between weak or indecisive thrusts rather than committing to sustained acceleration in one direction. The uniformity in returns and episode lengths highlights a persistent inability to escape the initial state region, reflecting a lack of effective exploration or exploitation in the policy.\n",
      "=== Iteration 6 ===\n",
      "Moving Avg Return: -99.90, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 consistently show a return of approximately -99.9 and the maximum episode length of 999 steps, indicating that the agent repeatedly fails to reach the goal within the allowed time. This pattern suggests the agent is likely stuck in suboptimal states, such as low velocity or insufficient position progress, and is unable to generate effective actions to escape the valley. The uniformity in returns and episode length points to a persistent issue with the policy, possibly producing weak or repetitive actions that do not exploit the environment's dynamics for momentum buildup. Overall, the agent exhibits a lack of exploration or inadequate force application, resulting in prolonged episodes without success.\n",
      "=== Iteration 7 ===\n",
      "Moving Avg Return: -99.90, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 indicate a consistent failure pattern: each episode reaches the maximum length of 999 steps with a low return of approximately -99.9. This suggests the agent is unable to reach the goal, likely oscillating near the starting position or moving inefficiently. Common state characteristics in such failures include the car remaining stuck in valleys or failing to build enough momentum to ascend the hill. Action issues may involve insufficient or poorly timed force application, resulting in ineffective movement. The uniformity in returns and episode lengths highlights a lack of learning progress or exploration, with the agent repeatedly exhibiting the same unsuccessful behavior.\n",
      "=== Iteration 8 ===\n",
      "Moving Avg Return: -99.90, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 consistently show a return of approximately -99.9 and maximum episode length (999 steps), indicating the agent repeatedly fails to reach the goal. This pattern suggests the agent is likely stuck in suboptimal states, such as low velocity near the valley, unable to generate enough momentum to climb the hill. The actions taken may be too weak or poorly timed, failing to exploit the environment’s dynamics for acceleration. The uniformity in returns and episode lengths highlights a persistent inability to escape the initial region, reflecting ineffective exploration or policy stagnation.\n",
      "=== Iteration 9 ===\n",
      "Moving Avg Return: -99.90, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 show repeated failures, with each episode reaching the maximum length of 999 steps and a consistently poor return of approximately -99.9. This pattern suggests the agent is unable to reach the goal, likely remaining stuck in low-velocity or low-position states without generating enough momentum to ascend the hill. The repeated returns and lengths indicate the policy is either taking ineffective or near-zero actions, failing to exploit the environment’s dynamics. Overall, the agent exhibits a lack of exploration or insufficient force application, resulting in prolonged episodes with minimal progress and consistently negative returns.\n",
      "=== Iteration 10 ===\n",
      "Moving Avg Return: -99.90, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCarContinuous-v0 consistently show a return of approximately -99.9 and a maximum episode length of 999 steps, indicating that the agent repeatedly fails to reach the goal within the allowed time. This pattern suggests the agent is likely stuck in suboptimal states, such as low velocity or poor position near the valley, and is unable to generate sufficient momentum to ascend the hill. The repeated returns and lengths imply ineffective action selection, possibly oscillating or applying insufficient force, leading to prolonged episodes without progress. Overall, the key failure points are persistent low returns, maximum episode duration, and likely poor exploration or exploitation of the environment’s dynamics.\n",
      "Consecutive truncated >= 10. Regenerating new policy via LLM.\n",
      "==== Running MountainCar-v0 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: -123.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 show consistently negative returns close to the episode lengths, indicating that the agent typically fails to reach the goal and instead exhausts the maximum allowed steps. This pattern suggests the agent often gets stuck oscillating between states with insufficient momentum, likely near the bottom of the valley, without effectively building up speed to escape. Action selection may be suboptimal, with the policy possibly alternating actions without strategic planning to maximize velocity. The returns closely matching the episode lengths further confirm that the agent is not achieving early success, highlighting a lack of effective exploration or exploitation in the state space.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: -122.67, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 show consistently negative returns close to -120, with episode lengths matching the absolute value of the returns, indicating that the agent typically fails to reach the goal and terminates due to the time limit. This pattern suggests the agent often gets stuck in low-velocity states near the bottom of the hill, unable to generate enough momentum to reach the flag. Action selection likely lacks sufficient exploration or fails to exploit the environment’s dynamics, resulting in repetitive, suboptimal moves. Overall, the key failure pattern is insufficient energy accumulation, reflected in long episodes with poor returns and little progress toward the goal state.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: -122.67, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 3 show consistently low returns (around -122 to -123) and episode lengths near the maximum (122–123 steps), indicating the agent frequently fails to reach the goal before the time limit. This pattern suggests the agent struggles to build sufficient momentum, likely due to suboptimal action choices—such as not alternating left and right actions effectively to escape the valley. The persistent negative returns and long episode durations point to a failure in exploiting the environment’s dynamics, with the agent often remaining in low-velocity, low-position states near the bottom of the hill.\n",
      "=== Iteration 4 ===\n",
      "Moving Avg Return: -121.33, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 indicate consistently poor performance, with returns of -121 and -122 and episode lengths matching the returns, suggesting the agent typically fails to reach the goal and terminates at the maximum step limit. This pattern is characteristic of agents that struggle to build sufficient momentum, often remaining stuck in local minima near the valley or failing to exploit the environment’s dynamics. Common failure states likely involve the car oscillating near the starting position without gaining enough speed to ascend the hill. Action issues may include suboptimal or repetitive choices, such as alternating left and right without strategic acceleration, preventing escape from the valley. Overall, the agent’s policy version 4 does not effectively solve the environment, as evidenced by the consistently negative returns and maximum-length episodes.\n",
      "=== Iteration 5 ===\n",
      "Moving Avg Return: -122.67, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 5 show consistently negative returns (-121.0, -124.0, -123.0) and episode lengths matching the absolute value of the returns, indicating the agent typically fails to reach the goal before the maximum allowed steps. This pattern suggests the agent often gets stuck in low-velocity or suboptimal positions, unable to build enough momentum to escape the valley. Common failure characteristics likely include insufficient exploration of the leftmost states and poor timing of acceleration actions, resulting in repeated ineffective movements and prolonged episodes. The returns and lengths reflect a lack of progress, with the agent frequently oscillating without achieving the goal state.\n",
      "=== Iteration 6 ===\n",
      "Moving Avg Return: -122.33, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 show consistently low returns (ranging from -121.0 to -124.0) and episode lengths matching the negative returns, indicating the agent typically reaches the goal at the last possible moment or fails to do so efficiently. This pattern suggests common failures such as insufficient momentum buildup, poor timing in switching between acceleration actions, and possibly frequent oscillation near the valley without escaping. State-wise, the agent likely spends excessive time in low-velocity, low-position regions, unable to leverage the environment’s dynamics. Action-wise, suboptimal or indecisive choices prevent early escape, leading to long episodes and minimal reward improvement across runs.\n",
      "=== Iteration 7 ===\n",
      "Moving Avg Return: -123.33, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 7 consistently show returns of -122.0 to -124.0 and episode lengths matching the returns, indicating the agent typically fails to reach the goal and exhausts the maximum allowed steps. This suggests common failure patterns include insufficient momentum buildup, likely due to suboptimal action selection (e.g., not alternating left and right actions effectively to gain speed). State characteristics likely involve the car remaining near the valley or failing to reach the rightmost position, while the return pattern reflects repeated unsuccessful episodes with little variation, pointing to persistent policy shortcomings rather than sporadic mistakes.\n",
      "=== Iteration 8 ===\n",
      "Moving Avg Return: -122.33, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 8 consistently show low returns (-121.0, -123.0, -123.0) and episode lengths matching the negative returns, indicating the agent typically fails to reach the goal before the time limit. This pattern suggests the agent often gets stuck oscillating between states with insufficient momentum, likely near the valley or slopes, without effectively building up speed to escape. Action selection issues may include repetitive or poorly timed accelerations, preventing the agent from leveraging the environment’s physics. Overall, the key failure pattern is inadequate exploration and momentum-building, resulting in premature episode termination and consistently poor returns.\n",
      "=== Iteration 9 ===\n",
      "Moving Avg Return: -122.67, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 9 show consistently negative returns (-121.0 to -124.0) and episode lengths matching the absolute return values, indicating the agent typically reaches the goal at the last possible moment. This pattern suggests the agent struggles to efficiently build momentum, likely oscillating near the valley without effectively leveraging the environment’s dynamics. Common failure characteristics include insufficient exploration of high-potential states (near the slopes), suboptimal action selection (possibly alternating between left and right without sustained acceleration), and a lack of early progress, resulting in minimal improvement across episodes. The returns and lengths imply the agent is not exploiting shortcuts or optimal trajectories, instead relying on the environment’s time limit to terminate episodes.\n",
      "=== Iteration 10 ===\n",
      "Moving Avg Return: -123.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 indicate a consistent failure pattern: each episode ends with a return of -123.0 and a length of 123 steps, suggesting the agent repeatedly fails to reach the goal within the maximum allowed steps. This typically reflects insufficient exploration or suboptimal policy, where the agent may oscillate near the starting position or fail to build enough momentum to climb the hill. Common state characteristics in such failures include low velocity and positions near the valley. Action issues likely involve repetitive or poorly timed left/right moves that do not exploit the environment's physics. The uniform negative return and episode length highlight a lack of learning progress with policy_version 10.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-inf\n",
      "==== Running Pendulum-v1 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: -750.67, Success Rate: 0.33\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 1 show significant variability in returns, ranging from -1129.12 to -138.62, all with the maximum episode length of 200 steps. The most common failure pattern is consistently low returns, indicating the agent often struggles to keep the pendulum upright and stable. This suggests issues with either the policy's ability to select appropriate torque actions or insufficient exploration during training. The extremely low returns (e.g., -1129.12) likely correspond to episodes where the pendulum remains far from the upright position, while the less negative return (-138.62) suggests occasional partial success. Overall, the agent's actions may be too weak, inconsistent, or poorly timed, leading to persistent deviations from the upright state and suboptimal performance.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: -1157.52, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the consistently low returns (ranging from -817 to -1487) indicate frequent failure to keep the pendulum upright and near its target angle. The fixed episode length (200 steps) suggests the agent often survives the full duration but accumulates substantial negative rewards, likely due to persistent oscillations or inability to stabilize the pendulum. Common failure patterns likely include states with large angular deviations and high velocities, and actions that either overcorrect or underactuate, preventing smooth recovery. Overall, the agent struggles with precise control, leading to inefficient energy use and poor stabilization throughout each episode.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: -1398.53, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 3 consistently show low returns (ranging from approximately -1193 to -1509), indicating the agent struggles to keep the pendulum upright and stable. The fixed episode length of 200 suggests the agent rarely achieves sustained control, likely failing to minimize the pendulum's angle and velocity over time. Common failure patterns may include frequent large corrective actions or oscillatory behavior, leading to high energy expenditure and poor reward accumulation. These results point to issues with policy stability, such as inadequate exploration or suboptimal action selection, preventing the agent from learning effective strategies for balancing the pendulum.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-inf\n",
      "=== Iteration 4 ===\n",
      "Moving Avg Return: -1215.00, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes summarized, the most common failure patterns include consistently low returns (ranging from -1783.59 to -633.76), indicating poor control over the pendulum’s upright position. The fixed episode length of 200 steps suggests the agent often fails to stabilize the pendulum early, leading to prolonged periods of suboptimal states—typically with the pendulum far from vertical and high angular velocities. Action issues likely involve insufficient or poorly timed torque applications, preventing effective correction of the pendulum’s swing. Overall, the key pattern is persistent instability and inefficient action choices, resulting in high negative returns and failure to achieve the environment’s objective.\n",
      "=== Iteration 5 ===\n",
      "Moving Avg Return: -1067.76, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 5 show consistently negative returns, ranging from -1249.31 to -862.78, all over the maximum episode length of 200 steps. This suggests the agent struggles to keep the pendulum upright and stable, likely failing to minimize the angle from vertical or to control angular velocity effectively. The high negative returns indicate frequent large deviations from the upright position, possibly due to suboptimal or inconsistent torque actions. The uniform episode length implies the agent survives the full duration but fails to achieve meaningful control, pointing to persistent issues in action selection and state stabilization rather than early episode termination.\n",
      "=== Iteration 6 ===\n",
      "Moving Avg Return: -1083.42, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes summarized, the most common failure patterns include consistently low returns (ranging from -1263 to -986), indicating the agent struggles to keep the pendulum upright and near its target state. The episode lengths are all at the maximum (200 steps), suggesting the agent does not terminate early due to success but rather persists in suboptimal behavior. Typical state characteristics likely involve the pendulum swinging far from vertical, with high angular velocities and frequent deviations from the upright position. Action issues may include insufficient or poorly timed torque applications, failing to counteract the pendulum’s momentum effectively. Overall, the agent’s policy (version 6) appears unable to stabilize the pendulum, leading to repeated, prolonged episodes with poor cumulative rewards.\n",
      "=== Iteration 7 ===\n",
      "Moving Avg Return: -1263.59, Success Rate: 0.00\n",
      "Failure Pattern: Across the provided Pendulum-v1 episodes, the returns are consistently low (ranging from -1350.40 to -1139.52), indicating that the agent struggles to keep the pendulum upright and stable. The fixed episode length of 200 suggests the agent survives the full duration but fails to achieve efficient control. Common failure patterns likely include the pendulum swinging erratically or remaining far from the upright position, leading to persistent negative rewards. Action-wise, the agent may be applying insufficient or poorly timed torques, failing to counteract the pendulum's momentum effectively. Overall, the policy appears unable to stabilize the pendulum, resulting in consistently suboptimal returns.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-inf\n",
      "=== Iteration 8 ===\n",
      "Moving Avg Return: -1392.66, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episode summaries, the most common failure patterns include consistently low returns (ranging from approximately -1592 to -1046), indicating the agent struggles to keep the pendulum upright and near its target state. The episode lengths are always 200, suggesting the agent rarely achieves early success or termination, likely oscillating or failing to stabilize the pendulum. Typical state characteristics in such failures involve large angular deviations and high angular velocities, while action issues often include insufficient or poorly timed torque applications, preventing effective correction of the pendulum’s motion. Overall, the agent’s policy version 8 demonstrates persistent difficulty in maintaining balance, as reflected by the negative returns and lack of improvement across episodes.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-inf\n",
      "=== Iteration 9 ===\n",
      "Moving Avg Return: -1064.52, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes summarized, the most common failure patterns include consistently low returns (ranging from -1286 to -937), indicating poor control over the pendulum's upright position. The episode lengths are all at the maximum (200 steps), suggesting the agent survives but fails to stabilize the pendulum effectively. Typical state characteristics likely involve the pendulum remaining far from vertical, with high angular velocity and frequent oscillations. Action issues may include insufficient or poorly timed torque applications, leading to persistent swinging rather than correction. Overall, the agent's policy struggles to minimize energy loss and maintain balance, as reflected in the negative returns and lack of improvement across episodes.\n",
      "=== Iteration 10 ===\n",
      "Moving Avg Return: -1102.69, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the most common failure patterns include consistently low returns (ranging from -1590 to -747), indicating poor stabilization of the pendulum. This suggests frequent deviations from the upright position, likely due to suboptimal action choices such as insufficient or excessive torque application. The uniform episode length (200 steps) implies the policy struggles throughout the entire episode rather than recovering mid-way. Overall, the key issues are persistent instability in state (pendulum angle and velocity) and ineffective actions, leading to high cumulative penalties and limited control performance.\n",
      "Consecutive truncated >= 10. Regenerating new policy via LLM.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "changed part:\n",
    "Rollback mechanism added\n",
    "\n",
    "problem:\n",
    "Easily converges to local optima (rules are too rigid).\n",
    "Lacks diversity in exploration.\n",
    "\"\"\"\n",
    "\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import inspect\n",
    "import math\n",
    "\n",
    "# ----------------------------\n",
    "# LLM HTTP call function\n",
    "# ----------------------------\n",
    "LLM_URL = 'https://api.yesapikey.com/v1/chat/completions'\n",
    "LLM_HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer sk-DygYzdGba7V5ggRwDf0d28B193D84c90Af2eE34b68C1C892'\n",
    "}\n",
    "\n",
    "def call_llm(prompt, model=\"gpt-4.1-2025-04-14\", temperature=0.2, max_tokens=1024):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(LLM_URL, json=data, headers=LLM_HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                resp_json = response.json()\n",
    "                if 'choices' in resp_json and resp_json['choices']:\n",
    "                    content = resp_json['choices'][0].get('message', {}).get('content')\n",
    "                    return content\n",
    "        except Exception as e:\n",
    "            print(\"LLM call exception:\", e)\n",
    "        time.sleep(2)\n",
    "\n",
    "# ----------------------------\n",
    "# Environment documentation mapping\n",
    "# ----------------------------\n",
    "ENV_DOC_URL = {\n",
    "    \"Acrobot-v1\": \"https://gymnasium.farama.org/environments/classic_control/acrobot/\",\n",
    "    \"CartPole-v1\": \"https://gymnasium.farama.org/environments/classic_control/cart_pole/\",\n",
    "    \"MountainCarContinuous-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\",\n",
    "    \"MountainCar-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car/\",\n",
    "    \"Pendulum-v1\": \"https://gymnasium.farama.org/environments/classic_control/pendulum/\"\n",
    "}\n",
    "\n",
    "def get_env_doc_url(env_id: str) -> str:\n",
    "    return ENV_DOC_URL.get(env_id, \"https://gymnasium.farama.org/\")\n",
    "\n",
    "# ----------------------------\n",
    "# Static Knowledge\n",
    "# ----------------------------\n",
    "STATIC_KNOWLEDGE = {\n",
    "    \"CartPole-v1\": {\n",
    "        \"state_dim\": 4,\n",
    "        \"state_vars\": [\"cart_position\", \"cart_velocity\", \"pole_angle\", \"pole_velocity\"],\n",
    "        \"state_ranges\": [(-4.8, 4.8), (-float(\"inf\"), float(\"inf\")), (-0.418, 0.418), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1],\n",
    "        \"reward_threshold\": 475,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"Acrobot-v1\": {\n",
    "        \"state_dim\": 6,\n",
    "        \"state_vars\": [\"cos_theta1\", \"sin_theta1\", \"cos_theta2\", \"sin_theta2\", \"theta1_dot\", \"theta2_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\")), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -100,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"MountainCar-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -110,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"MountainCarContinuous-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [-1.0, 1.0],\n",
    "        \"reward_threshold\": 90,\n",
    "        \"action_type\": \"continuous\"\n",
    "    },\n",
    "    \"Pendulum-v1\": {\n",
    "        \"state_dim\": 3,\n",
    "        \"state_vars\": [\"cos_theta\", \"sin_theta\", \"theta_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [-2.0, 2.0],\n",
    "        \"reward_threshold\": -200,\n",
    "        \"action_type\": \"continuous\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Knowledge module\n",
    "# ----------------------------\n",
    "class Knowledge:\n",
    "    def __init__(self):\n",
    "        self.static_knowledge = {}\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def load_static_knowledge(self, env_id):\n",
    "        if env_id not in STATIC_KNOWLEDGE:\n",
    "            raise ValueError(\"Unsupported environment\")\n",
    "        self.static_knowledge = STATIC_KNOWLEDGE[env_id]\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def add_dynamic_entry(self, entry):\n",
    "        self.dynamic_knowledge.append(entry)\n",
    "\n",
    "    def get_dynamic_guidance(self, env_id):\n",
    "        prompt = f\"\"\"\n",
    "I am generating a policy in environment {env_id}.\n",
    "Current dynamic knowledge entries: {self.dynamic_knowledge}\n",
    "\n",
    "Focus on environment **principles, physics, and dynamics**, not superficial patterns.\n",
    "Please provide concise heuristic suggestions for policy generation based on this knowledge, such as:\n",
    "- State ranges to prioritize\n",
    "- Common failing action patterns\n",
    "- Recommended threshold adjustments\n",
    "\n",
    "Return a short, structured bullet list (no prose).\n",
    "\"\"\"\n",
    "        guidance = call_llm(prompt)\n",
    "        return guidance\n",
    "\n",
    "# ----------------------------\n",
    "# Memory module\n",
    "# ----------------------------\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.episodes = []\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.episodes.append({\"steps\": [], \"summary\": None})\n",
    "\n",
    "    def add_step(self, s, a, r, done):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"Please call start_episode() before adding steps!\")\n",
    "        self.episodes[-1][\"steps\"].append({\"s\": s, \"a\": a, \"r\": r, \"done\": done})\n",
    "\n",
    "    def add_episode_summary(self, env_id, policy_version):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"No running episode!\")\n",
    "        steps = self.episodes[-1][\"steps\"]\n",
    "        total_reward = sum(step[\"r\"] for step in steps)\n",
    "        length = len(steps)\n",
    "        self.episodes[-1][\"summary\"] = {\n",
    "            \"env_id\": env_id,\n",
    "            \"policy_version\": policy_version,\n",
    "            \"return\": total_reward,\n",
    "            \"length\": length\n",
    "        }\n",
    "\n",
    "    def get_recent_episodes(self, n=5):\n",
    "        summaries = [ep[\"summary\"] for ep in self.episodes if ep[\"summary\"] is not None]\n",
    "        return summaries[-n:]\n",
    "\n",
    "# ----------------------------\n",
    "# Reflection module\n",
    "# ----------------------------\n",
    "class Reflection:\n",
    "    def __init__(self, knowledge: Knowledge):\n",
    "        self.knowledge = knowledge\n",
    "\n",
    "    def metrics(self, recent_episodes):\n",
    "        returns = [ep[\"return\"] for ep in recent_episodes]\n",
    "        lengths = [ep[\"length\"] for ep in recent_episodes]\n",
    "        avg_return = np.mean(returns) if returns else 0\n",
    "        avg_length = np.mean(lengths) if lengths else 0\n",
    "        threshold = self.knowledge.static_knowledge.get(\"reward_threshold\", 0)\n",
    "        success_count = sum(1 for ep in recent_episodes if ep[\"return\"] >= threshold)\n",
    "        success_rate = success_count / len(recent_episodes) if recent_episodes else 0\n",
    "        return {\"avg_return\": avg_return, \"avg_length\": avg_length, \"success_rate\": success_rate}\n",
    "\n",
    "    def failure_pattern(self, recent_episodes, env_id):\n",
    "        prompt = f\"\"\"\n",
    "I have the following {env_id} environment episode summaries: {recent_episodes}\n",
    "Please analyze the most common failure patterns, including state characteristics, action issues, and return patterns. Focus only on key points.\n",
    "Return a concise paragraph.\n",
    "\"\"\"\n",
    "        pattern = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"failure_pattern\": pattern})\n",
    "        return pattern\n",
    "\n",
    "    def edit_suggestion(self, recent_episodes, env_id):\n",
    "        prompt = f\"\"\"\n",
    "Based on recent episode data from environment {env_id}: {recent_episodes}\n",
    "Generate one policy editing suggestion in one of the following formats:\n",
    "- add_rule(condition -> action)\n",
    "- modify_threshold(variable, old_value, new_value)\n",
    "- reprioritize(rule_i over rule_j)\n",
    "\n",
    "Return exactly one line with one edit.\n",
    "\"\"\"\n",
    "        suggestion = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"edit_suggestion\": suggestion})\n",
    "        return suggestion\n",
    "\n",
    "# ----------------------------\n",
    "# Safe policy call\n",
    "# ----------------------------\n",
    "def safe_policy_call(state, policy_fn, sk):\n",
    "    try:\n",
    "        a = policy_fn(state)\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            a = sk[\"action_space\"][0]\n",
    "        else:\n",
    "            a = (sk[\"action_space\"][0] + sk[\"action_space\"][1]) / 2.0\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        a = np.clip(a, sk[\"action_space\"][0], sk[\"action_space\"][1])\n",
    "    return a\n",
    "\n",
    "# ----------------------------\n",
    "# Safe step wrapper\n",
    "# ----------------------------\n",
    "def safe_step(env, action):\n",
    "    sk = STATIC_KNOWLEDGE[env.unwrapped.spec.id]\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        action = np.array([np.clip(action, sk[\"action_space\"][0], sk[\"action_space\"][1])]) if np.isscalar(action) else np.clip(np.array(action), sk[\"action_space\"][0], sk[\"action_space\"][1])\n",
    "    return env.step(action)\n",
    "\n",
    "# ----------------------------\n",
    "# Policy generation helper with rule-based constraint\n",
    "# ----------------------------\n",
    "def _action_constraints_text(static_knowledge: dict) -> str:\n",
    "    a = static_knowledge[\"action_space\"]\n",
    "    if static_knowledge.get(\"action_type\") == \"discrete\":\n",
    "        return f\"Discrete actions; valid actions are exactly the integers in {a}.\"\n",
    "    else:\n",
    "        lo, hi = a[0], a[1]\n",
    "        return f\"Continuous action; return a single float within [{lo}, {hi}]. Clip if necessary.\"\n",
    "\n",
    "def generate_base_policy(env_id, knowledge: Knowledge):\n",
    "    guidance = knowledge.get_dynamic_guidance(env_id) or \"\"\n",
    "    doc_url = get_env_doc_url(env_id)\n",
    "    sk = knowledge.static_knowledge\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "    state_vars_text = \"\\n\".join([f\"- {name} in range {rng}\" for name, rng in zip(sk[\"state_vars\"], sk[\"state_ranges\"])])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are writing a deterministic, white-box **rule-based policy** for Gymnasium environment \"{env_id}\".\n",
    "Focus on **environment principles, physics, and dynamics**, not superficial patterns.\n",
    "The policy must be based on simple if-else statements or threshold comparisons using state variables.\n",
    "Environment documentation: {doc_url}\n",
    "\n",
    "Observation (state vector):\n",
    "{state_vars_text}\n",
    "\n",
    "Action constraints:\n",
    "- {action_desc}\n",
    "- May import 'math' if needed\n",
    "- Must be deterministic\n",
    "- Do not use loops, functions, or external libraries except math\n",
    "- Example: if state[2] > 0: return 1 else: return 0\n",
    "\n",
    "Dynamic guidance:\n",
    "{guidance}\n",
    "\n",
    "Output requirements:\n",
    "- Only one Python function: def policy(state): ...\n",
    "- No explanations, no markdown, no print\n",
    "- Returned action strictly satisfies constraints\n",
    "\"\"\"\n",
    "    policy_code = call_llm(prompt)\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    try:\n",
    "        exec(policy_code, local_vars)\n",
    "        policy_fn = local_vars.get(\"policy\")\n",
    "        if policy_fn is None:\n",
    "            if sk[\"action_type\"] == \"discrete\":\n",
    "                def policy_fn(state): return sk[\"action_space\"][0]\n",
    "            else:\n",
    "                lo, hi = sk[\"action_space\"]\n",
    "                def policy_fn(state): return (lo + hi)/2.0\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            def policy_fn(state): return sk[\"action_space\"][0]\n",
    "        else:\n",
    "            lo, hi = sk[\"action_space\"]\n",
    "            def policy_fn(state): return (lo + hi)/2.0\n",
    "    return policy_fn\n",
    "\n",
    "def apply_edit(policy_fn, edit_text, knowledge: Knowledge):\n",
    "    sk = knowledge.static_knowledge\n",
    "    try:\n",
    "        existing_src = inspect.getsource(policy_fn)\n",
    "    except Exception:\n",
    "        existing_src = \"def policy(state):\\n    return \" + (str(sk[\"action_space\"][0]) if sk[\"action_type\"]==\"discrete\" else str((sk[\"action_space\"][0]+sk[\"action_space\"][1])/2.0))\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "\n",
    "    doc_url = get_env_doc_url(knowledge.dynamic_knowledge[0].get(\"env_id\", \"\"))\n",
    "    prompt = f\"\"\"\n",
    "Revise deterministic, **rule-based** policy for environment at {doc_url}.\n",
    "Focus on physics, dynamics, and environment principles.\n",
    "Constraints: {action_desc}\n",
    "Current policy:\n",
    "{existing_src}\n",
    "Edit suggestion: {edit_text}\n",
    "You may use 'math' module.\n",
    "- Must remain if-else or threshold based\n",
    "\n",
    "Output only a valid Python function def policy(state): ...\n",
    "\"\"\"\n",
    "    policy_code = call_llm(prompt)\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    try:\n",
    "        exec(policy_code, local_vars)\n",
    "        new_policy_fn = local_vars.get(\"policy\")\n",
    "        return new_policy_fn if new_policy_fn else policy_fn\n",
    "    except Exception:\n",
    "        return policy_fn\n",
    "\n",
    "# ----------------------------\n",
    "# Main closed-loop training\n",
    "# ----------------------------\n",
    "def run_env_loop(env_id, max_iters=10, episodes_per_iter=10, ma_window=3,\n",
    "                 success_rate_threshold=0.8, rollback_window=3,\n",
    "                 truncated_threshold=10):\n",
    "    knowledge = Knowledge()\n",
    "    knowledge.load_static_knowledge(env_id)\n",
    "    memory = Memory()\n",
    "    reflection = Reflection(knowledge)\n",
    "    policy_version = 0\n",
    "    first_iter = True\n",
    "    policy_fn = None\n",
    "    best_policy_fn = None\n",
    "    best_metrics = {\"avg_return\": -np.inf, \"success_rate\": 0}\n",
    "    recent_metrics_history = []\n",
    "    consecutive_truncated = 0\n",
    "\n",
    "    for iter_idx in range(max_iters):\n",
    "        policy_version += 1\n",
    "        print(f\"=== Iteration {iter_idx+1} ===\")\n",
    "\n",
    "        if first_iter:\n",
    "            policy_fn = generate_base_policy(env_id, knowledge)\n",
    "            first_iter = False\n",
    "        else:\n",
    "            recent_episodes = memory.get_recent_episodes()\n",
    "            suggestion = reflection.edit_suggestion(recent_episodes, env_id)\n",
    "            policy_fn = apply_edit(policy_fn, suggestion, knowledge)\n",
    "\n",
    "        env = gym.make(env_id)\n",
    "        iteration_returns = []\n",
    "        iteration_truncated = False\n",
    "\n",
    "        for ep in range(episodes_per_iter):\n",
    "            s, _ = env.reset()\n",
    "            done = False\n",
    "            memory.start_episode()\n",
    "\n",
    "            while not done:\n",
    "                a = safe_policy_call(s, policy_fn, knowledge.static_knowledge)\n",
    "                s_next, r, terminated, truncated, info = safe_step(env, a)\n",
    "                if truncated:\n",
    "                    iteration_truncated = True\n",
    "                done = terminated or truncated\n",
    "                memory.add_step(s, a, r, done)\n",
    "                s = s_next\n",
    "\n",
    "            memory.add_episode_summary(env_id, policy_version)\n",
    "            iteration_returns.append(memory.episodes[-1][\"summary\"][\"return\"])\n",
    "\n",
    "        recent_episodes_ma = memory.get_recent_episodes(n=ma_window)\n",
    "        metrics = reflection.metrics(recent_episodes_ma)\n",
    "        print(f\"Moving Avg Return: {metrics['avg_return']:.2f}, Success Rate: {metrics['success_rate']:.2f}\")\n",
    "\n",
    "        # Update failure pattern\n",
    "        pattern = reflection.failure_pattern(recent_episodes_ma, env_id)\n",
    "        print(\"Failure Pattern:\", pattern)\n",
    "\n",
    "        # Update best policy if improved\n",
    "        if metrics[\"avg_return\"] > best_metrics[\"avg_return\"] and metrics[\"success_rate\"] >= success_rate_threshold:\n",
    "            best_metrics = metrics.copy()\n",
    "            best_policy_fn = policy_fn\n",
    "\n",
    "        recent_metrics_history.append(metrics[\"avg_return\"])\n",
    "        if len(recent_metrics_history) > rollback_window:\n",
    "            recent_metrics_history.pop(0)\n",
    "\n",
    "        # Check rollback: if consecutive rollback_window drops, restore best policy\n",
    "        if len(recent_metrics_history) == rollback_window:\n",
    "            if all(recent_metrics_history[i] < recent_metrics_history[i-1] for i in range(1, rollback_window)):\n",
    "                print(f\"Rollback triggered. Restoring previous best policy with Avg Return={best_metrics['avg_return']:.2f}\")\n",
    "                policy_fn = best_policy_fn\n",
    "\n",
    "        # Check convergence based on moving average over ma_window\n",
    "        if len(memory.episodes) >= ma_window * episodes_per_iter:\n",
    "            if metrics[\"avg_return\"] >= knowledge.static_knowledge.get(\"reward_threshold\", 0) and metrics[\"success_rate\"] >= success_rate_threshold:\n",
    "                print(f\"Converged! Stop training. Moving Avg Return={metrics['avg_return']:.2f}, Success Rate={metrics['success_rate']:.2f}\")\n",
    "                break\n",
    "\n",
    "        # Truncated handling: only trigger if consecutive_truncated >= threshold\n",
    "        if iteration_truncated:\n",
    "            consecutive_truncated += 1\n",
    "            if consecutive_truncated >= truncated_threshold:\n",
    "                print(f\"Consecutive truncated >= {truncated_threshold}. Regenerating new policy via LLM.\")\n",
    "                policy_fn = generate_base_policy(env_id, knowledge)\n",
    "                consecutive_truncated = 0\n",
    "        else:\n",
    "            consecutive_truncated = 0\n",
    "\n",
    "# ----------------------------\n",
    "# Run multiple control tasks\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    env_list = [\n",
    "        \"Acrobot-v1\",\n",
    "        \"CartPole-v1\",\n",
    "        \"MountainCarContinuous-v0\",\n",
    "        \"MountainCar-v0\",\n",
    "        \"Pendulum-v1\"\n",
    "    ]\n",
    "    for env_id in env_list:\n",
    "        print(f\"==== Running {env_id} ====\")\n",
    "        run_env_loop(env_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4685dea-0585-4079-873b-a3026520fd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_rl",
   "language": "python",
   "name": "dl_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
