{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f413d2-5321-4dce-bdeb-9e2c98b7bb39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Running Acrobot-v1 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: -94.33, Success Rate: 1.00\n",
      "Failure Pattern: Across the Acrobot-v1 episodes, the policy consistently achieves returns between -92 and -97, with episode lengths closely matching these values, indicating that the agent typically solves the task just before the 100-step cutoff. The policy uses a swing-up phase based on tip height and angular velocity, then switches to stabilization near the goal. The most common failure pattern is likely insufficient stabilization near the upright position: the agent reaches the vicinity of the goal (tip_y ≈ 1.0) but struggles to finely control the tip due to high angular velocities or imprecise angle corrections. This is reflected in the policy’s reliance on simple thresholds for velocity damping and upright angle correction, which may not be robust enough for all state variations. As a result, the agent often overshoots or oscillates near the goal, leading to slightly suboptimal returns and episode lengths just shy of perfect performance.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: -124.00, Success Rate: 0.33\n",
      "Failure Pattern: Across the Acrobot-v1 episodes, the policy consistently applies an energy-based swing-up followed by a stabilization phase near the goal. Returns range from -148 to -95, with episode lengths closely matching the returns, indicating that failures are due to not reaching the goal quickly enough rather than catastrophic instability. The most common failure pattern is insufficiently rapid or precise stabilization once the tip nears the upright position (tip_y ≈ 1.0): the policy dampens high velocities but may not adequately counteract residual oscillations or fine-tune the upright angle, leading to repeated overshooting or dithering around the goal. Action selection is deterministic and based on summed angular velocities and upright angle, which may not adapt well to subtle state variations, causing episodes to terminate before the goal is reliably achieved. Overall, failures stem from limited fine control near the upright and possibly delayed or oscillatory action switching in the final stabilization phase.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: -124.33, Success Rate: 0.00\n",
      "Failure Pattern: Across the Acrobot-v1 episodes using policy version 3, the most common failure pattern is an inability to consistently achieve and maintain the upright (goal) position, as reflected by moderately negative returns (ranging from -115 to -130) and episode lengths near the minimum required for success. The policy employs a reasonable swing-up strategy based on tip height and angular velocity, but likely struggles with precise stabilization near the goal: the fine control phase (when tip_y ≥ 0.9) relies on simple thresholding of the upright angle and velocity damping, which may be insufficient for the inherently unstable dynamics of the Acrobot. As a result, the agent often overshoots or oscillates near the upright, failing to stabilize before the episode ends. Action selection is deterministic and may not adapt well to nuanced state variations, leading to repeated failures in the final stabilization phase rather than during the swing-up.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-94.33\n",
      "=== Iteration 4 ===\n",
      "Moving Avg Return: -104.00, Success Rate: 0.67\n",
      "Failure Pattern: Across the Acrobot-v1 episodes using policy version 4, the most common failure patterns involve inconsistent stabilization after the swing-up phase. The policy reliably swings the tip upward by applying energy in the direction of total angular velocity when the tip is low (tip_y < 0.9), but struggles to consistently stabilize near the goal (tip_y ≥ 0.9), especially when angular velocities remain high. This leads to overshooting or oscillations around the upright position, as indicated by the policy’s repeated switching between damping and fine stabilization actions. The episode returns (−89.0, −137.0, −86.0) and lengths (90, 138, 87) show significant variance, suggesting that while the swing-up is often successful, the transition to and maintenance of the upright state is unreliable, causing premature episode termination. The key issues are high post-swing velocities and insufficiently robust fine control near the goal, resulting in inconsistent episode durations and suboptimal returns.\n",
      "=== Iteration 5 ===\n",
      "Moving Avg Return: -103.67, Success Rate: 0.67\n",
      "Failure Pattern: Across the Acrobot-v1 episodes, the policy reliably swings up and stabilizes the system, as shown by moderate episode returns (ranging from -125 to -92) and lengths (93–126 steps), indicating it often succeeds but not optimally. The most common failure pattern is insufficient stabilization near the upright: after reaching the tip height threshold (tip_y ≈ 0.9), the policy sometimes fails to dampen high angular velocities or overcorrects when the upright angle is slightly off, leading to oscillations or tip-overs. Action selection is based on summed angular velocities and upright angle, but the thresholds (e.g., |theta1_dot| + |theta2_dot| > 2.0, upright_angle > 0.05) may be too coarse, causing delayed or excessive torque reversals. Returns are consistently negative, reflecting the environment’s reward structure and indicating that while the policy can achieve swing-up, it struggles with precise, sustained balance at the goal.\n",
      "=== Iteration 6 ===\n",
      "Moving Avg Return: -105.33, Success Rate: 0.33\n",
      "Failure Pattern: Across the Acrobot-v1 episodes using the provided policy, the most common failure pattern is incomplete or delayed swing-up and stabilization near the goal. The policy uses tip height (y) to switch between swing-up and stabilization, but often fails to reach or maintain the upright position efficiently, as indicated by episode returns ranging from -94.0 to -121.0 and lengths from 95 to 122 steps (far from optimal). State-wise, failures occur when the tip height remains below the goal threshold (y < 0.9) for too long, or when high joint velocities (|theta1_dot| + |theta2_dot| > 2.0) persist, leading to oscillatory or indecisive actions. Action-wise, the policy alternates torque directions based on summed velocities and upright angle, but this can result in overcorrection or insufficient damping, preventing stable tip elevation. Overall, the episodes reveal that the policy struggles most with transitioning smoothly from swing-up to stabilization, leading to suboptimal returns and longer episode durations.\n",
      "=== Iteration 7 ===\n",
      "Moving Avg Return: -143.33, Success Rate: 0.33\n",
      "Failure Pattern: Across the Acrobot-v1 episodes, the most common failure pattern is prolonged swing-up phases where the tip height (y) remains below the goal threshold (y < 1.0), resulting in extended episode lengths and low returns (e.g., -240.0 over 241 steps). State characteristics during failures include insufficient tip elevation and high angular velocities, which trigger repeated energy-based actions (torque +1 or -1) but fail to transition efficiently to stabilization. Action issues arise when the policy oscillates between swing-up and damping without achieving fine stabilization, especially if the upright angle (theta1 + theta2) is not quickly corrected. Returns are consistently negative, with longer episodes correlating to poorer performance, indicating that failure is primarily due to delayed or ineffective stabilization near the goal state.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-94.33\n",
      "=== Iteration 8 ===\n",
      "Moving Avg Return: -133.33, Success Rate: 0.33\n",
      "Failure Pattern: Across the Acrobot-v1 episodes, the most common failure pattern stems from inconsistent swing-up and stabilization phases. The policy uses tip height and angular velocities to switch between energy injection and damping, but episodes with poor returns (e.g., -193.0) likely result from the tip failing to reach or maintain the goal height (y ≥ 1.0) efficiently, often lingering in the swing-up phase. High angular velocities trigger damping actions, but if the timing or magnitude is off, the system oscillates or stalls below the goal. Additionally, the fine stabilization logic based on upright angle may be too simplistic, causing indecisive or delayed torque application near the target. Returns and episode lengths vary widely, indicating that the policy sometimes succeeds quickly but often gets stuck in suboptimal states due to insufficient energy or poor stabilization, leading to longer episodes and lower returns.\n",
      "=== Iteration 9 ===\n",
      "Moving Avg Return: -123.67, Success Rate: 0.33\n",
      "Failure Pattern: Across the Acrobot-v1 episodes using policy version 9, the most common failure pattern is an inability to consistently achieve and maintain the upright goal state, as indicated by moderate negative returns (-90.0, -142.0, -139.0) and episode lengths clustering around 91–143 steps. The policy attempts energy-based swing-up when the tip height is below the threshold (y < 0.9), but often fails to generate sufficient momentum or stabilize near the goal, likely due to oscillatory or delayed action switching—especially when angular velocities are high or the upright angle is near the threshold. The fine stabilization logic (based on upright_angle) may be too coarse, causing indecisive or oscillatory actions (e.g., switching between torque directions or applying no torque). Overall, failures stem from insufficient swing-up energy, poor damping of high velocities near upright, and imprecise stabilization, resulting in repeated misses of the goal and extended episode lengths without success.\n",
      "=== Iteration 10 ===\n",
      "Moving Avg Return: -124.00, Success Rate: 0.00\n",
      "Failure Pattern: Across the Acrobot-v1 episodes, the policy consistently achieves episode lengths around 120–130 steps, with returns between -119 and -129, indicating it often swings up but struggles to stabilize the tip near the goal. The policy uses a phase-based approach: swinging up when the tip is low (tip_y < 0.9) by pushing in the direction of total angular velocity, then attempting to dampen high velocities and finely stabilize near upright. The most common failure pattern is insufficient stabilization after swing-up—when the tip nears the goal, high angular velocities persist, and the policy’s simple velocity-dampening and upright-angle corrections are not robust enough to maintain balance, leading to repeated overshooting or oscillation. This results in the episode terminating before long-term stabilization is achieved, reflected in the moderate negative returns and episode lengths just above the minimum.\n",
      "==== Running CartPole-v1 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: 9.33, Success Rate: 0.00\n",
      "Failure Pattern: Across the three CartPole-v1 episodes using the provided policy, the agent consistently fails early, with episode lengths of 9, 10, and 9 steps, and corresponding low returns. The policy primarily reacts to the pole angle (theta) and its velocity (theta_dot), but the short episode lengths suggest it struggles to recover from moderate deviations, likely failing when the pole angle exceeds the 0.05 threshold or when position (x) drifts beyond ±1.0. The action logic may be too sensitive or insufficiently robust to stabilize the pole, especially in borderline cases where small errors in theta or x_dot accumulate. Overall, the most common failure pattern is an inability to correct for small but compounding deviations in pole angle or cart position, leading to rapid episode termination.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: 9.33, Success Rate: 0.00\n",
      "Failure Pattern: The provided CartPole-v1 episode summaries, all using the same rule-based policy (policy_version 2), consistently result in short episodes with returns and lengths of 9 or 10, indicating early failures. The policy prioritizes correcting pole angle (theta) when it deviates beyond 0.05 radians, then cart position (x) if it exceeds 1.0, and otherwise reacts to angular velocity (theta_dot) and cart velocity (x_dot). The most common failure pattern appears to be insufficient correction for small but accumulating pole angles or velocities, leading to rapid loss of balance. The policy’s thresholds may be too coarse, failing to react early enough to subtle instabilities, and its reliance on simple linear combinations (e.g., theta + 0.5 * theta_dot) may not capture the complexity needed for longer balancing. Overall, the episodes terminate quickly due to delayed or inadequate responses to growing instability, as reflected in the low, consistent returns.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: 10.00, Success Rate: 0.00\n",
      "Failure Pattern: The CartPole-v1 episodes using policy version 3 consistently result in short episode lengths (8, 10, and 12 steps), indicating frequent early failures. The policy prioritizes correcting pole angle (theta) when it exceeds 0.05 radians, but its threshold may be too sensitive, causing abrupt or oscillatory actions. When the cart position (x) exceeds 1.0, the policy switches focus, but this rarely occurs before failure. The reliance on theta_dot and x_dot for fine control in near-upright states may not provide sufficient corrective force, especially if the pole is already falling. Overall, the most common failure pattern is an inability to recover from small pole deviations, leading to rapid termination and low returns (8–12), suggesting the policy is too reactive and lacks robustness to small disturbances.\n",
      "=== Iteration 4 ===\n",
      "Moving Avg Return: 9.33, Success Rate: 0.00\n",
      "Failure Pattern: Across the CartPole-v1 episodes using policy version 4, the agent consistently fails early, with episode lengths of 9 or 10 and returns matching the lengths, indicating termination soon after starting. The policy prioritizes correcting large pole angles (|theta| > 0.05) and cart positions (|x| > 1.0), but the frequent short episodes suggest it struggles to stabilize the pole, likely due to insufficiently responsive actions to small deviations in angle or velocity. The decision logic may be too coarse, causing delayed or incorrect actions when the pole is near upright or the cart is near the center, leading to rapid loss of balance. Overall, the most common failure pattern is an inability to recover from initial perturbations, resulting in quick episode termination.\n",
      "=== Iteration 5 ===\n",
      "Moving Avg Return: 10.67, Success Rate: 0.00\n",
      "Failure Pattern: The provided CartPole-v1 episode summaries using policy version 5 consistently result in short episode lengths (9, 10, and 13 steps) and low returns, indicating frequent early failures. The policy primarily reacts to the pole angle (theta) and its derivative, with secondary checks on cart position (x) and velocity (x_dot). The most common failure pattern likely arises from the policy's threshold-based logic: it may be too sensitive to small deviations in theta, causing abrupt or inappropriate actions that fail to stabilize the pole, especially when theta is near the ±0.05 threshold. Additionally, the policy's handling of cart position only activates when |x| > 1.0, potentially allowing the cart to drift too far before corrective action is taken. Overall, the policy's rigid thresholds and lack of nuanced response to combined state variables lead to premature episode termination and low returns.\n",
      "=== Iteration 6 ===\n",
      "Moving Avg Return: 10.00, Success Rate: 0.00\n",
      "Failure Pattern: The provided CartPole-v1 episode summaries, all using the same policy (policy_version 6), show consistently low returns (8.0, 13.0, and 9.0), indicating frequent early failures. The policy prioritizes correcting pole angle (theta) when it exceeds 0.05 radians, using a linear combination of theta and theta_dot, but otherwise falls back on cart position (x) and velocities. This suggests failures often occur when the pole angle quickly exceeds the threshold or when the cart drifts beyond ±1.0, triggering less effective corrective actions. The policy’s reliance on simple thresholds and linear combinations may not adequately handle rapid or combined deviations in state variables, leading to abrupt loss of balance. Overall, the main failure patterns are insufficient recovery from moderate pole angles and inadequate handling of simultaneous cart and pole deviations, resulting in short episode lengths and low returns.\n",
      "=== Iteration 7 ===\n",
      "Moving Avg Return: 10.00, Success Rate: 0.00\n",
      "Failure Pattern: The provided CartPole-v1 episode summaries, all using the same policy (version 7), consistently result in short episodes with returns of 9.0, 10.0, and 11.0, indicating early failures. The policy primarily reacts to the pole angle (theta) and its velocity (theta_dot), with secondary checks on cart position (x) and velocity (x_dot). The most common failure pattern appears to be insufficient correction when the pole angle deviates slightly (abs(theta) > 0.05), as the policy may not respond aggressively enough to prevent the pole from falling. Additionally, the policy’s thresholds for action are relatively coarse, potentially causing delayed or inadequate responses to rapidly changing states. The low returns and short episode lengths suggest that the policy struggles to stabilize the pole beyond the initial few steps, likely due to these simplistic and threshold-based action decisions.\n",
      "=== Iteration 8 ===\n",
      "Moving Avg Return: 10.00, Success Rate: 0.00\n",
      "Failure Pattern: The provided CartPole-v1 episode summaries, all using the same policy (policy_version 8), show consistently low returns (9.0, 10.0, 11.0) and short episode lengths, indicating early failures. The policy prioritizes correcting pole angle (theta) when it exceeds 0.05 radians, using a linear combination of theta and theta_dot, but may be too reactive or threshold-sensitive, potentially missing subtler corrections when the pole is near upright. When the cart position (x) exceeds 1.0, the policy switches to correcting x, but this may occur too late to recover. In states where both theta and x are within thresholds, the policy relies on theta_dot and x_dot, which may not provide enough anticipatory control. Overall, failures likely stem from delayed or insufficient corrective actions, especially when the pole is near upright but drifting, leading to rapid loss of balance and short episodes.\n",
      "=== Iteration 9 ===\n",
      "Moving Avg Return: 9.67, Success Rate: 0.00\n",
      "Failure Pattern: The provided CartPole-v1 episodes, all using the same policy (version 9), consistently result in short episode lengths (9–10 steps) and low returns (9.0–10.0), indicating early failures. The policy prioritizes correcting pole angle (theta) when it exceeds 0.05 radians, using a combination of theta and theta_dot, and only considers cart position (x) if it exceeds 1.0. In less extreme states, it relies on theta_dot and x_dot. The most common failure pattern likely stems from the policy's high sensitivity to small angle deviations and lack of nuanced control when the pole is near upright but moving, causing overcorrection or delayed responses. Additionally, the policy only reacts to relatively large cart displacements, potentially allowing the cart to drift too far before acting. Overall, the short episode lengths and low returns suggest the policy fails to stabilize the pole, likely due to insufficiently responsive or overly simplistic action selection in borderline states.\n",
      "=== Iteration 10 ===\n",
      "Moving Avg Return: 9.33, Success Rate: 0.00\n",
      "Failure Pattern: The provided CartPole-v1 episode summaries, all using the same hand-crafted policy, show consistently low returns (9–10 steps per episode), indicating early failure. The policy prioritizes correcting pole angle (theta) and its velocity (theta_dot), with secondary checks on cart position (x) and velocity (x_dot). Despite these heuristics, the agent fails quickly, suggesting the policy may overreact or underreact to small deviations, especially when theta is near zero but theta_dot or x_dot are non-negligible. The repeated short episode lengths imply the policy struggles with initial state variations or lacks sufficient nuance to stabilize the pole, often failing to recover from small disturbances. Overall, the main failure pattern is insufficient corrective action for subtle but compounding deviations in pole angle and cart position, leading to rapid episode termination.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-inf\n",
      "==== Running MountainCarContinuous-v0 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: 92.25, Success Rate: 1.00\n",
      "Failure Pattern: Across the three MountainCarContinuous-v0 episodes using the provided policy, returns are consistently high (91.6–93.5) and episode lengths are moderate (79–108 steps), indicating successful completion but with some variability in efficiency. The policy’s structure suggests potential failure patterns: near the goal (position > 0.45), overshooting may occur if velocity is not sufficiently dampened, risking oscillation or inefficient stopping. On the left hill (position < -0.4, velocity < 0), strong left pushes could lead to excessive backward movement if not balanced. Similarly, on the right hill (position > 0.4, velocity > 0), strong right pushes may cause premature or excessive acceleration. The reliance on velocity thresholds (±0.001) for momentum building could result in indecisive or oscillatory actions near zero velocity, especially if the car stalls. Overall, while the policy is robust, inefficiencies likely stem from aggressive force application near critical positions and potential overcorrection when transitioning between pushing and damping actions.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: 92.55, Success Rate: 1.00\n",
      "Failure Pattern: Across the MountainCarContinuous-v0 episodes using policy version 2, the agent consistently achieves high returns (91.87–92.92) and relatively short episode lengths (86–104), indicating successful completion. However, the policy’s structure reveals potential failure patterns: it relies heavily on threshold-based action switching for position and velocity, which may cause abrupt force changes near critical boundaries (e.g., near the goal at position > 0.45 or when transitioning between hills). Such abrupt switches risk overshooting or inefficient energy buildup, especially if the velocity is close to zero or the position is near action thresholds. Additionally, the use of hard-coded momentum pushes (±0.9, ±1.0) can lead to oscillatory behavior or suboptimal energy transfer if the state is not precisely aligned with the intended direction. Overall, while the policy is robust, its main vulnerabilities are at state boundaries where small changes can trigger large action shifts, potentially causing inefficient transitions or brief stalls before reaching the goal.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: 92.73, Success Rate: 1.00\n",
      "Failure Pattern: Across the three MountainCarContinuous-v0 episodes using policy version 3, returns are consistently high (91.7–93.4) and episode lengths are relatively short (80–109 steps), indicating successful goal achievement. The policy’s logic aggressively builds momentum on the hills and carefully reduces force near the goal to avoid overshooting, which minimizes common failure modes like stalling on slopes or overshooting the goal. However, the slight variation in episode length suggests occasional inefficiency, likely when the car’s velocity is near zero and the policy must rely on slope-based nudges (using math.cos(3 * position)), which may not always provide optimal acceleration. Overall, the main potential failure pattern is brief indecision or suboptimal force application near velocity zero or at the transition between hills, but the policy generally avoids major pitfalls such as getting stuck or oscillating without progress.\n",
      "Converged! Stop training. Moving Avg Return=92.73, Success Rate=1.00\n",
      "==== Running MountainCar-v0 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: -112.00, Success Rate: 0.33\n",
      "Failure Pattern: The provided MountainCar-v0 episode summaries, all using the same hand-crafted policy, show returns of -122, -92, and -122, with episode lengths matching the negative returns (indicating the agent typically takes the maximum allowed steps before reaching the goal, except for one faster episode). The policy heavily relies on velocity thresholds and position checks to decide when to push left or right, aiming to build momentum. However, the frequent occurrence of the maximum episode length (-122 return) suggests the policy often fails to generate enough momentum to reach the goal efficiently, likely due to overly simplistic or rigid velocity cutoffs and insufficient adaptation to the car's current state. The most common failure pattern is stalling near the left edge or mid-slope, repeatedly switching actions without gaining enough speed to crest the right hill, resulting in long, inefficient episodes.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: -119.33, Success Rate: 0.33\n",
      "Failure Pattern: The provided MountainCar-v0 episode summaries, all using the same rule-based policy, show returns between -110 and -125, indicating the agent typically takes 110–125 steps to reach the goal (with -1 reward per step). The policy heavily favors pushing left when velocity is negative and right when velocity is positive, with special cases for extreme positions and velocities. The most common failure pattern is likely inefficient momentum building: the agent may overcommit to pushing left at the far left edge or when moving left fast, but then switches to pushing right only when velocity is sufficiently positive, possibly missing optimal timing for momentum reversal. This can cause oscillations near the left edge or slow progress up the right hill. The returns suggest occasional suboptimal action choices, such as switching direction too late or too early, leading to longer episodes. Overall, the policy’s deterministic thresholds may not adapt well to nuanced state variations, resulting in inconsistent episode lengths and suboptimal returns.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: -121.67, Success Rate: 0.00\n",
      "Failure Pattern: Across the MountainCar-v0 episodes, the policy consistently achieves returns between -117 and -127, indicating it reaches the goal but not optimally. The most common failure patterns stem from suboptimal momentum management: the policy often pushes left when velocity is negative, but may not sufficiently build up speed before switching to rightward actions, especially near the center. This can result in extra oscillations and longer episode lengths (117–127 steps). State-wise, failures tend to occur when the car is near the left edge with low velocity, or when it switches direction prematurely, causing inefficient climbs. Action-wise, the policy’s reliance on velocity thresholds sometimes leads to delayed or mistimed pushes, reducing overall efficiency. The return pattern reflects these inefficiencies, as the agent consistently takes more steps than necessary to reach the goal.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-inf\n",
      "=== Iteration 4 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 consistently show a return of -200.0 and a maximum episode length of 200 steps, indicating that the agent repeatedly fails to reach the goal within the allowed time. This pattern suggests the agent is likely stuck in suboptimal states, such as oscillating near the bottom of the valley without building enough momentum to escape. The uniformity in returns and lengths points to ineffective action selection, possibly defaulting to random or non-strategic moves that do not exploit the environment’s dynamics. Overall, the agent exhibits persistent failure to solve the task, characterized by poor state transitions and lack of progress toward the goal.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-inf\n",
      "=== Iteration 5 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 consistently show a return of -200.0 and maximum episode length (200 steps), indicating the agent repeatedly fails to reach the goal and terminates only due to the time limit. This pattern suggests the agent is likely stuck oscillating between states near the bottom of the valley, unable to generate enough momentum to reach the flag. The actions taken are probably not effectively exploiting the environment’s dynamics—either alternating ineffectively or remaining stationary—resulting in no progress. The uniform returns and episode lengths highlight a persistent failure to learn or apply a successful strategy.\n",
      "=== Iteration 6 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 consistently show a return of -200.0 and a maximum episode length of 200 steps, indicating the agent repeatedly fails to reach the goal within the allowed time. This pattern suggests the agent is likely stuck in suboptimal states, such as oscillating near the valley without building enough momentum to reach the flag. Action selection issues may include insufficient exploration or consistently choosing actions that do not effectively leverage the car’s physics (e.g., failing to alternate between left and right to gain speed). The uniform returns and lengths highlight a persistent inability to escape the initial state dynamics, pointing to a lack of learning or ineffective policy updates.\n",
      "=== Iteration 7 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 consistently show a return of -200.0 and a maximum episode length of 200 steps, indicating that the agent repeatedly fails to reach the goal within the allowed time. This pattern suggests the agent is likely stuck in suboptimal states, such as oscillating near the valley without building enough momentum to ascend the hill. The uniformity in returns and lengths points to ineffective action selection, possibly defaulting to non-strategic or random actions rather than leveraging the environment’s physics to escape the valley. Overall, the key failure pattern is persistent inability to solve the task, characterized by repeated full-length episodes with minimum possible return.\n",
      "=== Iteration 8 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 consistently show a return of -200.0 and a maximum episode length of 200 steps, indicating the agent repeatedly fails to reach the goal within the allowed time. This pattern suggests the agent is likely stuck in suboptimal states, such as oscillating at the bottom of the valley without gaining enough momentum to climb the hill. The uniformity in returns and episode lengths points to persistent action selection issues, possibly with the agent choosing actions that do not sufficiently alternate between left and right to build speed. Overall, the agent exhibits a lack of effective exploration and fails to escape the initial low-reward region, resulting in repeated episode failures.\n",
      "=== Iteration 9 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 consistently show a return of -200.0 and a maximum episode length of 200 steps, indicating the agent repeatedly fails to reach the goal within the allowed time. This suggests a common failure pattern where the agent is likely stuck oscillating between states near the bottom of the valley, unable to generate enough momentum to escape. The actions taken may lack sufficient exploration or effective force application, resulting in minimal progress. The uniformity in returns and episode lengths highlights a persistent inability to solve the environment, likely due to suboptimal policy or insufficient learning.\n",
      "=== Iteration 10 ===\n",
      "Moving Avg Return: -200.00, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 all show a return of -200.0 and a maximum episode length of 200 steps, indicating that the agent consistently fails to reach the goal within the allowed time. This pattern suggests the agent is either stuck oscillating in a low-energy state or taking ineffective actions that do not build enough momentum to climb the hill. The uniformity in returns and episode lengths points to a policy that is not learning or exploiting the environment dynamics, likely defaulting to suboptimal or random actions. Overall, the key failure pattern is persistent inability to escape the initial valley, resulting in repeated timeouts and minimal progress toward the goal.\n",
      "==== Running Pendulum-v1 ====\n",
      "=== Iteration 1 ===\n",
      "Moving Avg Return: -1297.34, Success Rate: 0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes using the provided swing-up and PD control policy, the most common failure pattern is insufficient stabilization near the upright position. While the policy aggressively swings up when far from upright (cos_theta < 0.95), it often fails to maintain low angular velocity and small angle once upright, as indicated by consistently poor returns (ranging from -1261.94 to -1345.20 over 200 steps). The PD controller may not be tuned optimally, leading to lingering oscillations or overcorrection, and the threshold for \"do nothing\" (abs(theta) < 0.15, abs(theta_dot) < 0.2) may be too narrow, causing unnecessary actuation. Overall, the episodes are characterized by frequent transitions between high-torque actions and brief stabilization attempts, resulting in high energy usage and suboptimal reward accumulation.\n",
      "=== Iteration 2 ===\n",
      "Moving Avg Return: -1320.61, Success Rate: 0.00\n",
      "Failure Pattern: The provided Pendulum-v1 episode summaries, all using the same version 2 policy, show consistently poor returns (ranging from approximately -1236 to -1370 over 200 steps), indicating the policy struggles to keep the pendulum upright and stable. The policy uses a swing-up strategy when the pendulum is far from upright (cos(theta) < 0.95), applying maximum torque in the direction of the angle, and switches to a PD controller near upright. However, the persistent low returns suggest frequent failures to reach or maintain the upright position, likely due to aggressive torque application causing overshooting or oscillation, and possibly insufficient damping or precision in the PD control phase. The policy’s thresholds for \"upright\" and \"small angle/velocity\" may be too loose, resulting in instability or delayed stabilization. Overall, the main failure patterns are excessive swinging, inability to settle near upright, and inadequate fine control, as reflected by the consistently high negative returns.\n",
      "=== Iteration 3 ===\n",
      "Moving Avg Return: -1547.18, Success Rate: 0.00\n",
      "Failure Pattern: Across the three Pendulum-v1 episodes using policy version 3, the most common failure pattern is suboptimal stabilization near the upright position, as indicated by consistently negative returns (ranging from -1881.5 to -1379.1). The policy applies maximum torque when the pendulum is far from upright (cos(theta) < 0.95), but this aggressive swing-up can lead to overshooting and oscillations. Near upright, the policy uses a PD controller, but the threshold for \"doing nothing\" (|theta| < 0.15 and |theta_dot| < 0.2) may be too strict, causing unnecessary corrections and preventing smooth stabilization. The action clipping at ±2.0 further limits fine control. Overall, the episodes are long (200 steps), suggesting the pendulum rarely settles efficiently, and the high negative returns reflect persistent energy loss and instability, especially during the transition from swing-up to stabilization.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-inf\n",
      "=== Iteration 4 ===\n",
      "Moving Avg Return: -1045.72, Success Rate: 0.00\n",
      "Failure Pattern: Across the three Pendulum-v1 episodes, the agent consistently achieves low returns (ranging from -815.54 to -1230.10), with each episode lasting the full 200 steps, indicating persistent difficulty in stabilizing the pendulum upright. The most common failure pattern is the agent’s inability to maintain balance, likely resulting in frequent large negative rewards due to high angular deviation and velocity. Action-wise, this suggests either insufficient or poorly timed torque applications, possibly oscillating or failing to counteract the pendulum’s fall effectively. Overall, the consistently poor returns and maximum episode lengths point to a policy that struggles with both state stabilization and precise action selection.\n",
      "=== Iteration 5 ===\n",
      "Moving Avg Return: -1022.18, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 5 show consistently poor returns (ranging from -1450.56 to -745.43), indicating the agent frequently fails to keep the pendulum upright and stable. The fixed episode length (200 steps) suggests the agent survives the full duration but accumulates negative rewards, likely due to large angular deviations and excessive torque usage. Common failure patterns likely include the pendulum swinging erratically or remaining far from the upright position, with actions that overcorrect or fail to stabilize the system. The significant variability in returns also hints at inconsistent policy performance, possibly due to unstable or poorly tuned action outputs.\n",
      "=== Iteration 6 ===\n",
      "Moving Avg Return: -1320.67, Success Rate: 0.00\n",
      "Failure Pattern: Across the provided Pendulum-v1 episodes, the agent consistently completes the maximum episode length (200 steps), indicating it does not terminate early due to catastrophic failures. However, the returns remain substantially negative (ranging from -1632 to -1051), suggesting persistent difficulty in keeping the pendulum upright and stable. This pattern typically points to common failure modes such as the pendulum frequently swinging far from the upright position, possibly oscillating or spinning uncontrollably. Likely action issues include insufficient or poorly timed torque application, leading to overcorrection or undercorrection. Overall, the agent struggles to learn effective stabilization, as reflected by the high negative returns and lack of improvement across episodes.\n",
      "=== Iteration 7 ===\n",
      "Moving Avg Return: -1244.85, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 7 consistently show poor performance, as indicated by low returns (ranging from -1082.5 to -1556.8) over the maximum episode length (200 steps). This suggests the agent frequently fails to keep the pendulum upright and stable, likely spending significant time in high-angle, high-velocity states far from the upright position. The consistently negative returns and lack of early termination imply persistent suboptimal actions, such as insufficient or poorly timed torque application, leading to repeated swings or oscillations rather than stabilization. Overall, the main failure patterns are an inability to recover from unstable states and ineffective action selection, resulting in sustained poor performance throughout each episode.\n",
      "=== Iteration 8 ===\n",
      "Moving Avg Return: -923.54, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 8 show consistently long episodes (length 200), indicating the agent survives the full duration but struggles to maximize reward. The returns are all negative and vary widely (from -1255 to -753), suggesting inconsistent performance and frequent failure to keep the pendulum upright. Common failure patterns likely include the agent's inability to stabilize the pendulum near the upright position, leading to high negative rewards. Action issues may involve excessive or poorly timed torque, causing oscillations or overcorrections. Overall, the agent demonstrates difficulty in precise control, resulting in suboptimal returns and revealing a need for improved policy stability and action selection.\n",
      "=== Iteration 9 ===\n",
      "Moving Avg Return: -1089.64, Success Rate: 0.00\n",
      "Failure Pattern: Across the provided Pendulum-v1 episodes, the agent consistently achieves low returns (ranging from -991.6 to -1162.9), indicating persistent failure to keep the pendulum upright and stable. The episode lengths are always 200 steps, suggesting the agent survives the full episode but fails to improve the pendulum’s energy or orientation. Common failure patterns likely include the pendulum swinging at large angles away from the upright position (high potential energy states) and the agent applying suboptimal or inconsistent torques (action issues), resulting in poor control and inability to recover balance. The consistently negative returns highlight a lack of effective learning or policy optimization in these runs.\n",
      "=== Iteration 10 ===\n",
      "Moving Avg Return: -1264.37, Success Rate: 0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 10 show consistently poor performance, as indicated by low returns (ranging from -755 to -1733, with all episodes reaching the maximum length of 200 steps). This suggests the policy struggles to keep the pendulum upright and stable, likely resulting in frequent large negative rewards due to high angular displacement and velocity. The wide variation in returns points to inconsistent control, possibly from erratic or suboptimal actions—such as overcorrection or insufficient torque—leading to repeated swings away from the upright position. Overall, the main failure patterns are unstable state trajectories, ineffective action selection, and consistently negative returns, highlighting a need for improved policy stability and precision.\n",
      "Rollback triggered. Restoring previous best policy with Avg Return=-inf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "changed part:\n",
    "1.Record the source code string in Memory so that it can be reused for fine-tuning.\n",
    "2.Introduce an ε-greedy hybrid.\n",
    "\n",
    "acrobot往往会truncate，所以如果 rollback 触发时没有检查 truncated 状态，可能直接退回一个更差策略，而不是生成新的策略。\n",
    "\n",
    "\n",
    "解决方法：\n",
    "延迟更新 best_policy：\n",
    "只有当策略平均回报 > -inf 或者超过一定阈值时才更新 best_policy_fn。\n",
    "避免回滚到空策略。\n",
    "\n",
    "truncated-triggered reset 优先：\n",
    "当连续 truncated 超过阈值时，直接生成新的策略，而不是回滚。\n",
    "Rollback 只针对“已找到有效策略，但连续下降”的情况。\n",
    "\"\"\"\n",
    "\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import inspect\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "# ----------------------------\n",
    "# LLM HTTP call function\n",
    "# ----------------------------\n",
    "LLM_URL = 'https://api.yesapikey.com/v1/chat/completions'\n",
    "LLM_HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer sk-DygYzdGba7V5ggRwDf0d28B193D84c90Af2eE34b68C1C892'\n",
    "}\n",
    "\n",
    "def call_llm(prompt, model=\"gpt-4.1-2025-04-14\", temperature=0.2, max_tokens=1024):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(LLM_URL, json=data, headers=LLM_HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                resp_json = response.json()\n",
    "                if 'choices' in resp_json and resp_json['choices']:\n",
    "                    content = resp_json['choices'][0].get('message', {}).get('content')\n",
    "                    return content\n",
    "        except Exception as e:\n",
    "            print(\"LLM call exception:\", e)\n",
    "        time.sleep(2)\n",
    "\n",
    "# ----------------------------\n",
    "# Environment documentation mapping\n",
    "# ----------------------------\n",
    "ENV_DOC_URL = {\n",
    "    \"Acrobot-v1\": \"https://gymnasium.farama.org/environments/classic_control/acrobot/\",\n",
    "    \"CartPole-v1\": \"https://gymnasium.farama.org/environments/classic_control/cart_pole/\",\n",
    "    \"MountainCarContinuous-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\",\n",
    "    \"MountainCar-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car/\",\n",
    "    \"Pendulum-v1\": \"https://gymnasium.farama.org/environments/classic_control/pendulum/\"\n",
    "}\n",
    "\n",
    "def get_env_doc_url(env_id: str) -> str:\n",
    "    return ENV_DOC_URL.get(env_id, \"https://gymnasium.farama.org/\")\n",
    "\n",
    "# ----------------------------\n",
    "# Static Knowledge (with physics formulas)\n",
    "# ----------------------------\n",
    "STATIC_KNOWLEDGE = {\n",
    "    \"CartPole-v1\": {\n",
    "        \"state_dim\": 4,\n",
    "        \"state_vars\": [\"cart_position\", \"cart_velocity\", \"pole_angle\", \"pole_velocity\"],\n",
    "        \"state_ranges\": [(-4.8, 4.8), (-float(\"inf\"), float(\"inf\")), (-0.418, 0.418), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1],\n",
    "        \"reward_threshold\": 475,\n",
    "        \"action_type\": \"discrete\",\n",
    "        \"physics\": {\n",
    "            \"theta_accel\": \"theta_accel = g*sin(theta) + cos(theta)*(-F - m*l*theta_dot^2*sin(theta))/(M+m)\",\n",
    "            \"cart_accel\": \"x_accel = (F + m*l*(theta_dot^2*sin(theta) - theta_accel*cos(theta))) / (M+m)\"\n",
    "        }\n",
    "    },\n",
    "    \"Acrobot-v1\": {\n",
    "        \"state_dim\": 6,\n",
    "        \"state_vars\": [\"cos_theta1\", \"sin_theta1\", \"cos_theta2\", \"sin_theta2\", \"theta1_dot\", \"theta2_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\")), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -100,\n",
    "        \"action_type\": \"discrete\",\n",
    "        \"physics\": {\n",
    "            \"theta1_accel\": \"theta1_ddot = -(d2*(theta2_dot^2*sin(theta2)+g*sin(theta1+theta2)+...) ) / denominator\",\n",
    "            \"theta2_accel\": \"theta2_ddot = formula depends on theta1, theta1_dot, theta2, theta2_dot, torque\"\n",
    "        }\n",
    "    },\n",
    "    \"MountainCar-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -110,\n",
    "        \"action_type\": \"discrete\",\n",
    "        \"physics\": {\n",
    "            \"velocity_update\": \"v_next = v + 0.001*action - 0.0025*cos(3*position)\",\n",
    "            \"position_update\": \"pos_next = pos + v_next\"\n",
    "        }\n",
    "    },\n",
    "    \"MountainCarContinuous-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [-1.0, 1.0],\n",
    "        \"reward_threshold\": 90,\n",
    "        \"action_type\": \"continuous\",\n",
    "        \"physics\": {\n",
    "            \"velocity_update\": \"v_next = v + 0.001*force - 0.0025*cos(3*position)\",\n",
    "            \"position_update\": \"pos_next = pos + v_next\"\n",
    "        }\n",
    "    },\n",
    "    \"Pendulum-v1\": {\n",
    "        \"state_dim\": 3,\n",
    "        \"state_vars\": [\"cos_theta\", \"sin_theta\", \"theta_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [-2.0, 2.0],\n",
    "        \"reward_threshold\": -200,\n",
    "        \"action_type\": \"continuous\",\n",
    "        \"physics\": {\n",
    "            \"theta_accel\": \"theta_ddot = (-3*g/(2*l)*sin(theta + pi) + 3./(m*l^2)*torque)\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Knowledge module with timestamp, validity, weight\n",
    "# ----------------------------\n",
    "class Knowledge:\n",
    "    def __init__(self):\n",
    "        self.static_knowledge = {}\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def load_static_knowledge(self, env_id):\n",
    "        if env_id not in STATIC_KNOWLEDGE:\n",
    "            raise ValueError(\"Unsupported environment\")\n",
    "        self.static_knowledge = STATIC_KNOWLEDGE[env_id]\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def add_dynamic_entry(self, entry, weight=1.0):\n",
    "        entry_copy = entry.copy()\n",
    "        entry_copy.update({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"valid\": True,\n",
    "            \"weight\": weight\n",
    "        })\n",
    "        self.dynamic_knowledge.append(entry_copy)\n",
    "\n",
    "    def filter_dynamic_knowledge(self, consecutive_inactive=3):\n",
    "        valid_entries = []\n",
    "        for entry in self.dynamic_knowledge:\n",
    "            if \"last_seen_iter\" not in entry:\n",
    "                entry[\"last_seen_iter\"] = 0\n",
    "            if entry[\"valid\"]:\n",
    "                valid_entries.append(entry)\n",
    "        self.dynamic_knowledge = valid_entries\n",
    "\n",
    "    def get_dynamic_guidance(self, env_id):\n",
    "        valid_entries = [e for e in self.dynamic_knowledge if e.get(\"valid\", True)]\n",
    "        prompt = f\"\"\"\n",
    "I am generating a policy in environment {env_id}.\n",
    "Current valid dynamic knowledge entries: {valid_entries}\n",
    "\n",
    "Focus on environment **principles, physics, and dynamics**, not superficial patterns.\n",
    "Please provide concise heuristic suggestions for policy generation based on this knowledge, such as:\n",
    "- State ranges to prioritize\n",
    "- Common failing action patterns\n",
    "- Recommended threshold adjustments\n",
    "\n",
    "Return a short, structured bullet list (no prose).\n",
    "\"\"\"\n",
    "        guidance = call_llm(prompt)\n",
    "        return guidance\n",
    "\n",
    "# ----------------------------\n",
    "# Memory module\n",
    "# ----------------------------\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.episodes = []\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.episodes.append({\"steps\": [], \"summary\": None})\n",
    "\n",
    "    def add_step(self, s, a, r, done):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"Please call start_episode() before adding steps!\")\n",
    "        self.episodes[-1][\"steps\"].append({\"s\": s, \"a\": a, \"r\": r, \"done\": done})\n",
    "\n",
    "    def add_episode_summary(self, env_id, policy_version, policy_source):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"No running episode!\")\n",
    "        steps = self.episodes[-1][\"steps\"]\n",
    "        total_reward = sum(step[\"r\"] for step in steps)\n",
    "        length = len(steps)\n",
    "        self.episodes[-1][\"summary\"] = {\n",
    "            \"env_id\": env_id,\n",
    "            \"policy_version\": policy_version,\n",
    "            \"policy_source\": policy_source,\n",
    "            \"return\": total_reward,\n",
    "            \"length\": length\n",
    "        }\n",
    "\n",
    "    def get_recent_episodes(self, n=5):\n",
    "        summaries = [ep[\"summary\"] for ep in self.episodes if ep[\"summary\"] is not None]\n",
    "        return summaries[-n:]\n",
    "\n",
    "# ----------------------------\n",
    "# Reflection module\n",
    "# ----------------------------\n",
    "class Reflection:\n",
    "    def __init__(self, knowledge: Knowledge):\n",
    "        self.knowledge = knowledge\n",
    "\n",
    "    def metrics(self, recent_episodes):\n",
    "        returns = [ep[\"return\"] for ep in recent_episodes]\n",
    "        lengths = [ep[\"length\"] for ep in recent_episodes]\n",
    "        avg_return = np.mean(returns) if returns else 0\n",
    "        avg_length = np.mean(lengths) if lengths else 0\n",
    "        threshold = self.knowledge.static_knowledge.get(\"reward_threshold\", 0)\n",
    "        success_count = sum(1 for ep in recent_episodes if ep[\"return\"] >= threshold)\n",
    "        success_rate = success_count / len(recent_episodes) if recent_episodes else 0\n",
    "        return {\"avg_return\": avg_return, \"avg_length\": avg_length, \"success_rate\": success_rate}\n",
    "\n",
    "    def failure_pattern(self, recent_episodes, env_id, iter_idx=None):\n",
    "        prompt = f\"\"\"\n",
    "I have the following {env_id} environment episode summaries: {recent_episodes}\n",
    "Please analyze the most common failure patterns, including state characteristics, action issues, and return patterns. Focus only on key points.\n",
    "Return a concise paragraph.\n",
    "\"\"\"\n",
    "        pattern = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"failure_pattern\": pattern, \"env_id\": env_id, \"last_seen_iter\": iter_idx}, weight=1.0)\n",
    "        return pattern\n",
    "\n",
    "    def edit_suggestion(self, recent_episodes, env_id, iter_idx=None):\n",
    "        prompt = f\"\"\"\n",
    "Based on recent episode data from environment {env_id}: {recent_episodes}\n",
    "Generate one policy editing suggestion in one of the following formats:\n",
    "- add_rule(condition -> action)\n",
    "- modify_threshold(variable, old_value, new_value)\n",
    "- reprioritize(rule_i over rule_j)\n",
    "\n",
    "Return exactly one line with one edit.\n",
    "\"\"\"\n",
    "        suggestion = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"edit_suggestion\": suggestion, \"env_id\": env_id, \"last_seen_iter\": iter_idx}, weight=1.0)\n",
    "        return suggestion\n",
    "\n",
    "# ----------------------------\n",
    "# Safe policy call with ε-greedy hybrid\n",
    "# ----------------------------\n",
    "def safe_policy_call(state, policy_fn, sk, epsilon=0.1):\n",
    "    if random.random() < epsilon:\n",
    "        # random action\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            return random.choice(sk[\"action_space\"])\n",
    "        else:\n",
    "            return random.uniform(sk[\"action_space\"][0], sk[\"action_space\"][1])\n",
    "    try:\n",
    "        a = policy_fn(state)\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            a = sk[\"action_space\"][0]\n",
    "        else:\n",
    "            a = (sk[\"action_space\"][0] + sk[\"action_space\"][1]) / 2.0\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        a = np.clip(a, sk[\"action_space\"][0], sk[\"action_space\"][1])\n",
    "    return a\n",
    "\n",
    "# ----------------------------\n",
    "# Safe step wrapper\n",
    "# ----------------------------\n",
    "def safe_step(env, action):\n",
    "    sk = STATIC_KNOWLEDGE[env.unwrapped.spec.id]\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        action = np.array([np.clip(action, sk[\"action_space\"][0], sk[\"action_space\"][1])]) if np.isscalar(action) else np.clip(np.array(action), sk[\"action_space\"][0], sk[\"action_space\"][1])\n",
    "    return env.step(action)\n",
    "\n",
    "# ----------------------------\n",
    "# Policy generation helper with rule-based constraint\n",
    "# ----------------------------\n",
    "def _action_constraints_text(static_knowledge: dict) -> str:\n",
    "    a = static_knowledge[\"action_space\"]\n",
    "    if static_knowledge.get(\"action_type\") == \"discrete\":\n",
    "        return f\"Discrete actions; valid actions are exactly the integers in {a}.\"\n",
    "    else:\n",
    "        lo, hi = a[0], a[1]\n",
    "        return f\"Continuous action; return a single float within [{lo}, {hi}]. Clip if necessary.\"\n",
    "\n",
    "def generate_base_policy(env_id, knowledge: Knowledge):\n",
    "    guidance = knowledge.get_dynamic_guidance(env_id) or \"\"\n",
    "    doc_url = get_env_doc_url(env_id)\n",
    "    sk = knowledge.static_knowledge\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "    state_vars_text = \"\\n\".join([f\"- {name} in range {rng}\" for name, rng in zip(sk[\"state_vars\"], sk[\"state_ranges\"])])\n",
    "    physics_text = \"\\n\".join([f\"- {k}: {v}\" for k,v in sk.get(\"physics\", {}).items()])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are writing a deterministic, white-box **rule-based policy** for Gymnasium environment \"{env_id}\".\n",
    "Focus on **environment principles, physics, and dynamics**, not superficial patterns.\n",
    "The policy must be based on simple if-else statements or threshold comparisons using state variables.\n",
    "Environment documentation: {doc_url}\n",
    "\n",
    "Observation (state vector):\n",
    "{state_vars_text}\n",
    "\n",
    "Physics references:\n",
    "{physics_text}\n",
    "\n",
    "Action constraints:\n",
    "- {action_desc}\n",
    "- May import 'math' if needed\n",
    "- Must be deterministic\n",
    "- Do not use loops, functions, or external libraries except math\n",
    "- Example: if state[2] > 0: return 1 else: return 0\n",
    "\n",
    "Dynamic guidance:\n",
    "{guidance}\n",
    "\n",
    "Output requirements:\n",
    "- Only one Python function: def policy(state): ...\n",
    "- No explanations, no markdown, no print\n",
    "- Returned action strictly satisfies constraints\n",
    "\"\"\"\n",
    "    policy_code = call_llm(prompt)\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    try:\n",
    "        exec(policy_code, local_vars)\n",
    "        policy_fn = local_vars.get(\"policy\")\n",
    "        if policy_fn is None:\n",
    "            if sk[\"action_type\"] == \"discrete\":\n",
    "                def policy_fn(state): return sk[\"action_space\"][0]\n",
    "            else:\n",
    "                lo, hi = sk[\"action_space\"]\n",
    "                def policy_fn(state): return (lo + hi)/2.0\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            def policy_fn(state): return sk[\"action_space\"][0]\n",
    "        else:\n",
    "            lo, hi = sk[\"action_space\"]\n",
    "            def policy_fn(state): return (lo + hi)/2.0\n",
    "    return policy_fn, policy_code\n",
    "\n",
    "def apply_edit(policy_fn, policy_source, edit_text, knowledge: Knowledge):\n",
    "    sk = knowledge.static_knowledge\n",
    "    existing_src = policy_source\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "\n",
    "    doc_url = get_env_doc_url(knowledge.dynamic_knowledge[0].get(\"env_id\", \"\"))\n",
    "    prompt = f\"\"\"\n",
    "Revise deterministic, **rule-based** policy for environment at {doc_url}.\n",
    "Focus on physics, dynamics, and environment principles.\n",
    "Constraints: {action_desc}\n",
    "Current policy:\n",
    "{existing_src}\n",
    "Edit suggestion: {edit_text}\n",
    "You may use 'math' module.\n",
    "- Must remain if-else or threshold based\n",
    "\n",
    "Output only a valid Python function def policy(state): ...\n",
    "\"\"\"\n",
    "    policy_code = call_llm(prompt)\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    try:\n",
    "        exec(policy_code, local_vars)\n",
    "        new_policy_fn = local_vars.get(\"policy\")\n",
    "        return new_policy_fn, policy_code if new_policy_fn else (policy_fn, policy_source)\n",
    "    except Exception:\n",
    "        return policy_fn, policy_source\n",
    "\n",
    "# ----------------------------\n",
    "# Main closed-loop training\n",
    "# ----------------------------\n",
    "def run_env_loop(env_id, max_iters=10, episodes_per_iter=10, ma_window=3,\n",
    "                 success_rate_threshold=0.8, rollback_window=3,\n",
    "                 truncated_threshold=10, epsilon=0.1):\n",
    "    knowledge = Knowledge()\n",
    "    knowledge.load_static_knowledge(env_id)\n",
    "    memory = Memory()\n",
    "    reflection = Reflection(knowledge)\n",
    "    policy_version = 0\n",
    "    first_iter = True\n",
    "    policy_fn = None\n",
    "    policy_source = None\n",
    "    best_policy_fn = None\n",
    "    best_policy_source = None\n",
    "    best_metrics = {\"avg_return\": -np.inf, \"success_rate\": 0}\n",
    "    recent_metrics_history = []\n",
    "    consecutive_truncated = 0\n",
    "\n",
    "    for iter_idx in range(max_iters):\n",
    "        policy_version += 1\n",
    "        print(f\"=== Iteration {iter_idx+1} ===\")\n",
    "\n",
    "        if first_iter:\n",
    "            policy_fn, policy_source = generate_base_policy(env_id, knowledge)\n",
    "            first_iter = False\n",
    "        else:\n",
    "            recent_episodes = memory.get_recent_episodes()\n",
    "            suggestion = reflection.edit_suggestion(recent_episodes, env_id, iter_idx)\n",
    "            policy_fn, policy_source = apply_edit(policy_fn, policy_source, suggestion, knowledge)\n",
    "\n",
    "        env = gym.make(env_id)\n",
    "        iteration_returns = []\n",
    "        iteration_truncated = False\n",
    "\n",
    "        for ep in range(episodes_per_iter):\n",
    "            s, _ = env.reset()\n",
    "            done = False\n",
    "            memory.start_episode()\n",
    "\n",
    "            while not done:\n",
    "                a = safe_policy_call(s, policy_fn, knowledge.static_knowledge, epsilon=epsilon)\n",
    "                s_next, r, terminated, truncated, info = safe_step(env, a)\n",
    "                if truncated:\n",
    "                    iteration_truncated = True\n",
    "                done = terminated or truncated\n",
    "                memory.add_step(s, a, r, done)\n",
    "                s = s_next\n",
    "\n",
    "            memory.add_episode_summary(env_id, policy_version, policy_source)\n",
    "            iteration_returns.append(memory.episodes[-1][\"summary\"][\"return\"])\n",
    "\n",
    "        recent_episodes_ma = memory.get_recent_episodes(n=ma_window)\n",
    "        metrics = reflection.metrics(recent_episodes_ma)\n",
    "        print(f\"Moving Avg Return: {metrics['avg_return']:.2f}, Success Rate: {metrics['success_rate']:.2f}\")\n",
    "\n",
    "        # Update failure pattern\n",
    "        pattern = reflection.failure_pattern(recent_episodes_ma, env_id, iter_idx)\n",
    "        print(\"Failure Pattern:\", pattern)\n",
    "\n",
    "        # Filter dynamic knowledge periodically\n",
    "        knowledge.filter_dynamic_knowledge(consecutive_inactive=3)\n",
    "\n",
    "        # Update best policy if improved\n",
    "        if metrics[\"avg_return\"] > best_metrics[\"avg_return\"] and metrics[\"success_rate\"] >= success_rate_threshold:\n",
    "            best_metrics = metrics.copy()\n",
    "            best_policy_fn = policy_fn\n",
    "            best_policy_source = policy_source\n",
    "\n",
    "        recent_metrics_history.append(metrics[\"avg_return\"])\n",
    "        if len(recent_metrics_history) > rollback_window:\n",
    "            recent_metrics_history.pop(0)\n",
    "\n",
    "        # Check rollback: if consecutive rollback_window drops, restore best policy\n",
    "        if len(recent_metrics_history) == rollback_window:\n",
    "            if all(recent_metrics_history[i] < recent_metrics_history[i-1] for i in range(1, rollback_window)):\n",
    "                print(f\"Rollback triggered. Restoring previous best policy with Avg Return={best_metrics['avg_return']:.2f}\")\n",
    "                policy_fn = best_policy_fn\n",
    "                policy_source = best_policy_source\n",
    "\n",
    "        # Check convergence based on moving average over ma_window\n",
    "        if len(memory.episodes) >= ma_window * episodes_per_iter:\n",
    "            if metrics[\"avg_return\"] >= knowledge.static_knowledge.get(\"reward_threshold\", 0) and metrics[\"success_rate\"] >= success_rate_threshold:\n",
    "                print(f\"Converged! Stop training. Moving Avg Return={metrics['avg_return']:.2f}, Success Rate={metrics['success_rate']:.2f}\")\n",
    "                break\n",
    "\n",
    "        # Truncated handling: only trigger if consecutive_truncated >= threshold\n",
    "        # Truncated handling (skip for Pendulum-v1)\n",
    "        if env_id != \"Pendulum-v1\":\n",
    "            if iteration_truncated:\n",
    "                consecutive_truncated += 1\n",
    "                if consecutive_truncated >= truncated_threshold:\n",
    "                    print(f\"Consecutive truncated >= {truncated_threshold}. Regenerating new policy via LLM.\")\n",
    "                    policy_fn, policy_source = generate_base_policy(env_id, knowledge)\n",
    "                    consecutive_truncated = 0\n",
    "            else:\n",
    "                consecutive_truncated = 0\n",
    "\n",
    "# ----------------------------\n",
    "# Run multiple control tasks\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    env_list = [\n",
    "        \"Acrobot-v1\",\n",
    "        \"CartPole-v1\",\n",
    "        \"MountainCarContinuous-v0\",\n",
    "        \"MountainCar-v0\",\n",
    "        \"Pendulum-v1\"\n",
    "    ]\n",
    "    for env_id in env_list:\n",
    "        print(f\"==== Running {env_id} ====\")\n",
    "        run_env_loop(env_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71002ea-8fc5-496b-abdc-452c032b8c51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_rl",
   "language": "python",
   "name": "dl_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
