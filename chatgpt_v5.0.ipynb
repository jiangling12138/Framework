{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607d07f8-98c5-4910-8ea2-a3c954602773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Running Acrobot-v1 ====\n",
      "=== Iteration 1 ===\n",
      "[Selected idx 0 v1] MA Return=-70.00  SR=1.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 show consistently negative returns (-72.0 and -66.0) and relatively short episode lengths (67–73 steps), indicating that the agent frequently fails to achieve the task objective, likely falling short of swinging the end-effector to the target height. The repeated returns and lengths suggest the agent is stuck in similar suboptimal behaviors, possibly due to poor exploration or ineffective action selection, such as failing to generate sufficient momentum or getting trapped in local oscillations. The lack of variation in outcomes points to a policy that does not adapt well to different state characteristics, leading to repeated early terminations and limited progress toward solving the environment.\n",
      "Edit Suggestion: modify_threshold(length, 73, 67)\n",
      "Replaced worst idx 2 with new v4: avg_return=-500.00\n",
      "=== Iteration 2 ===\n",
      "[Selected idx 1 v2] MA Return=-176.67  SR=0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 indicate a common failure pattern of prolonged episodes with low returns, as seen in the first summary (length 319, return -318.0), suggesting the agent often fails to achieve the goal efficiently and accumulates penalties for each time step. The shorter episodes (lengths 106 and 108, returns -105.0 and -107.0) imply occasional improvements but still reflect suboptimal performance. Key state characteristics likely include the agent’s inability to consistently swing the lower link upward to the target height, possibly stalling in low-energy or unstable configurations. Action issues may involve repetitive or ineffective torque applications, failing to exploit momentum for rapid ascent. Overall, the return pattern shows a tendency toward high negative values, indicating frequent failure to solve the task within the allowed steps.\n",
      "Edit Suggestion: modify_threshold(length, 319, 150)\n",
      "Replaced worst idx 2 with new v5: avg_return=-115.25\n",
      "=== Iteration 3 ===\n",
      "[Selected idx 2 v5] MA Return=-119.33  SR=0.67\n",
      "Failure Pattern: The Acrobot-v1 episode summaries show returns of -179.0, -96.0, and -83.0, with corresponding episode lengths of 180, 97, and 84 steps, all under policy_version 5. The most common failure pattern is prolonged episodes with highly negative returns, indicating the agent often struggles to reach the goal efficiently. This suggests frequent suboptimal actions, possibly oscillating or failing to generate sufficient upward momentum. State characteristics likely involve the agent remaining in low or unstable positions, unable to transition to the terminal state. Overall, the policy exhibits inconsistent performance, with occasional shorter episodes but generally poor reward outcomes, highlighting issues in action selection and state progression.\n",
      "Edit Suggestion: modify_threshold(length, 180, 90)\n",
      "Replaced worst idx 1 with new v6: avg_return=-106.25\n",
      "=== Iteration 4 ===\n",
      "[Selected idx 1 v6] MA Return=-94.67  SR=0.67\n",
      "Failure Pattern: The Acrobot-v1 episode summaries indicate returns of -97.0, -115.0, and -72.0 with corresponding episode lengths of 98, 116, and 73 steps, all using policy version 6. The consistently negative returns, all above the environment's minimum (-500), suggest the agent often fails to swing the tip above the target line efficiently, terminating episodes before the maximum allowed steps. The variation in episode lengths and returns points to inconsistent policy performance, possibly due to suboptimal action selection—such as insufficient momentum-building swings or premature torque reversals. Common failure patterns likely include the agent stalling in low-energy states or oscillating without achieving the upright position, reflecting challenges in both state transition management and action timing.\n",
      "Edit Suggestion: modify_threshold(return, -100.0, -90.0)\n",
      "Replaced worst idx 2 with new v7: avg_return=-343.50\n",
      "=== Iteration 5 ===\n",
      "[Selected idx 2 v7] MA Return=-327.00  SR=0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 with policy_version 7 show frequent failures to solve the task efficiently, as indicated by consistently low returns (ranging from -219.0 to -500.0) and long episode lengths (220 to the maximum 500 steps). The most common failure pattern is episodes reaching the maximum allowed steps without achieving the goal, suggesting the agent struggles to find effective action sequences to swing the end-effector upward. This likely reflects issues with policy exploration or exploitation, such as repeatedly selecting suboptimal actions that fail to generate sufficient momentum. The state characteristics in these episodes likely involve the Acrobot remaining in low-energy or oscillatory states, unable to escape local minima. Overall, the return patterns and episode lengths indicate the policy is not reliably solving the environment and often stagnates without progress.\n",
      "Edit Suggestion: modify_threshold(max_steps, 500, 300)\n",
      "Replaced worst idx 2 with new v8: avg_return=-500.00\n",
      "=== Iteration 6 ===\n",
      "[Selected idx 2 v8] MA Return=-500.00  SR=0.00\n",
      "Failure Pattern: All episodes for Acrobot-v1 with policy_version 8 show a consistent failure pattern: each episode reaches the maximum length of 500 steps with a return of -500.0, indicating the agent never successfully solves the task. This suggests the agent is either stuck in unproductive states—likely failing to swing the lower link above the required threshold—or repeatedly taking ineffective actions, such as oscillating or remaining stationary. The uniform returns and episode lengths point to a lack of exploration or learning, with the policy unable to escape poor state-action loops or exploit successful trajectories.\n",
      "Edit Suggestion: modify_threshold(max_steps, 500, 400)\n",
      "Replaced worst idx 2 with new v9: avg_return=-268.00\n",
      "=== Iteration 7 ===\n",
      "[Selected idx 2 v9] MA Return=-177.00  SR=0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 with policy_version 9 show consistently negative returns (-181.0, -160.0, -190.0) and episode lengths near the environment's maximum (182, 161, 191), indicating frequent failures to solve the task within the allotted steps. This suggests the agent often fails to swing the lower link above the required threshold, likely remaining in suboptimal states characterized by low vertical displacement and insufficient momentum. Action selection may be ineffective, possibly oscillating between actions without generating the necessary torque or failing to exploit key state transitions. The return pattern reflects persistent inability to reach the goal, with little variation, highlighting a need for improved exploration or more targeted policy updates.\n",
      "Edit Suggestion: modify_threshold(length, 200, 170)\n",
      "Replaced worst idx 2 with new v10: avg_return=-191.75\n",
      "=== Iteration 8 ===\n",
      "[Selected idx 2 v10] MA Return=-310.67  SR=0.00\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 with policy_version 10 show a notable failure pattern: one episode reaches the maximum length of 500 steps with a return of -500.0, indicating the agent failed to solve the task within the allowed time, likely getting stuck in unproductive state cycles or failing to reach the goal state. The other episodes terminate earlier with higher returns (-268.0 and -164.0), suggesting occasional success in progressing toward the goal but with inconsistent performance. The negative returns across all episodes reflect persistent difficulty in achieving upward swing-up and balance, possibly due to suboptimal action selection or poor exploitation of key state transitions. Overall, the agent struggles with efficient exploration and consistent policy execution, leading to frequent failures to complete the task efficiently.\n",
      "Edit Suggestion: modify_threshold(length, 500, 400)\n",
      "Replaced worst idx 2 with new v11: avg_return=-104.50\n",
      "=== Iteration 9 ===\n",
      "[Selected idx 2 v11] MA Return=-103.33  SR=0.67\n",
      "Failure Pattern: The episode summaries for Acrobot-v1 with policy_version 11 reveal consistent failure patterns: all episodes end with negative returns (-80.0, -92.0, -138.0) and episode lengths closely matching the magnitude of the return, suggesting the agent often fails to solve the task within the allowed steps. This pattern indicates the agent struggles to reach the goal state efficiently, likely due to suboptimal action selection—possibly oscillating or failing to generate sufficient momentum to swing the lower link upward. The persistent negative returns and lengthy episodes imply the agent frequently remains in low-reward states, unable to escape local minima or exploit effective state transitions, highlighting issues with exploration and policy effectiveness.\n",
      "Edit Suggestion: modify_threshold(length, 139, 120)\n",
      "Replaced worst idx 2 with new v12: avg_return=-170.25\n",
      "=== Iteration 10 ===\n",
      "[Selected idx 2 v12] MA Return=-110.00  SR=0.67\n",
      "Failure Pattern: Across the Acrobot-v1 episodes, the most common failure patterns include consistently negative returns, indicating the agent often fails to swing the end-effector to the target height efficiently. The episode lengths vary, but all are relatively short, suggesting premature termination likely due to suboptimal action selection—such as insufficient torque application or poor timing in joint movements. State characteristics likely involve the agent getting stuck in low-energy states or failing to build momentum, while action issues may stem from repetitive or uncoordinated actions that do not exploit the environment’s dynamics. Overall, the agent struggles with effective exploration and control, resulting in frequent failures and low returns.\n",
      "Edit Suggestion: modify_threshold(length, 155, 100)\n",
      "Replaced worst idx 2 with new v13: avg_return=-113.75\n",
      "==== Running CartPole-v1 ====\n",
      "=== Iteration 1 ===\n",
      "[Selected idx 0 v1] MA Return=500.00  SR=1.00\n",
      "Failure Pattern: Based on the provided episode summaries, all episodes in the CartPole-v1 environment achieved the maximum possible return and length (500), indicating that the agent consistently balanced the pole without failure. There are no observable failure patterns in terms of state characteristics or action selection, as the agent's policy (version 1) successfully avoided common issues such as pole angle deviation, cart position drift, or suboptimal actions that typically lead to early termination. The return patterns show perfect performance, suggesting robust and stable control throughout each episode.\n",
      "Edit Suggestion: modify_threshold(length, 500, 600)\n",
      "Replaced worst idx 0 with new v4: avg_return=500.00\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "=== Iteration 2 ===\n",
      "[Selected idx 0 v4] MA Return=500.00  SR=1.00\n",
      "Failure Pattern: Based on the provided episode summaries, all episodes in the CartPole-v1 environment achieved the maximum return and length (500.0 and 500, respectively) with policy_version 4. This indicates that no failures occurred within these episodes—each run lasted the full duration without the pole falling or the cart moving out of bounds. Consequently, there are no observable failure patterns in state characteristics (such as excessive pole angle or cart position), action selection issues (like repeated suboptimal moves), or return patterns (such as early termination or low scores). The policy appears robust and consistently successful under the tested conditions.\n",
      "Edit Suggestion: modify_threshold(max_episode_length, 500, 700)\n",
      "Replaced worst idx 0 with new v5: avg_return=500.00\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "=== Iteration 3 ===\n",
      "[Selected idx 0 v5] MA Return=500.00  SR=1.00\n",
      "Failure Pattern: Based on the provided episode summaries, all episodes in the CartPole-v1 environment with policy_version 5 achieved the maximum possible return and episode length (500.0 and 500, respectively), indicating no failures occurred. There are no observable failure patterns in terms of state characteristics or action issues, as the agent consistently maintained balance for the entire episode duration. The return pattern is perfect and uniform, suggesting the policy is highly effective and robust under the tested conditions.\n",
      "Edit Suggestion: modify_threshold(length, 500, 600)\n",
      "Replaced worst idx 0 with new v6: avg_return=500.00\n",
      "Converged! MA Return=500.00, SR=1.00\n",
      "==== Running MountainCarContinuous-v0 ====\n",
      "=== Iteration 1 ===\n",
      "[Selected idx 0 v1] MA Return=92.04  SR=1.00\n",
      "Failure Pattern: Across the provided MountainCarContinuous-v0 episodes, the returns are consistently high (around 91.6 to 92.4) with episode lengths ranging from 79 to 87 steps, indicating successful completion but with some variability in efficiency. Common failure patterns in this environment typically involve insufficient momentum buildup on early swings, leading to longer episode lengths. State-wise, failures often occur when the car stalls near the valley center due to suboptimal acceleration timing. Action issues may include overly cautious or inconsistent throttle application, preventing the car from gaining enough speed to reach the goal efficiently. The return pattern suggests the policy is generally effective but could be improved by optimizing action sequences to reduce episode length and further stabilize returns.\n",
      "Edit Suggestion: modify_threshold(length, 87, 79)\n",
      "Replaced worst idx 2 with new v4: avg_return=91.43\n",
      "=== Iteration 2 ===\n",
      "[Selected idx 1 v2] MA Return=89.37  SR=0.00\n",
      "Failure Pattern: Across the provided MountainCarContinuous-v0 episode summaries, the returns are consistently high (~89.2–89.5) and episode lengths are similar (105–108 steps), indicating the agent reliably reaches the goal but may not be optimizing for minimal steps. Common failure patterns in this environment typically involve the agent struggling to build sufficient momentum by oscillating between forward and backward actions, especially near the valley’s bottom. State characteristics at failure often include low velocity and positions near the center, while action issues may involve suboptimal force application or hesitation, resulting in longer episode lengths. The return pattern suggests the agent is effective but not fully efficient, likely due to conservative or repetitive actions in critical states.\n",
      "Edit Suggestion: modify_threshold(thrust_limit, 1.0, 1.1)\n",
      "Replaced worst idx 1 with new v5: avg_return=91.95\n",
      "=== Iteration 3 ===\n",
      "[Selected idx 1 v5] MA Return=92.30  SR=1.00\n",
      "Failure Pattern: The provided MountainCarContinuous-v0 episode summaries all show identical returns (92.3) and similar episode lengths (78 or 79 steps), suggesting a consistent policy behavior. The lack of variation in returns and lengths indicates the agent reliably reaches the goal but may not be optimizing for speed or efficiency. Common failure patterns in such scenarios often include suboptimal action selection, such as insufficient acceleration at key positions or hesitancy near the hill's peak, leading to longer episode lengths. State-wise, the agent may struggle with momentum buildup or timing the final push, resulting in near-identical but slightly prolonged episodes. Overall, the main issue appears to be repetitive, non-optimal trajectories rather than outright failure to solve the task.\n",
      "Edit Suggestion: modify_threshold(length, 78, 77)\n",
      "Replaced worst idx 2 with new v6: avg_return=89.43\n",
      "Converged! MA Return=92.30, SR=1.00\n",
      "==== Running MountainCar-v0 ====\n",
      "=== Iteration 1 ===\n",
      "[Selected idx 0 v1] MA Return=-198.00  SR=0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 show a consistent failure pattern: all episodes end with the agent failing to reach the goal within the maximum allowed steps (lengths of 200 or close to it), resulting in low returns (mostly -200.0, with one slightly better at -194.0). This suggests the agent is either stuck oscillating between states or unable to generate enough momentum to reach the goal. The repeated maximum episode lengths indicate inefficient exploration or suboptimal action selection, likely with the agent frequently choosing actions that do not build sufficient speed up the slope. Overall, the key issues are lack of effective state transitions toward the goal and poor action choices that prevent escape from the initial valley.\n",
      "Edit Suggestion: modify_threshold(max_steps, 200, 194)\n",
      "Replaced worst idx 1 with new v4: avg_return=-118.50\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "=== Iteration 2 ===\n",
      "[Selected idx 1 v4] MA Return=-120.67  SR=0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 4 show consistently negative returns (ranging from -119.0 to -122.0) and episode lengths matching the absolute value of the returns, indicating the agent typically reaches the goal at the last possible moment. This pattern suggests the agent struggles to efficiently build momentum, likely oscillating near the valley without effectively leveraging the environment’s physics. Common failure characteristics include spending excessive time in low-velocity states and possibly suboptimal action selection (e.g., not accelerating consistently in the correct direction), resulting in slow progress and minimal reward improvement across episodes.\n",
      "Edit Suggestion: modify_threshold(max_steps, 200, 125)\n",
      "Replaced worst idx 2 with new v5: avg_return=-94.25\n",
      "=== Iteration 3 ===\n",
      "[Selected idx 2 v5] MA Return=-97.67  SR=0.67\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 5 show consistently negative returns (-88.0, -116.0, -89.0) and episode lengths matching the magnitude of the returns, indicating the agent often fails to reach the goal efficiently. The most common failure pattern is prolonged episodes where the agent likely oscillates between states near the valley without building sufficient momentum to escape. This suggests issues with action selection—possibly favoring suboptimal or indecisive actions that do not exploit the environment’s dynamics. The returns and lengths imply the agent repeatedly receives the step penalty without achieving the goal, highlighting a lack of effective exploration or exploitation of the environment’s physics.\n",
      "Edit Suggestion: modify_threshold(length, 100, 90)\n",
      "Replaced worst idx 0 with new v6: avg_return=-102.25\n",
      "=== Iteration 4 ===\n",
      "[Selected idx 0 v6] MA Return=-104.00  SR=0.67\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 show consistently negative returns (-96.0, -100.0, -116.0) and episode lengths matching the returns, indicating the agent frequently fails to reach the goal before the maximum step limit. This suggests a common failure pattern where the agent struggles to build sufficient momentum, likely oscillating near the valley without effective acceleration. The actions may lack strategic alternation between left and right to leverage the environment's physics, resulting in inefficient exploration and repeated low-reward states. Overall, the agent's policy version 6 exhibits persistent difficulty escaping the initial state region, leading to premature episode termination and suboptimal returns.\n",
      "Edit Suggestion: modify_threshold(max_steps, 200, 120)\n",
      "Replaced worst idx 1 with new v7: avg_return=-101.00\n",
      "=== Iteration 5 ===\n",
      "[Selected idx 1 v7] MA Return=-108.33  SR=0.33\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 7 show consistently negative returns (-92.0, -119.0, -114.0) and episode lengths matching the magnitude of the returns, indicating the agent often fails to reach the goal before the time limit. This suggests common failure patterns include insufficient momentum buildup, likely due to suboptimal action selection (e.g., not oscillating enough to gain speed). State characteristics at failure likely involve the car remaining near the valley or failing to crest the hill. The return patterns reflect the environment’s reward structure, where each timestep incurs a penalty, so longer episodes without success result in more negative returns. Overall, the agent struggles with effective exploration and momentum management.\n",
      "Edit Suggestion: modify_threshold(length, 119, 90)\n",
      "Replaced worst idx 1 with new v8: avg_return=-165.50\n",
      "=== Iteration 6 ===\n",
      "[Selected idx 1 v8] MA Return=-155.33  SR=0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 8 show frequent failures characterized by long episode lengths (130–200 steps) and consistently low returns (ranging from -130.0 to -200.0), indicating the agent often fails to reach the goal before the time limit. This suggests the agent struggles to generate sufficient momentum, likely due to suboptimal action selection—such as inadequate oscillation between left and right actions needed to climb the hill. State characteristics in these failures likely involve the car remaining near the valley or failing to build up enough velocity, resulting in repeated, ineffective movements. Overall, the pattern points to inefficient exploration and poor exploitation of the environment’s dynamics.\n",
      "Edit Suggestion: modify_threshold(length, 200, 150)\n",
      "Replaced worst idx 1 with new v9: avg_return=-118.50\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "=== Iteration 7 ===\n",
      "[Selected idx 1 v9] MA Return=-120.33  SR=0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 show consistently negative returns (-114.0, -124.0, -123.0) with episode lengths matching the magnitude of the returns, indicating the agent often fails to reach the goal and terminates after many steps. This suggests common failure patterns include insufficient momentum buildup, likely due to suboptimal action selection (e.g., not alternating actions to build speed on slopes). State characteristics probably involve the car remaining near the valley or failing to escape the left side, while action issues may include repetitive or poorly timed accelerations. Overall, the agent struggles to exploit the environment’s dynamics, leading to long episodes with low rewards.\n",
      "Edit Suggestion: modify_threshold(length, 124, 110)\n",
      "Replaced worst idx 1 with new v10: avg_return=-116.75\n",
      "=== Iteration 8 ===\n",
      "[Selected idx 1 v10] MA Return=-118.33  SR=0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 with policy_version 10 show consistently negative returns (-122.0, -113.0, -120.0) and episode lengths matching the magnitude of returns, indicating the agent often fails to reach the goal before the time limit. This pattern suggests the agent frequently gets stuck in local minima, likely near the bottom of the valley, and struggles to build enough momentum to escape. Action selection may be suboptimal, with insufficient alternation between left and right actions to gain speed, resulting in repeated, inefficient movements. Overall, the agent's policy exhibits poor exploration and momentum-building, leading to premature episode termination and low returns.\n",
      "Edit Suggestion: modify_threshold(length, 120, 110)\n",
      "Replaced worst idx 1 with new v11: avg_return=-170.00\n",
      "=== Iteration 9 ===\n",
      "[Selected idx 1 v11] MA Return=-193.67  SR=0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 show a consistent failure pattern: most episodes terminate at the maximum length (200 steps) with the minimum possible return (-200.0), indicating the agent repeatedly fails to reach the goal. This suggests the agent often gets stuck oscillating near the starting position, unable to build enough momentum to escape the valley. The rare episode with a return of -181.0 and length 181 shows slightly better performance but still falls short of success. The key issues likely include poor action selection—such as not coordinating left/right moves to gain momentum—and insufficient exploration or exploitation of the environment’s dynamics. Overall, the agent’s policy version 11 struggles to escape the local minima and fails to achieve positive returns.\n",
      "Edit Suggestion: modify_threshold(max_steps, 200, 250)\n",
      "Replaced worst idx 1 with new v12: avg_return=-164.75\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "=== Iteration 10 ===\n",
      "[Selected idx 1 v12] MA Return=-177.00  SR=0.00\n",
      "Failure Pattern: The episode summaries for MountainCar-v0 show a common failure pattern where the agent frequently reaches the maximum episode length of 200 steps without achieving the goal, resulting in the minimum return of -200.0. This suggests the agent often gets stuck in suboptimal states, likely failing to build enough momentum to reach the top of the hill. The occasional shorter episode with a higher return (e.g., -131.0 over 131 steps) indicates sporadic success but overall inconsistency. The primary issues appear to be ineffective action selection—possibly oscillating or not accelerating optimally—and an inability to escape low-energy states, leading to repeated failures to solve the task within the time limit.\n",
      "Edit Suggestion: modify_threshold(max_steps, 200, 150)\n",
      "Replaced worst idx 1 with new v13: avg_return=-129.25\n",
      "[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\n",
      "==== Running Pendulum-v1 ====\n",
      "=== Iteration 1 ===\n",
      "[Selected idx 0 v1] MA Return=-969.53  SR=0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the consistently low returns (ranging from -929 to -1026) indicate persistent failure to keep the pendulum upright and stable, likely due to inadequate control strategies. The episode lengths are all at the maximum (200 steps), suggesting the agent survives but fails to achieve the goal state, often oscillating or swinging without stabilizing. Common failure patterns likely include the pendulum remaining far from the upright position, frequent large corrective actions, and poor exploitation of the environment’s dynamics. These issues point to insufficient policy learning, with actions failing to minimize angular velocity and keep the pendulum near vertical, resulting in consistently negative rewards.\n",
      "Edit Suggestion: modify_threshold(max_torque, 2.0, 2.5)\n",
      "Replaced worst idx 2 with new v4: avg_return=-1179.55\n",
      "=== Iteration 2 ===\n",
      "[Selected idx 1 v2] MA Return=-1179.62  SR=0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 2 show consistently negative returns (ranging from -1512.06 to -809.68) over the maximum episode length of 200 steps, indicating the agent struggles to keep the pendulum upright and balanced. The high magnitude of negative returns suggests frequent or prolonged deviations from the upright position, likely due to insufficient corrective actions or overcorrection. This points to common failure patterns such as the pendulum swinging wildly or failing to recover from large angles, possibly caused by suboptimal action selection (e.g., weak or inconsistent torque application). Overall, the agent fails to stabilize the pendulum, resulting in poor performance across episodes.\n",
      "Edit Suggestion: modify_threshold(return, -1500, -1000)\n",
      "Replaced worst idx 1 with new v5: avg_return=-986.83\n",
      "=== Iteration 3 ===\n",
      "[Selected idx 1 v5] MA Return=-1369.74  SR=0.00\n",
      "Failure Pattern: Across the provided Pendulum-v1 episodes, the most common failure pattern is consistently low returns (ranging from approximately -1094 to -1508), indicating poor control over the pendulum’s upright position. All episodes reach the maximum length of 200 steps, suggesting the agent does not terminate early but struggles to stabilize the pendulum. Typical state characteristics in such failures include large angular deviations and high angular velocities, reflecting an inability to keep the pendulum near vertical. Action issues likely involve insufficient or poorly timed torque applications, failing to counteract the pendulum’s momentum. Overall, the agent’s policy (version 5) appears unable to recover from destabilized states, leading to persistently negative rewards throughout each episode.\n",
      "Edit Suggestion: modify_threshold(max_torque, 2.0, 2.5)\n",
      "Replaced worst idx 1 with new v6: avg_return=-1313.81\n",
      "=== Iteration 4 ===\n",
      "[Selected idx 1 v6] MA Return=-1255.14  SR=0.00\n",
      "Failure Pattern: Across the provided Pendulum-v1 episodes, the most common failure pattern is consistently low returns (ranging from -1766 to -759), indicating the agent struggles to keep the pendulum upright and stable. All episodes reach the maximum length (200 steps), suggesting the agent does not terminate early but accumulates negative rewards throughout. This typically points to state characteristics where the pendulum remains far from the upright position, with frequent large angular deviations and velocities. Action-wise, the policy likely produces insufficient or poorly timed torques, failing to counteract the pendulum’s swing effectively. Overall, the return pattern shows some improvement (less negative returns), but persistent instability and suboptimal control remain the key issues.\n",
      "Edit Suggestion: modify_threshold(max_torque, 2.0, 2.5)\n",
      "Replaced worst idx 1 with new v7: avg_return=-1120.91\n",
      "=== Iteration 5 ===\n",
      "[Selected idx 1 v7] MA Return=-941.52  SR=0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes, the most common failure patterns include consistently low returns (ranging from -1251.76 to -738.02), indicating the policy struggles to keep the pendulum upright and near its target angle. The episode lengths are always 200 steps, suggesting the agent fails to solve the task early and likely oscillates or swings erratically throughout. Key state issues likely involve the pendulum frequently deviating from the upright position, while action problems may include insufficient or poorly timed torque application, preventing stabilization. Overall, the policy version 7 demonstrates persistent difficulty in achieving stable control, as reflected by the negative returns and lack of early episode termination.\n",
      "Edit Suggestion: modify_threshold(torque_limit, 2.0, 2.5)\n",
      "Replaced worst idx 2 with new v8: avg_return=-1151.17\n",
      "=== Iteration 6 ===\n",
      "[Selected idx 2 v8] MA Return=-1314.78  SR=0.00\n",
      "Failure Pattern: Across the Pendulum-v1 episodes with policy_version 8, the returns are consistently low (ranging from -1457.59 to -1186.04), indicating persistent failure to keep the pendulum upright and stable. The episode lengths are all at the maximum (200 steps), suggesting the agent does not terminate early but struggles throughout. Common failure patterns likely include the pendulum frequently swinging far from the upright position, with state characteristics showing large angles and high angular velocities. Action issues may involve insufficient or poorly timed torque applications, failing to counteract the pendulum’s momentum effectively. Overall, the agent exhibits difficulty in learning stable control, resulting in consistently poor returns.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, increased_value)\n",
      "Replaced worst idx 2 with new v9: avg_return=-1194.63\n",
      "=== Iteration 7 ===\n",
      "[Selected idx 2 v9] MA Return=-1238.55  SR=0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 9 consistently show long episode lengths (200 steps), indicating the agent survives for the full duration but struggles to achieve high returns, with all returns being significantly negative (ranging from -1492.99 to -1037.44). This suggests the agent frequently fails to keep the pendulum upright and near the target angle, likely spending much time in states with large angular deviations and high velocities. The negative returns imply inefficient or poorly timed actions, possibly with excessive or insufficient torque, leading to oscillations or failure to stabilize the pendulum. Overall, the key failure patterns are persistent instability, suboptimal action selection, and inability to recover from off-balance states, resulting in consistently poor performance.\n",
      "Edit Suggestion: modify_threshold(max_torque, 2.0, 2.5)\n",
      "Replaced worst idx 2 with new v10: avg_return=-1232.54\n",
      "=== Iteration 8 ===\n",
      "[Selected idx 2 v10] MA Return=-1219.39  SR=0.00\n",
      "Failure Pattern: Across the provided Pendulum-v1 episodes, the agent consistently completes the maximum episode length (200 steps) but achieves low returns (ranging from -1369 to -1136), indicating persistent difficulty in maintaining the pendulum upright and stable. The most common failure pattern is likely the agent's inability to apply sufficiently precise or timely torque, resulting in the pendulum swinging erratically or failing to recover from deviations. This suggests issues with action selection—possibly overly aggressive or poorly tuned actions—leading to inefficient energy use and frequent large-angle deviations. The consistently negative returns and full episode lengths imply that the agent rarely stabilizes the pendulum and spends most of the time in suboptimal, high-cost states.\n",
      "Edit Suggestion: modify_threshold(max_torque, 2.0, 2.5)\n",
      "Replaced worst idx 2 with new v11: avg_return=-904.33\n",
      "=== Iteration 9 ===\n",
      "[Selected idx 2 v11] MA Return=-786.25  SR=0.00\n",
      "Failure Pattern: Across the provided Pendulum-v1 episode summaries, the most common failure pattern is consistently low returns (ranging from approximately -997 to -406) despite each episode reaching the maximum length of 200 steps. This suggests the agent struggles to keep the pendulum upright and balanced, likely resulting in frequent large negative rewards. The wide variation in returns indicates inconsistent policy performance, possibly due to suboptimal or unstable action selection—such as applying excessive or insufficient torque at critical states. Overall, the agent appears to have difficulty maintaining control, leading to poor state trajectories and inefficient recovery from deviations.\n",
      "Edit Suggestion: modify_threshold(torque_limit, 2.0, 2.5)\n",
      "Replaced worst idx 0 with new v12: avg_return=-1157.53\n",
      "=== Iteration 10 ===\n",
      "[Selected idx 0 v12] MA Return=-1185.07  SR=0.00\n",
      "Failure Pattern: The episode summaries for Pendulum-v1 with policy_version 12 show consistently poor returns (ranging from approximately -627 to -1526), all at the maximum episode length of 200 steps, indicating the agent often fails to stabilize the pendulum upright. The large negative returns suggest frequent high-angle deviations and possibly high angular velocities, meaning the pendulum spends significant time far from the upright position. Action-wise, this may reflect either insufficient or overly aggressive torque application, leading to oscillations or failure to counteract gravity effectively. The pattern of consistently long episodes with poor returns highlights persistent control issues rather than early terminations, suggesting the policy struggles with fine-grained balance and energy-efficient corrections throughout the episode.\n",
      "Edit Suggestion: modify_threshold(torque_limit, current_value, current_value * 1.2)\n",
      "Replaced worst idx 0 with new v13: avg_return=-1128.24\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Closed-loop RL:\n",
    "- Policy Pool + UCB bandit selection\n",
    "- Parallel multi-candidate policy generation & evaluation\n",
    "- Safe 1-step MCTS (uses clone_state/restore_state if available)\n",
    "- Truncated handling: Acrobot/Pendulum won't trigger regeneration\n",
    "\n",
    "环境\t                    核心问题\n",
    "Acrobot-v1\t                swing-up 动作协调差，容易被低能量摆动卡住；truncated 常触发\n",
    "CartPole-v1\t                已收敛，但对 edge-case 仍可能 early failure；truncated 事件较少\n",
    "MountainCarContinuous-v0\t完成任务，但动作效率不高，略显保守\n",
    "MountainCar-v0\t            动能累积不足，policy oscillation，探索不足\n",
    "Pendulum-v1\ttorque          不够，动作精度差，策略不连续，失败频繁\n",
    "\n",
    "\n",
    "文件/函数\t修改建议\n",
    "generate_policy()\t1. 在 rule 生成时加入 ε-greedy 或 Gaussian noise。\n",
    "2. 连续环境动作幅度自适应。\n",
    "Memory\t1. 存储源码字符串 + 可解释规则。\n",
    "2. 增加权重机制，定期清理老旧策略。\n",
    "apply_edit()\t1. 增加 truncated 智能处理：若 episode truncated → 调整 ε 或动作幅度，而不是直接替换 policy。\n",
    "iteration loop\t1. 延迟更新 best_policy。\n",
    "2. MA return 收敛条件加入 SR > threshold 条件。\n",
    "failure_pattern_analysis()\t1. 增加对“动作幅度不足 / oscillation / momentum 不够”的识别。\n",
    "\"\"\"\n",
    "\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "import inspect\n",
    "import math\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ----------------------------\n",
    "# LLM HTTP call function\n",
    "# ----------------------------\n",
    "LLM_URL = 'https://api.yesapikey.com/v1/chat/completions'\n",
    "LLM_HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer sk-nNlzbvBIDhXdy8i02f95Ac27Db33490bAbFc66E725C2B9E9'\n",
    "}\n",
    "\n",
    "def call_llm(prompt, model=\"gpt-4.1-2025-04-14\", temperature=0.2, max_tokens=1024):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(LLM_URL, json=data, headers=LLM_HEADERS, timeout=60)\n",
    "            if response.status_code == 200:\n",
    "                resp_json = response.json()\n",
    "                if 'choices' in resp_json and resp_json['choices']:\n",
    "                    content = resp_json['choices'][0].get('message', {}).get('content')\n",
    "                    return content\n",
    "            else:\n",
    "                print(\"[LLM HTTP]\", response.status_code, response.text[:200])\n",
    "        except Exception as e:\n",
    "            print(\"[LLM Exception]\", e)\n",
    "        time.sleep(2)\n",
    "\n",
    "# ----------------------------\n",
    "# Environment documentation mapping\n",
    "# ----------------------------\n",
    "ENV_DOC_URL = {\n",
    "    \"Acrobot-v1\": \"https://gymnasium.farama.org/environments/classic_control/acrobot/\",\n",
    "    \"CartPole-v1\": \"https://gymnasium.farama.org/environments/classic_control/cart_pole/\",\n",
    "    \"MountainCarContinuous-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\",\n",
    "    \"MountainCar-v0\": \"https://gymnasium.farama.org/environments/classic_control/mountain_car/\",\n",
    "    \"Pendulum-v1\": \"https://gymnasium.farama.org/environments/classic_control/pendulum/\"\n",
    "}\n",
    "\n",
    "def get_env_doc_url(env_id: str) -> str:\n",
    "    return ENV_DOC_URL.get(env_id, \"https://gymnasium.farama.org/\")\n",
    "\n",
    "# ----------------------------\n",
    "# Static Knowledge\n",
    "# ----------------------------\n",
    "STATIC_KNOWLEDGE = {\n",
    "    \"CartPole-v1\": {\n",
    "        \"state_dim\": 4,\n",
    "        \"state_vars\": [\"cart_position\", \"cart_velocity\", \"pole_angle\", \"pole_velocity\"],\n",
    "        \"state_ranges\": [(-4.8, 4.8), (-float(\"inf\"), float(\"inf\")), (-0.418, 0.418), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1],\n",
    "        \"reward_threshold\": 475,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"Acrobot-v1\": {\n",
    "        \"state_dim\": 6,\n",
    "        \"state_vars\": [\"cos_theta1\", \"sin_theta1\", \"cos_theta2\", \"sin_theta2\", \"theta1_dot\", \"theta2_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\")), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -100,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"MountainCar-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [0, 1, 2],\n",
    "        \"reward_threshold\": -110,\n",
    "        \"action_type\": \"discrete\"\n",
    "    },\n",
    "    \"MountainCarContinuous-v0\": {\n",
    "        \"state_dim\": 2,\n",
    "        \"state_vars\": [\"position\", \"velocity\"],\n",
    "        \"state_ranges\": [(-1.2, 0.6), (-0.07, 0.07)],\n",
    "        \"action_space\": [-1.0, 1.0],\n",
    "        \"reward_threshold\": 90,\n",
    "        \"action_type\": \"continuous\"\n",
    "    },\n",
    "    \"Pendulum-v1\": {\n",
    "        \"state_dim\": 3,\n",
    "        \"state_vars\": [\"cos_theta\", \"sin_theta\", \"theta_dot\"],\n",
    "        \"state_ranges\": [(-1, 1), (-1, 1), (-float(\"inf\"), float(\"inf\"))],\n",
    "        \"action_space\": [-2.0, 2.0],\n",
    "        \"reward_threshold\": -200,\n",
    "        \"action_type\": \"continuous\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Knowledge module\n",
    "# ----------------------------\n",
    "class Knowledge:\n",
    "    def __init__(self):\n",
    "        self.static_knowledge = {}\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def load_static_knowledge(self, env_id):\n",
    "        if env_id not in STATIC_KNOWLEDGE:\n",
    "            raise ValueError(\"Unsupported environment\")\n",
    "        self.static_knowledge = STATIC_KNOWLEDGE[env_id]\n",
    "        self.dynamic_knowledge = []\n",
    "\n",
    "    def add_dynamic_entry(self, entry):\n",
    "        self.dynamic_knowledge.append(entry)\n",
    "\n",
    "    def get_dynamic_guidance(self, env_id):\n",
    "        prompt = f\"\"\"\n",
    "I am generating a policy in environment {env_id}.\n",
    "Current dynamic knowledge entries: {self.dynamic_knowledge}\n",
    "\n",
    "Focus on environment **principles, physics, and dynamics**, not superficial patterns.\n",
    "Please provide concise heuristic suggestions for policy generation based on this knowledge, such as:\n",
    "- State ranges to prioritize\n",
    "- Common failing action patterns\n",
    "- Recommended threshold adjustments\n",
    "\n",
    "Return a short, structured bullet list (no prose).\n",
    "\"\"\"\n",
    "        guidance = call_llm(prompt)\n",
    "        return guidance\n",
    "\n",
    "# ----------------------------\n",
    "# Memory module\n",
    "# ----------------------------\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.episodes = []\n",
    "\n",
    "    def start_episode(self):\n",
    "        self.episodes.append({\"steps\": [], \"summary\": None})\n",
    "\n",
    "    def add_step(self, s, a, r, done):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"Please call start_episode() before adding steps!\")\n",
    "        self.episodes[-1][\"steps\"].append({\"s\": s, \"a\": a, \"r\": r, \"done\": done})\n",
    "\n",
    "    def add_episode_summary(self, env_id, policy_version):\n",
    "        if not self.episodes:\n",
    "            raise ValueError(\"No running episode!\")\n",
    "        steps = self.episodes[-1][\"steps\"]\n",
    "        total_reward = sum(step[\"r\"] for step in steps)\n",
    "        length = len(steps)\n",
    "        self.episodes[-1][\"summary\"] = {\n",
    "            \"env_id\": env_id,\n",
    "            \"policy_version\": policy_version,\n",
    "            \"return\": total_reward,\n",
    "            \"length\": length\n",
    "        }\n",
    "\n",
    "    def get_recent_episodes(self, n=5):\n",
    "        summaries = [ep[\"summary\"] for ep in self.episodes if ep[\"summary\"] is not None]\n",
    "        return summaries[-n:]\n",
    "\n",
    "# ----------------------------\n",
    "# Reflection module\n",
    "# ----------------------------\n",
    "class Reflection:\n",
    "    def __init__(self, knowledge: Knowledge):\n",
    "        self.knowledge = knowledge\n",
    "\n",
    "    def metrics(self, recent_episodes):\n",
    "        returns = [ep[\"return\"] for ep in recent_episodes]\n",
    "        lengths = [ep[\"length\"] for ep in recent_episodes]\n",
    "        avg_return = np.mean(returns) if returns else 0.0\n",
    "        avg_length = np.mean(lengths) if lengths else 0.0\n",
    "        threshold = self.knowledge.static_knowledge.get(\"reward_threshold\", 0.0)\n",
    "        success_count = sum(1 for ep in recent_episodes if ep[\"return\"] >= threshold)\n",
    "        success_rate = (success_count / len(recent_episodes)) if recent_episodes else 0.0\n",
    "        return {\"avg_return\": float(avg_return), \"avg_length\": float(avg_length), \"success_rate\": float(success_rate)}\n",
    "\n",
    "    def failure_pattern(self, recent_episodes, env_id):\n",
    "        prompt = f\"\"\"\n",
    "I have the following {env_id} environment episode summaries: {recent_episodes}\n",
    "Please analyze the most common failure patterns, including state characteristics, action issues, and return patterns. Focus only on key points.\n",
    "Return a concise paragraph.\n",
    "\"\"\"\n",
    "        pattern = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"env_id\": env_id, \"failure_pattern\": pattern})\n",
    "        return pattern\n",
    "\n",
    "    def edit_suggestion(self, recent_episodes, env_id):\n",
    "        prompt = f\"\"\"\n",
    "Based on recent episode data from environment {env_id}: {recent_episodes}\n",
    "Generate one policy editing suggestion in one of the following formats:\n",
    "- add_rule(condition -> action)\n",
    "- modify_threshold(variable, old_value, new_value)\n",
    "- reprioritize(rule_i over rule_j)\n",
    "\n",
    "Return exactly one line with one edit.\n",
    "\"\"\"\n",
    "        suggestion = call_llm(prompt).strip()\n",
    "        self.knowledge.add_dynamic_entry({\"env_id\": env_id, \"edit_suggestion\": suggestion})\n",
    "        return suggestion\n",
    "\n",
    "# ----------------------------\n",
    "# Policy pool for multi-strategy search\n",
    "# ----------------------------\n",
    "class PolicyPool:\n",
    "    def __init__(self, max_size=5):\n",
    "        # each: {\"fn\": callable, \"version\": int, \"metrics\": dict|None}\n",
    "        self.policies = []\n",
    "        self.max_size = max_size\n",
    "        self.counts = []  # pulls for UCB\n",
    "        self.values = []  # mean value for UCB (moving average)\n",
    "\n",
    "    def add_policy(self, policy_fn, version, metrics=None):\n",
    "        entry = {\"fn\": policy_fn, \"version\": version, \"metrics\": metrics}\n",
    "        if len(self.policies) < self.max_size:\n",
    "            self.policies.append(entry)\n",
    "            self.counts.append(0)\n",
    "            self.values.append(0.0)\n",
    "        else:\n",
    "            idx = self.get_worst_policy_idx()\n",
    "            self.policies[idx] = entry\n",
    "            self.counts[idx] = 0\n",
    "            self.values[idx] = 0.0\n",
    "\n",
    "    def select_policy_ucb(self, c=1.0):\n",
    "        if not self.policies:\n",
    "            raise RuntimeError(\"PolicyPool is empty\")\n",
    "        total_counts = sum(self.counts) + 1\n",
    "        ucb_scores = []\n",
    "        for i in range(len(self.policies)):\n",
    "            if self.counts[i] == 0:\n",
    "                ucb_scores.append(float('inf'))\n",
    "            else:\n",
    "                ucb_scores.append(self.values[i] + c * math.sqrt(math.log(total_counts)/self.counts[i]))\n",
    "        idx = int(np.argmax(ucb_scores))\n",
    "        self.counts[idx] += 1\n",
    "        return self.policies[idx][\"fn\"], idx\n",
    "\n",
    "    def update_policy_value(self, idx, reward):\n",
    "        n = self.counts[idx]\n",
    "        if n <= 0:\n",
    "            self.values[idx] = reward\n",
    "        else:\n",
    "            # incremental mean\n",
    "            self.values[idx] = ((n-1)/n)*self.values[idx] + (1/n)*reward\n",
    "\n",
    "    def get_worst_policy_idx(self):\n",
    "        # define worst by avg_return in metrics; if None -> very bad\n",
    "        vals = []\n",
    "        for p in self.policies:\n",
    "            if p[\"metrics\"] and \"avg_return\" in p[\"metrics\"]:\n",
    "                vals.append(p[\"metrics\"][\"avg_return\"])\n",
    "            else:\n",
    "                vals.append(-float('inf'))\n",
    "        return int(np.argmin(vals))\n",
    "\n",
    "# ----------------------------\n",
    "# Safe step wrapper\n",
    "# ----------------------------\n",
    "def safe_step(env, action):\n",
    "    sk = STATIC_KNOWLEDGE[env.unwrapped.spec.id]\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        lo, hi = sk[\"action_space\"]\n",
    "        if np.isscalar(action):\n",
    "            action = np.array([np.clip(action, lo, hi)], dtype=np.float32)\n",
    "        else:\n",
    "            action = np.clip(np.array(action, dtype=np.float32), lo, hi)\n",
    "    return env.step(action)\n",
    "\n",
    "# ----------------------------\n",
    "# Safe policy call with optional 1-step MCTS\n",
    "# ----------------------------\n",
    "def safe_policy_call(state, policy_fn, sk, env):\n",
    "    \"\"\"\n",
    "    - Calls rule policy to propose action.\n",
    "    - For discrete envs, if env supports clone_state/restore_state, do 1-step lookahead\n",
    "      over all actions & pick best immediate reward (does NOT pollute real trajectory).\n",
    "    - If clone_state is not available, skip lookahead.\n",
    "    \"\"\"\n",
    "    # Base action from rule\n",
    "    try:\n",
    "        a = policy_fn(state)\n",
    "    except Exception as e:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            a = random.choice(sk[\"action_space\"])\n",
    "        else:\n",
    "            lo, hi = sk[\"action_space\"]\n",
    "            a = 0.5 * (lo + hi)\n",
    "\n",
    "    # Clip for continuous now (final safety also happens in safe_step)\n",
    "    if sk[\"action_type\"] == \"continuous\":\n",
    "        lo, hi = sk[\"action_space\"]\n",
    "        a = float(np.clip(a, lo, hi))\n",
    "        return a\n",
    "\n",
    "    # Discrete: try 1-step lookahead with clone_state\n",
    "    if sk[\"action_type\"] == \"discrete\":\n",
    "        uw = getattr(env, \"unwrapped\", env)\n",
    "        can_clone = hasattr(uw, \"clone_state\") and hasattr(uw, \"restore_state\")\n",
    "        if not can_clone:\n",
    "            return a  # graceful fallback\n",
    "\n",
    "        best_a, best_r = a, -float('inf')\n",
    "        try:\n",
    "            snapshot = uw.clone_state()\n",
    "        except Exception:\n",
    "            return a\n",
    "        for cand in sk[\"action_space\"]:\n",
    "            try:\n",
    "                # step then restore\n",
    "                obs2, r, term, trunc, _ = safe_step(env, cand)\n",
    "                # restore to snapshot for next branch\n",
    "                uw.restore_state(snapshot)\n",
    "                # purely 1-step score\n",
    "                if r > best_r:\n",
    "                    best_r, best_a = r, cand\n",
    "            except Exception:\n",
    "                # if any branch fails, ignore that candidate\n",
    "                try:\n",
    "                    uw.restore_state(snapshot)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                continue\n",
    "        return best_a\n",
    "\n",
    "    return a\n",
    "\n",
    "# ----------------------------\n",
    "# Policy generation helper with rule-based constraint\n",
    "# ----------------------------\n",
    "def _action_constraints_text(static_knowledge: dict) -> str:\n",
    "    a = static_knowledge[\"action_space\"]\n",
    "    if static_knowledge.get(\"action_type\") == \"discrete\":\n",
    "        return f\"Discrete actions; valid actions are exactly the integers in {a}.\"\n",
    "    else:\n",
    "        lo, hi = a[0], a[1]\n",
    "        return f\"Continuous action; return a single float within [{lo}, {hi}]. Clip if necessary.\"\n",
    "\n",
    "def generate_rule_policy_code(env_id, knowledge: Knowledge):\n",
    "    sk = knowledge.static_knowledge\n",
    "    guidance = knowledge.get_dynamic_guidance(env_id) or \"\"\n",
    "    doc_url = get_env_doc_url(env_id)\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "    state_vars_text = \"\\n\".join([f\"- {name} in range {rng}\" for name, rng in zip(sk[\"state_vars\"], sk[\"state_ranges\"])])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are writing a deterministic, white-box **rule-based policy** for Gymnasium environment \"{env_id}\".\n",
    "Focus on **environment principles, physics, and dynamics**, not superficial patterns.\n",
    "The policy must be based on simple if-else statements or threshold comparisons using state variables.\n",
    "Environment documentation: {doc_url}\n",
    "\n",
    "Observation (state vector):\n",
    "{state_vars_text}\n",
    "\n",
    "Action constraints:\n",
    "- {action_desc}\n",
    "- May import 'math' if needed\n",
    "- Must be deterministic\n",
    "- Do not use loops, functions, or external libraries except math\n",
    "- Example (discrete): if state[2] > 0: return 1 else: return 0\n",
    "- Example (continuous): return max(min(k1*state[1]-k2*state[0], hi), lo)\n",
    "\n",
    "Dynamic guidance:\n",
    "{guidance}\n",
    "\n",
    "Output requirements:\n",
    "- Only one Python function: def policy(state): ...\n",
    "- No explanations, no markdown, no print\n",
    "- Returned action strictly satisfies constraints\n",
    "\"\"\"\n",
    "    code = call_llm(prompt)\n",
    "    return code\n",
    "\n",
    "def compile_policy_or_default(code, sk):\n",
    "    local_vars = {\"math\": math, \"np\": np}\n",
    "    try:\n",
    "        exec(code, local_vars)\n",
    "        policy_fn = local_vars.get(\"policy\")\n",
    "        if policy_fn is None:\n",
    "            raise ValueError(\"No function 'policy' found\")\n",
    "        return policy_fn\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            def policy(state): return sk[\"action_space\"][0]\n",
    "        else:\n",
    "            lo, hi = sk[\"action_space\"]\n",
    "            def policy(state): return 0.5 * (lo + hi)\n",
    "        return policy\n",
    "\n",
    "def generate_base_policies(env_id, knowledge: Knowledge, n_candidates=3):\n",
    "    \"\"\"Return list[policy_fn] of length n_candidates.\"\"\"\n",
    "    sk = knowledge.static_knowledge\n",
    "    fns = []\n",
    "    for _ in range(n_candidates):\n",
    "        code = generate_rule_policy_code(env_id, knowledge)\n",
    "        fns.append(compile_policy_or_default(code, sk))\n",
    "    return fns\n",
    "\n",
    "def apply_edit(policy_fn, edit_text, knowledge: Knowledge, env_id: str):\n",
    "    \"\"\"Ask LLM to edit the current rule policy.\"\"\"\n",
    "    sk = knowledge.static_knowledge\n",
    "    try:\n",
    "        existing_src = inspect.getsource(policy_fn)\n",
    "    except Exception:\n",
    "        if sk[\"action_type\"] == \"discrete\":\n",
    "            existing_src = \"def policy(state):\\n    return \" + str(sk[\"action_space\"][0])\n",
    "        else:\n",
    "            lo, hi = sk[\"action_space\"]\n",
    "            existing_src = \"def policy(state):\\n    return \" + str(0.5*(lo+hi))\n",
    "\n",
    "    action_desc = _action_constraints_text(sk)\n",
    "    doc_url = get_env_doc_url(env_id)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Revise deterministic, **rule-based** policy for environment at {doc_url}.\n",
    "Focus on physics, dynamics, and environment principles.\n",
    "Constraints: {action_desc}\n",
    "Current policy:\n",
    "{existing_src}\n",
    "\n",
    "Edit suggestion: {edit_text}\n",
    "\n",
    "Rules:\n",
    "- Keep it deterministic and rule-based (if-else / threshold)\n",
    "- Only output a single valid Python function: def policy(state): ...\n",
    "- You may use 'math'\n",
    "\"\"\"\n",
    "    code = call_llm(prompt)\n",
    "    return compile_policy_or_default(code, sk)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation helpers\n",
    "# ----------------------------\n",
    "def eval_policy_once(env_id, policy_fn, episodes=5, use_mcts=True, no_trunc_reset_for=None):\n",
    "    \"\"\"\n",
    "    Evaluate a policy for given env_id.\n",
    "    - use_mcts: if True, safe_policy_call attempts 1-step lookahead when supported\n",
    "    - no_trunc_reset_for: set of env_ids that won't trigger regeneration on truncated (here just info)\n",
    "    Returns dict: {\"avg_return\":..., \"avg_length\":..., \"success_rate\":...}\n",
    "    \"\"\"\n",
    "    knowledge = Knowledge()\n",
    "    knowledge.load_static_knowledge(env_id)\n",
    "    sk = knowledge.static_knowledge\n",
    "    mem = Memory()\n",
    "\n",
    "    env = gym.make(env_id)\n",
    "    trunc_sensitive = (env_id not in (no_trunc_reset_for or set()))\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        mem.start_episode()\n",
    "        while not done:\n",
    "            if use_mcts:\n",
    "                a = safe_policy_call(s, policy_fn, sk, env)\n",
    "            else:\n",
    "                # plain policy call\n",
    "                try:\n",
    "                    a = policy_fn(s)\n",
    "                except Exception:\n",
    "                    if sk[\"action_type\"] == \"discrete\":\n",
    "                        a = random.choice(sk[\"action_space\"])\n",
    "                    else:\n",
    "                        lo, hi = sk[\"action_space\"]\n",
    "                        a = 0.5 * (lo + hi)\n",
    "            s_next, r, terminated, truncated, info = safe_step(env, a)\n",
    "            done = terminated or truncated\n",
    "            # truncated info retained—外层会根据需求决定是否重置策略，这里仅记录\n",
    "            mem.add_step(s, a, r, done)\n",
    "            s = s_next\n",
    "        mem.add_episode_summary(env_id, policy_version=0)\n",
    "\n",
    "    env.close()\n",
    "    refl = Reflection(knowledge)\n",
    "    return refl.metrics(mem.get_recent_episodes(n=episodes))\n",
    "\n",
    "def parallel_eval_candidates(env_id, policy_fns, episodes_each=5):\n",
    "    \"\"\"\n",
    "    Parallel evaluate candidate policies; return list of (policy_fn, metrics)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    no_trunc_reset_for = {\"Acrobot-v1\", \"Pendulum-v1\"}\n",
    "    with ThreadPoolExecutor(max_workers=min(len(policy_fns), 3)) as ex:\n",
    "        fut2fn = {\n",
    "            ex.submit(eval_policy_once, env_id, fn, episodes_each, True, no_trunc_reset_for): fn\n",
    "            for fn in policy_fns\n",
    "        }\n",
    "        for fut in as_completed(fut2fn):\n",
    "            fn = fut2fn[fut]\n",
    "            try:\n",
    "                metrics = fut.result()\n",
    "            except Exception as e:\n",
    "                metrics = {\"avg_return\": -1e9, \"avg_length\": 0.0, \"success_rate\": 0.0}\n",
    "                print(\"[Parallel Eval Exception]\", e)\n",
    "            results.append((fn, metrics))\n",
    "    return results\n",
    "\n",
    "# ----------------------------\n",
    "# Main closed-loop training with Policy Pool\n",
    "# ----------------------------\n",
    "def run_env_loop(env_id, max_iters=10, episodes_per_iter=10, ma_window=3,\n",
    "                 success_rate_threshold=0.8, pool_size=5, n_init_candidates=3):\n",
    "    knowledge = Knowledge()\n",
    "    knowledge.load_static_knowledge(env_id)\n",
    "    memory = Memory()\n",
    "    reflection = Reflection(knowledge)\n",
    "    policy_version = 0\n",
    "    policy_pool = PolicyPool(max_size=pool_size)\n",
    "\n",
    "    # === Initial multi-candidate generation + parallel evaluation ===\n",
    "    init_fns = generate_base_policies(env_id, knowledge, n_candidates=n_init_candidates)\n",
    "    evaluated = parallel_eval_candidates(env_id, init_fns, episodes_each=max(2, episodes_per_iter//2))\n",
    "    # pick top by avg_return to fill the pool (and keep all if pool bigger)\n",
    "    evaluated.sort(key=lambda t: t[1][\"avg_return\"], reverse=True)\n",
    "    for fn, m in evaluated[:pool_size]:\n",
    "        policy_version += 1\n",
    "        policy_pool.add_policy(fn, policy_version, metrics=m)\n",
    "\n",
    "    # === Iterative loop ===\n",
    "    for iter_idx in range(max_iters):\n",
    "        print(f\"=== Iteration {iter_idx+1} ===\")\n",
    "\n",
    "        # Select policy via UCB\n",
    "        policy_fn, idx = policy_pool.select_policy_ucb()\n",
    "\n",
    "        # Run selected policy for this iteration\n",
    "        env = gym.make(env_id)\n",
    "        iteration_returns = []\n",
    "        truncated_seen = False\n",
    "        for ep in range(episodes_per_iter):\n",
    "            s, _ = env.reset()\n",
    "            done = False\n",
    "            memory.start_episode()\n",
    "            while not done:\n",
    "                a = safe_policy_call(s, policy_fn, knowledge.static_knowledge, env)\n",
    "                s_next, r, terminated, truncated, info = safe_step(env, a)\n",
    "                done = terminated or truncated\n",
    "                if truncated and env_id not in [\"Acrobot-v1\", \"Pendulum-v1\"]:\n",
    "                    truncated_seen = True\n",
    "                memory.add_step(s, a, r, done)\n",
    "                s = s_next\n",
    "            memory.add_episode_summary(env_id, policy_version=policy_pool.policies[idx][\"version\"])\n",
    "            iteration_returns.append(memory.episodes[-1][\"summary\"][\"return\"])\n",
    "        env.close()\n",
    "\n",
    "        # Compute moving metrics\n",
    "        recent_ma = memory.get_recent_episodes(n=ma_window)\n",
    "        metrics = reflection.metrics(recent_ma)\n",
    "        policy_pool.policies[idx][\"metrics\"] = metrics\n",
    "        policy_pool.update_policy_value(idx, metrics[\"avg_return\"])\n",
    "\n",
    "        print(f\"[Selected idx {idx} v{policy_pool.policies[idx]['version']}] \"\n",
    "              f\"MA Return={metrics['avg_return']:.2f}  SR={metrics['success_rate']:.2f}\")\n",
    "\n",
    "        # Reflection diagnostics & knowledge update\n",
    "        pattern = reflection.failure_pattern(recent_ma, env_id)\n",
    "        print(\"Failure Pattern:\", pattern)\n",
    "        edit = reflection.edit_suggestion(recent_ma, env_id)\n",
    "        print(\"Edit Suggestion:\", edit)\n",
    "\n",
    "        # === Eliminate worst & refill with an improved/new policy ===\n",
    "        worst_idx = policy_pool.get_worst_policy_idx()\n",
    "        # Generate 3 new candidates (edited from current policy as seed for diversity)\n",
    "        seed_fn = policy_fn\n",
    "        # Try one edit from seed, plus 2 fresh policies\n",
    "        new_candidates = []\n",
    "        try:\n",
    "            edited_fn = apply_edit(seed_fn, edit, knowledge, env_id)\n",
    "            new_candidates.append(edited_fn)\n",
    "        except Exception:\n",
    "            pass\n",
    "        fresh = generate_base_policies(env_id, knowledge, n_candidates=2)\n",
    "        new_candidates.extend(fresh)\n",
    "\n",
    "        # Evaluate candidates in parallel and pick the best to insert\n",
    "        cand_eval = parallel_eval_candidates(env_id, new_candidates, episodes_each=max(2, episodes_per_iter//2))\n",
    "        cand_eval.sort(key=lambda t: t[1][\"avg_return\"], reverse=True)\n",
    "        best_new_fn, best_new_metrics = cand_eval[0]\n",
    "        policy_version += 1\n",
    "        policy_pool.policies[worst_idx] = {\"fn\": best_new_fn, \"version\": policy_version, \"metrics\": best_new_metrics}\n",
    "        policy_pool.counts[worst_idx] = 0\n",
    "        policy_pool.values[worst_idx] = 0.0\n",
    "        print(f\"Replaced worst idx {worst_idx} with new v{policy_version}: avg_return={best_new_metrics['avg_return']:.2f}\")\n",
    "\n",
    "        # === Convergence check on moving average ===\n",
    "        if len(memory.episodes) >= ma_window * episodes_per_iter:\n",
    "            thr = knowledge.static_knowledge.get(\"reward_threshold\", 0.0)\n",
    "            if metrics[\"avg_return\"] >= thr and metrics[\"success_rate\"] >= success_rate_threshold:\n",
    "                print(f\"Converged! MA Return={metrics['avg_return']:.2f}, SR={metrics['success_rate']:.2f}\")\n",
    "                break\n",
    "\n",
    "        # Note: truncated_seen no longer regenerates for Acrobot/Pendulum by design\n",
    "        if truncated_seen and env_id not in [\"Acrobot-v1\", \"Pendulum-v1\"]:\n",
    "            print(\"[Info] Truncated observed (non-exempt env) — handled via pool search (no hard reset).\")\n",
    "\n",
    "# ----------------------------\n",
    "# Run multiple control tasks\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    env_list = [\n",
    "        \"Acrobot-v1\",\n",
    "        \"CartPole-v1\",\n",
    "        \"MountainCarContinuous-v0\",\n",
    "        \"MountainCar-v0\",\n",
    "        \"Pendulum-v1\"\n",
    "    ]\n",
    "    for env_id in env_list:\n",
    "        print(f\"==== Running {env_id} ====\")\n",
    "        run_env_loop(env_id, max_iters=10, episodes_per_iter=8, ma_window=3,\n",
    "                     success_rate_threshold=0.8, pool_size=5, n_init_candidates=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4685dea-0585-4079-873b-a3026520fd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_rl",
   "language": "python",
   "name": "dl_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
